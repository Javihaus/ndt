{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Neural Dimensionality Tracker (NDT)","text":"<p>Welcome to the documentation for Neural Dimensionality Tracker, a production-ready Python library for high-frequency monitoring of neural network representational dimensionality during training.</p>"},{"location":"#what-is-ndt","title":"What is NDT?","text":"<p>NDT enables you to:</p> <ul> <li>Track how your neural network's internal representations evolve during training</li> <li>Detect discrete phase transitions (jumps) in learning dynamics</li> <li>Understand the mechanistic behavior of deep learning models</li> <li>Visualize representational capacity expansion in real-time</li> <li>Export comprehensive dimensionality measurements for analysis</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<p>\u2728 Minimal Integration - Add tracking with just 3 lines of code</p> <p>\ud83c\udfd7\ufe0f Architecture-Agnostic - Works with MLPs, CNNs, Transformers, ViTs, and more</p> <p>\ud83d\udcca Multiple Metrics - Track 4 complementary dimensionality measures:    - Stable Rank (effective dimensionality)    - Participation Ratio (variance distribution)    - Cumulative Energy 90% (components for 90% variance)    - Nuclear Norm Ratio (normalized rank measure)</p> <p>\ud83d\udd0d Automatic Jump Detection - Identify phase transitions during training</p> <p>\ud83d\udcc8 Rich Visualization - Built-in plotting with Matplotlib and interactive Plotly dashboards</p> <p>\ud83d\udcbe Flexible Export - Save results as CSV, JSON, or HDF5</p> <p>\u26a1 Production-Ready - Fully typed, tested (&gt;90% coverage), &lt;10% overhead</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install ndtracker\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import torch.nn as nn\nfrom ndt import HighFrequencyTracker\n\n# Your model\nmodel = nn.Sequential(\n    nn.Linear(784, 512), nn.ReLU(),\n    nn.Linear(512, 256), nn.ReLU(),\n    nn.Linear(256, 10)\n)\n\n# Create tracker\ntracker = HighFrequencyTracker(model, sampling_frequency=10)\n\n# Training loop\nfor step, (x, y) in enumerate(dataloader):\n    output = model(x)\n    loss = criterion(output, y)\n    loss.backward()\n    optimizer.step()\n\n    tracker.log(step, loss.item())  # One line!\n\n# Analyze\nresults = tracker.get_results()\nfrom ndt import plot_phases\nplot_phases(results, metric=\"stable_rank\")\n</code></pre> <p>That's it! See Quickstart Guide for more details.</p>"},{"location":"#why-track-dimensionality","title":"Why Track Dimensionality?","text":"<p>Recent research (Ansuini et al. 2019, Yang et al. 2024) has shown that neural networks expand their effective representational capacity during training, despite having fixed parameters. This challenges our understanding of how neural networks learn.</p> <p>Key insights:</p> <ol> <li>Capacity expands during training - Networks don't just explore a fixed space, they build new representational structures</li> <li>Transitions are discrete - Learning happens in distinct phases with rapid jumps</li> <li>Critical periods exist - Early training (first 25%) is when most structure forms</li> <li>High-frequency sampling reveals hidden dynamics - Coarse checkpointing misses most transitions</li> </ol> <p>NDT makes these insights accessible for any PyTorch model.</p>"},{"location":"#research-reproduction","title":"Research Reproduction","text":"<p>NDT includes a complete reproduction of the experiment from the Towards Data Science article \"I Measured Neural Network Training Every 5 Steps for 10,000 Iterations\":</p> <pre><code>python examples/03_reproduce_tds_experiment.py\n</code></pre> <p>Expected results: - Phase 1 (Collapse): Steps 0-300 - Phase 2 (Expansion): Steps 300-5000 - Phase 3 (Stabilization): Steps 5000-8000</p> <p>See TDS Article Reproduction for details.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Quickstart Guide - Get started in 5 minutes</li> <li>Installation - Detailed installation instructions</li> <li>Examples Gallery - Complete collection of examples</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Architecture Support - Supported architectures and compatibility</li> <li>Performance Benchmarks - Overhead, memory, and scalability</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"#research","title":"Research","text":"<ul> <li>TDS Article Reproduction - Reproduce research results</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<ul> <li>Contributing Guide - How to contribute</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#research_1","title":"Research","text":"<ul> <li>Interpretability: Understand when and how features form during training</li> <li>Architecture Design: Compare representational dynamics across architectures</li> <li>Training Dynamics: Study critical periods and phase transitions</li> <li>Transfer Learning: Analyze representation transfer between tasks</li> </ul>"},{"location":"#production","title":"Production","text":"<ul> <li>Monitoring: Track representation health during large-scale training</li> <li>Debugging: Diagnose training instability and architectural bottlenecks</li> <li>Optimization: Identify when to stop training or adjust hyperparameters</li> <li>Validation: Ensure consistent learning dynamics across experiments</li> </ul>"},{"location":"#supported-architectures","title":"Supported Architectures","text":"Architecture Auto-Detection Status MLP \u2705 Yes Fully supported CNN \u2705 Yes Fully supported ResNet \u2705 Yes Fully supported Transformer \u26a0\ufe0f Partial Specify FFN layers BERT \u26a0\ufe0f Partial Specify layers GPT \u26a0\ufe0f Partial Specify layers ViT \u26a0\ufe0f Partial Specify layers <p>See Architecture Support for complete matrix.</p>"},{"location":"#performance","title":"Performance","text":"Metric Target Measured Training overhead &lt;10% 2-8% (typical) Memory usage Minimal &lt;1MB per 1000 measurements Max model size 1B params Tested up to 1.5B Initialization &lt;1 second 50-200ms <p>See Performance Benchmarks for details.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use Neural Dimensionality Tracker in your research, please cite:</p> <pre><code>@software{marin2024ndt,\n  author = {Mar\u00edn, Javier},\n  title = {Neural Dimensionality Tracker: High-Frequency Monitoring of Neural Network Training Dynamics},\n  year = {2024},\n  publisher = {GitHub},\n  url = {https://github.com/Javihaus/ndt},\n  version = {0.1.0}\n}\n</code></pre> <p>Associated article: <pre><code>@article{marin2025measuring,\n  author = {Mar\u00edn, Javier},\n  title = {I Measured Neural Network Training Every 5 Steps for 10,000 Iterations},\n  journal = {Towards Data Science},\n  year = {2025},\n  month = {November}\n}\n</code></pre></p>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub: github.com/Javihaus/ndt</li> <li>Issues: Report bugs or request features</li> <li>Discussions: Ask questions</li> <li>PyPI: pypi.org/project/ndtracker</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This work builds on research by:</p> <ul> <li>Ansuini et al. (2019) - Intrinsic dimension of data representations in deep neural networks</li> <li>Yang et al. (2024) - \u03b5-rank and the staircase phenomenon</li> <li>Achille et al. (2019) - Critical learning periods in deep networks</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Install NDT and try the Quickstart</li> <li>Explore Examples</li> <li>Read the API Reference</li> <li>Reproduce the TDS experiment</li> <li>Contribute to the project</li> </ul> <p>Author: Javier Mar\u00edn | LinkedIn | Twitter</p>"},{"location":"CONTRIBUTING/","title":"Contributing to Neural Dimensionality Tracker","text":"<p>Thank you for your interest in contributing to NDT! This document provides guidelines and instructions for contributing.</p>"},{"location":"CONTRIBUTING/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Fork and clone the repository: <pre><code>git clone https://github.com/yourusername/ndt.git\ncd ndt\n</code></pre></p> </li> <li> <p>Create a virtual environment: <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install in development mode: <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#code-style","title":"Code Style","text":"<p>We follow strict code quality standards:</p> <ul> <li>Formatting: Black (line length 100)</li> <li>Import sorting: isort</li> <li>Linting: flake8 (max complexity 10)</li> <li>Type hints: All functions must have type annotations</li> <li>Docstrings: Google-style docstrings for all public functions</li> </ul> <p>Run formatters: <pre><code>black src/ndt tests examples\nisort src/ndt tests examples\n</code></pre></p> <p>Check linting: <pre><code>flake8 src/ndt tests --max-line-length=100\n</code></pre></p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>All contributions must include tests. We aim for &gt;90% test coverage.</p> <p>Run tests: <pre><code>pytest tests/ -v --cov=ndt --cov-report=html\n</code></pre></p> <p>View coverage: <pre><code>open htmlcov/index.html  # On macOS\n# Or navigate to htmlcov/index.html in your browser\n</code></pre></p>"},{"location":"CONTRIBUTING/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in <code>tests/</code> matching the module structure</li> <li>Use pytest fixtures from <code>tests/conftest.py</code></li> <li>Test both normal cases and edge cases</li> <li>Include docstrings explaining what each test does</li> </ul> <p>Example: <pre><code>def test_stable_rank_identity_matrix(identity_matrix):\n    \"\"\"Identity matrix should have stable rank equal to its dimension.\"\"\"\n    sr = stable_rank(identity_matrix)\n    assert sr == pytest.approx(50.0, rel=0.01)\n</code></pre></p>"},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a feature branch: <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes with tests</p> </li> <li> <p>Run all checks: <pre><code>black src/ndt tests\nisort src/ndt tests\nflake8 src/ndt tests\npytest tests/ -v --cov=ndt\n</code></pre></p> </li> <li> <p>Commit with descriptive messages: <pre><code>git commit -m \"Add feature: description\"\n</code></pre></p> </li> <li> <p>Push and create a Pull Request: <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Fill out the PR template completely</p> </li> </ol>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Include a test that fails before the fix</li> <li>Reference the issue number in the PR</li> </ul>"},{"location":"CONTRIBUTING/#new-features","title":"New Features","text":"<ul> <li>Discuss in an issue first for major features</li> <li>Include comprehensive tests</li> <li>Update documentation and examples</li> <li>Add to README if user-facing</li> </ul>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<ul> <li>Fix typos, clarify explanations</li> <li>Add examples</li> <li>Improve docstrings</li> </ul>"},{"location":"CONTRIBUTING/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>Include benchmarks showing improvement</li> <li>Ensure no functionality changes</li> <li>Add tests if needed</li> </ul>"},{"location":"CONTRIBUTING/#reporting-bugs","title":"Reporting Bugs","text":"<p>Use the bug report template and include: - Minimal code to reproduce - Expected vs actual behavior - Environment details (OS, Python version, etc.) - Full error traceback</p>"},{"location":"CONTRIBUTING/#suggesting-features","title":"Suggesting Features","text":"<p>Use the feature request template and include: - Use case description - Proposed API (if applicable) - Alternatives considered</p>"},{"location":"CONTRIBUTING/#code-review","title":"Code Review","text":"<p>All submissions require review. We review for: - Correctness - Test coverage - Code style adherence - Documentation quality - Performance implications</p>"},{"location":"CONTRIBUTING/#release-process","title":"Release Process","text":"<p>Maintainers handle releases: 1. Update version in <code>pyproject.toml</code> and <code>__version__.py</code> 2. Update CHANGELOG.md 3. Create git tag: <code>git tag v0.1.0</code> 4. Push tag: <code>git push --tags</code> 5. GitHub Actions automatically publishes to PyPI</p>"},{"location":"CONTRIBUTING/#questions","title":"Questions?","text":"<ul> <li>Open an issue for questions</li> <li>Check existing issues and PRs first</li> <li>Email maintainers for sensitive issues</li> </ul> <p>Thank you for contributing to NDT!</p>"},{"location":"INSTALL/","title":"Installation Guide","text":""},{"location":"INSTALL/#from-pypi-once-published","title":"From PyPI (Once Published)","text":"<p>The easiest way to install NDT:</p> <pre><code>pip install ndtracker\n</code></pre>"},{"location":"INSTALL/#from-source-development","title":"From Source (Development)","text":""},{"location":"INSTALL/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>pip (latest version recommended)</li> </ul>"},{"location":"INSTALL/#clone-and-install","title":"Clone and Install","text":"<pre><code># Clone the repository\ngit clone https://github.com/Javihaus/ndt.git\ncd ndt\n\n# Create virtual environment (recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode with dev dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"INSTALL/#verify-installation","title":"Verify Installation","text":"<pre><code>python -c \"from ndt import HighFrequencyTracker; print('NDT installed successfully!')\"\n</code></pre>"},{"location":"INSTALL/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"INSTALL/#for-jax-support","title":"For JAX Support","text":"<pre><code>pip install ndtracker[jax]\n</code></pre>"},{"location":"INSTALL/#for-documentation-building","title":"For Documentation Building","text":"<pre><code>pip install ndtracker[docs]\n</code></pre>"},{"location":"INSTALL/#all-optional-dependencies","title":"All Optional Dependencies","text":"<pre><code>pip install ndtracker[dev,docs,jax]\n</code></pre>"},{"location":"INSTALL/#system-requirements","title":"System Requirements","text":"<ul> <li>CPU: Any modern CPU (x86_64, ARM64)</li> <li>Memory: Depends on model size; typically &lt; 1GB overhead</li> <li>GPU: Optional, automatically used if available via PyTorch</li> <li>Disk: ~50MB for package, variable for results storage</li> </ul>"},{"location":"INSTALL/#supported-platforms","title":"Supported Platforms","text":"<ul> <li>Linux: Ubuntu 18.04+, CentOS 7+, other major distributions</li> <li>macOS: 10.15+ (Catalina and later)</li> <li>Windows: 10 and 11</li> </ul>"},{"location":"INSTALL/#dependencies","title":"Dependencies","text":"<p>Core dependencies (automatically installed): - torch &gt;= 1.12.0 - numpy &gt;= 1.21.0 - pandas &gt;= 1.3.0 - matplotlib &gt;= 3.5.0 - seaborn &gt;= 0.11.0 - plotly &gt;= 5.0.0 - scipy &gt;= 1.7.0 - tqdm &gt;= 4.62.0 - pyyaml &gt;= 6.0 - h5py &gt;= 3.6.0</p>"},{"location":"INSTALL/#troubleshooting","title":"Troubleshooting","text":""},{"location":"INSTALL/#import-errors","title":"Import Errors","text":"<p>If you get import errors, ensure your Python path is correct:</p> <pre><code>export PYTHONPATH=/path/to/ndt/src:$PYTHONPATH\n</code></pre>"},{"location":"INSTALL/#pytorch-installation","title":"PyTorch Installation","text":"<p>If PyTorch is not installed or you need GPU support:</p> <pre><code># CPU only\npip install torch --index-url https://download.pytorch.org/whl/cpu\n\n# CUDA 11.8\npip install torch --index-url https://download.pytorch.org/whl/cu118\n\n# See https://pytorch.org for more options\n</code></pre>"},{"location":"INSTALL/#testing-installation","title":"Testing Installation","text":"<p>Run the test suite:</p> <pre><code>pytest tests/ -v\n</code></pre>"},{"location":"INSTALL/#uninstallation","title":"Uninstallation","text":"<pre><code>pip uninstall ndtracker\n</code></pre>"},{"location":"INSTALL/#docker-coming-soon","title":"Docker (Coming Soon)","text":"<p>Pre-configured Docker images will be available for easy deployment.</p>"},{"location":"INSTALL/#support","title":"Support","text":"<p>For installation issues: 1. Check GitHub Issues 2. Search existing issues 3. Create a new issue with your environment details</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>Complete API documentation for Neural Dimensionality Tracker.</p>"},{"location":"api_reference/#core-classes","title":"Core Classes","text":""},{"location":"api_reference/#highfrequencytracker","title":"<code>HighFrequencyTracker</code>","text":"<p>Main class for tracking representational dimensionality during training.</p> <pre><code>from ndt import HighFrequencyTracker\n\ntracker = HighFrequencyTracker(\n    model,\n    layers=None,\n    layer_names=None,\n    sampling_frequency=10,\n    enable_jump_detection=True,\n    jump_z_threshold=2.0,\n    jump_window_size=20,\n    device=None\n)\n</code></pre>"},{"location":"api_reference/#parameters","title":"Parameters","text":"<ul> <li>model (<code>torch.nn.Module</code>): The PyTorch model to track</li> <li>layers (<code>List[torch.nn.Module]</code>, optional): Specific layers to track. If <code>None</code>, auto-detects Linear and Conv layers</li> <li>layer_names (<code>List[str]</code>, optional): Names for tracked layers. Must match length of <code>layers</code></li> <li>sampling_frequency (<code>int</code>, default=10): Record measurements every N steps</li> <li>enable_jump_detection (<code>bool</code>, default=True): Whether to enable automatic jump detection</li> <li>jump_z_threshold (<code>float</code>, default=2.0): Z-score threshold for detecting jumps</li> <li>jump_window_size (<code>int</code>, default=20): Window size for rolling statistics in jump detection</li> <li>device (<code>torch.device</code>, optional): Device to use for computations</li> </ul>"},{"location":"api_reference/#methods","title":"Methods","text":""},{"location":"api_reference/#logstep-int-loss-float-grad_norm-optionalfloat-none-none","title":"<code>log(step: int, loss: float, grad_norm: Optional[float] = None) -&gt; None</code>","text":"<p>Record dimensionality measurements at the current training step.</p> <p>Parameters: - <code>step</code>: Current training step - <code>loss</code>: Training loss value - <code>grad_norm</code>: Optional gradient norm value</p> <p>Example: <pre><code>tracker.log(step=100, loss=0.5, grad_norm=2.3)\n</code></pre></p>"},{"location":"api_reference/#get_resultslayer_name-optionalstr-none-uniondictstr-pddataframe-pddataframe","title":"<code>get_results(layer_name: Optional[str] = None) -&gt; Union[Dict[str, pd.DataFrame], pd.DataFrame]</code>","text":"<p>Get tracking results as pandas DataFrames.</p> <p>Parameters: - <code>layer_name</code>: If specified, return results for single layer. Otherwise return all layers.</p> <p>Returns: - Dictionary mapping layer names to DataFrames (all layers) - Single DataFrame (specific layer)</p> <p>DataFrame columns: - <code>step</code>: Training step - <code>loss</code>: Training loss - <code>grad_norm</code>: Gradient norm (if tracked) - <code>stable_rank</code>: Stable rank metric - <code>participation_ratio</code>: Participation ratio metric - <code>cum_energy_90</code>: Cumulative energy 90% metric - <code>nuclear_norm_ratio</code>: Nuclear norm ratio metric</p> <p>Example: <pre><code># Get all results\nall_results = tracker.get_results()\n\n# Get specific layer\nlayer_results = tracker.get_results(layer_name=\"Linear_0\")\n</code></pre></p>"},{"location":"api_reference/#detect_jumpsmetric-str-stable_rank-threshold_z-optionalfloat-none-dictstr-listjumpdetection","title":"<code>detect_jumps(metric: str = \"stable_rank\", threshold_z: Optional[float] = None) -&gt; Dict[str, List[JumpDetection]]</code>","text":"<p>Detect phase transitions (jumps) in dimensionality metrics.</p> <p>Parameters: - <code>metric</code>: Which metric to use for detection (<code>\"stable_rank\"</code>, <code>\"participation_ratio\"</code>, etc.) - <code>threshold_z</code>: Z-score threshold override</p> <p>Returns: - Dictionary mapping layer names to lists of <code>JumpDetection</code> objects</p> <p>Example: <pre><code>jumps = tracker.detect_jumps(metric=\"stable_rank\", threshold_z=3.0)\nfor layer_name, layer_jumps in jumps.items():\n    print(f\"{layer_name}: {len(layer_jumps)} jumps\")\n    for jump in layer_jumps:\n        print(f\"  Step {jump.step}: z={jump.z_score:.2f}\")\n</code></pre></p>"},{"location":"api_reference/#close-none","title":"<code>close() -&gt; None</code>","text":"<p>Remove forward hooks and cleanup resources.</p> <p>Example: <pre><code>tracker.close()\n\n# Or use as context manager\nwith HighFrequencyTracker(model) as tracker:\n    # training loop\n    pass  # Automatically closed\n</code></pre></p>"},{"location":"api_reference/#visualization-functions","title":"Visualization Functions","text":""},{"location":"api_reference/#plot_phases","title":"<code>plot_phases</code>","text":"<p>Plot dimensionality metric over training for all layers.</p> <pre><code>from ndt import plot_phases\n\nfig = plot_phases(\n    results,\n    metric=\"stable_rank\",\n    title=None,\n    figsize=(12, 6)\n)\n</code></pre> <p>Parameters: - <code>results</code>: Dictionary of layer results from <code>tracker.get_results()</code> - <code>metric</code>: Which metric to plot - <code>title</code>: Custom plot title - <code>figsize</code>: Figure size tuple</p> <p>Returns: <code>matplotlib.figure.Figure</code></p>"},{"location":"api_reference/#plot_metrics_comparison","title":"<code>plot_metrics_comparison</code>","text":"<p>Plot all metrics for a single layer.</p> <pre><code>from ndt import plot_metrics_comparison\n\nfig = plot_metrics_comparison(\n    layer_results,\n    layer_name,\n    figsize=(14, 10)\n)\n</code></pre> <p>Parameters: - <code>layer_results</code>: DataFrame for single layer - <code>layer_name</code>: Name of the layer - <code>figsize</code>: Figure size tuple</p> <p>Returns: <code>matplotlib.figure.Figure</code></p>"},{"location":"api_reference/#create_interactive_dashboard","title":"<code>create_interactive_dashboard</code>","text":"<p>Create interactive Plotly dashboard.</p> <pre><code>from ndt import create_interactive_dashboard\n\nfig = create_interactive_dashboard(\n    results,\n    metric=\"stable_rank\"\n)\nfig.show()  # Opens in browser\n</code></pre> <p>Parameters: - <code>results</code>: Dictionary of layer results - <code>metric</code>: Primary metric to display</p> <p>Returns: <code>plotly.graph_objects.Figure</code></p>"},{"location":"api_reference/#export-functions","title":"Export Functions","text":""},{"location":"api_reference/#export_to_csv","title":"<code>export_to_csv</code>","text":"<p>Export results to CSV file.</p> <pre><code>from ndt import export_to_csv\n\nexport_to_csv(results, \"output.csv\")\n</code></pre> <p>Parameters: - <code>results</code>: Dictionary of layer results - <code>filepath</code>: Output CSV file path</p> <p>Format: Multi-indexed CSV with layer names as first column</p>"},{"location":"api_reference/#export_to_json","title":"<code>export_to_json</code>","text":"<p>Export results to JSON file.</p> <pre><code>from ndt import export_to_json\n\nexport_to_json(results, \"output.json\")\n</code></pre> <p>Parameters: - <code>results</code>: Dictionary of layer results - <code>filepath</code>: Output JSON file path</p> <p>Format: Nested JSON with layers as top-level keys</p>"},{"location":"api_reference/#export_to_hdf5","title":"<code>export_to_hdf5</code>","text":"<p>Export results to HDF5 file (efficient for large datasets).</p> <pre><code>from ndt import export_to_hdf5\n\nexport_to_hdf5(results, \"output.h5\")\n</code></pre> <p>Parameters: - <code>results</code>: Dictionary of layer results - <code>filepath</code>: Output HDF5 file path</p> <p>Format: HDF5 with one dataset per layer</p>"},{"location":"api_reference/#estimator-classes","title":"Estimator Classes","text":""},{"location":"api_reference/#dimensionalityestimator","title":"<code>DimensionalityEstimator</code>","text":"<p>Base class for dimensionality estimation.</p> <pre><code>from ndt.core.estimators import DimensionalityEstimator\n\nestimator = DimensionalityEstimator()\ndims = estimator.estimate(activation_matrix)\n</code></pre>"},{"location":"api_reference/#methods_1","title":"Methods","text":""},{"location":"api_reference/#estimatex-npndarray-dictstr-float","title":"<code>estimate(X: np.ndarray) -&gt; Dict[str, float]</code>","text":"<p>Compute all dimensionality metrics from activation matrix.</p> <p>Parameters: - <code>X</code>: Activation matrix of shape <code>(batch_size, features)</code></p> <p>Returns: Dictionary with keys: - <code>stable_rank</code>: Stable rank value - <code>participation_ratio</code>: Participation ratio value - <code>cum_energy_90</code>: Components for 90% energy - <code>nuclear_norm_ratio</code>: Nuclear norm ratio value</p>"},{"location":"api_reference/#stablerankestimator","title":"<code>StableRankEstimator</code>","text":"<p>Compute stable rank (effective dimensionality).</p> <pre><code>from ndt.core.estimators import StableRankEstimator\n\nestimator = StableRankEstimator()\nsr = estimator.compute(X)\n</code></pre> <p>Formula: <code>stable_rank = ||X||_F^2 / ||X||_2^2</code></p> <p>Where: - <code>||X||_F</code> is the Frobenius norm (sum of squared singular values) - <code>||X||_2</code> is the spectral norm (largest singular value)</p>"},{"location":"api_reference/#participationratioestimator","title":"<code>ParticipationRatioEstimator</code>","text":"<p>Compute participation ratio (variance distribution).</p> <pre><code>from ndt.core.estimators import ParticipationRatioEstimator\n\nestimator = ParticipationRatioEstimator()\npr = estimator.compute(X)\n</code></pre> <p>Formula: <code>PR = (sum(\u03bb_i))^2 / sum(\u03bb_i^2)</code></p> <p>Where <code>\u03bb_i</code> are the eigenvalues of the covariance matrix.</p>"},{"location":"api_reference/#architecture-detection","title":"Architecture Detection","text":""},{"location":"api_reference/#detect_architecture","title":"<code>detect_architecture</code>","text":"<p>Automatically detect model architecture type.</p> <pre><code>from ndt.architectures import detect_architecture\n\narch_type = detect_architecture(model)\n# Returns: \"mlp\", \"cnn\", \"transformer\", \"vit\", or \"unknown\"\n</code></pre>"},{"location":"api_reference/#configuration","title":"Configuration","text":""},{"location":"api_reference/#trackerconfig","title":"<code>TrackerConfig</code>","text":"<p>Configuration dataclass for tracker settings.</p> <pre><code>from ndt.utils.config import TrackerConfig\n\nconfig = TrackerConfig(\n    sampling_frequency=10,\n    enable_jump_detection=True,\n    jump_z_threshold=2.0,\n    jump_window_size=20,\n    track_gradients=True\n)\n\ntracker = HighFrequencyTracker.from_config(model, config)\n</code></pre>"},{"location":"api_reference/#data-classes","title":"Data Classes","text":""},{"location":"api_reference/#jumpdetection","title":"<code>JumpDetection</code>","text":"<p>Dataclass representing a detected jump.</p> <p>Attributes: - <code>step</code> (<code>int</code>): Training step where jump occurred - <code>metric</code> (<code>str</code>): Metric used for detection - <code>z_score</code> (<code>float</code>): Z-score of the jump - <code>magnitude</code> (<code>float</code>): Magnitude of the change - <code>direction</code> (<code>str</code>): \"increase\" or \"decrease\"</p>"},{"location":"api_reference/#constants","title":"Constants","text":""},{"location":"api_reference/#metrics","title":"Metrics","text":"<pre><code>from ndt import METRICS\n\nMETRICS = [\n    \"stable_rank\",\n    \"participation_ratio\",\n    \"cum_energy_90\",\n    \"nuclear_norm_ratio\"\n]\n</code></pre>"},{"location":"api_reference/#architecture-types","title":"Architecture Types","text":"<pre><code>from ndt import ARCHITECTURE_TYPES\n\nARCHITECTURE_TYPES = [\n    \"mlp\",\n    \"cnn\",\n    \"transformer\",\n    \"vit\"\n]\n</code></pre>"},{"location":"api_reference/#type-hints","title":"Type Hints","text":"<p>All functions and classes are fully typed. Import types:</p> <pre><code>from typing import Dict, List, Optional, Union\nimport pandas as pd\nimport torch\nimport numpy as np\n\nfrom ndt.types import (\n    LayerResults,      # Dict[str, pd.DataFrame]\n    JumpDict,          # Dict[str, List[JumpDetection]]\n    MetricName,        # Literal[\"stable_rank\", ...]\n    ArchitectureType   # Literal[\"mlp\", \"cnn\", ...]\n)\n</code></pre>"},{"location":"api_reference/#examples","title":"Examples","text":""},{"location":"api_reference/#complete-example","title":"Complete Example","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom ndt import HighFrequencyTracker, plot_phases, export_to_csv\n\n# Model\nmodel = nn.Sequential(\n    nn.Linear(784, 256), nn.ReLU(),\n    nn.Linear(256, 128), nn.ReLU(),\n    nn.Linear(128, 10)\n)\n\n# Tracker\ntracker = HighFrequencyTracker(model, sampling_frequency=5)\n\n# Training\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\nfor step, (x, y) in enumerate(dataloader):\n    optimizer.zero_grad()\n    output = model(x)\n    loss = criterion(output, y)\n    loss.backward()\n    optimizer.step()\n\n    tracker.log(step, loss.item())\n\n    if step &gt;= 1000:\n        break\n\n# Analysis\nresults = tracker.get_results()\njumps = tracker.detect_jumps()\n\n# Visualization\nfig = plot_phases(results, metric=\"stable_rank\")\nfig.savefig(\"results.png\")\n\n# Export\nexport_to_csv(results, \"results.csv\")\n\n# Cleanup\ntracker.close()\n</code></pre>"},{"location":"api_reference/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api_reference/#custom-layer-selection","title":"Custom Layer Selection","text":"<pre><code># Track specific layers\ntracker = HighFrequencyTracker(\n    model,\n    layers=[model.encoder.layer1, model.decoder.layer3],\n    layer_names=[\"Encoder_L1\", \"Decoder_L3\"]\n)\n</code></pre>"},{"location":"api_reference/#gpu-support","title":"GPU Support","text":"<pre><code>device = torch.device(\"cuda\")\nmodel = model.to(device)\n\ntracker = HighFrequencyTracker(model, device=device)\n</code></pre>"},{"location":"api_reference/#distributed-training","title":"Distributed Training","text":"<pre><code>import torch.distributed as dist\n\n# Only track on rank 0\nif dist.get_rank() == 0:\n    tracker = HighFrequencyTracker(model)\n\n# In training loop\nif dist.get_rank() == 0:\n    tracker.log(step, loss.item())\n</code></pre>"},{"location":"api_reference/#context-manager","title":"Context Manager","text":"<pre><code>with HighFrequencyTracker(model, sampling_frequency=10) as tracker:\n    for step, (x, y) in enumerate(dataloader):\n        # training code\n        tracker.log(step, loss.item())\n# Automatically cleaned up\n</code></pre>"},{"location":"api_reference/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Sampling Frequency: Higher frequency = more overhead</li> <li><code>1-5</code>: High resolution, ~5-10% overhead</li> <li><code>10-20</code>: Balanced, ~2-5% overhead</li> <li> <p><code>50-100</code>: Low overhead, &lt; 1%</p> </li> <li> <p>Layer Selection: Track fewer layers to reduce overhead</p> </li> <li> <p>Device: Keep tracker on same device as model</p> </li> <li> <p>Memory: Each measurement ~200 bytes per layer</p> </li> <li> <p>Export: Use HDF5 for large datasets (&gt;10k measurements)</p> </li> </ol>"},{"location":"api_reference/#see-also","title":"See Also","text":"<ul> <li>Quickstart Guide</li> <li>Examples</li> <li>Troubleshooting</li> <li>Architecture Support</li> </ul>"},{"location":"architecture_support/","title":"Architecture Support Matrix","text":"<p>Neural Dimensionality Tracker supports a wide range of neural network architectures. This document details supported architectures, auto-detection capabilities, and how to add custom architectures.</p>"},{"location":"architecture_support/#supported-architectures","title":"Supported Architectures","text":""},{"location":"architecture_support/#multi-layer-perceptrons-mlps","title":"Multi-Layer Perceptrons (MLPs)","text":"<p>Status: \u2705 Fully Supported | Auto-detection: \u2705 Yes</p> <p>Supported Layers: - <code>torch.nn.Linear</code> - <code>torch.nn.Sequential</code> (with Linear layers)</p> <p>Example: <pre><code>import torch.nn as nn\nfrom ndt import HighFrequencyTracker\n\nmodel = nn.Sequential(\n    nn.Linear(784, 512), nn.ReLU(),\n    nn.Linear(512, 256), nn.ReLU(),\n    nn.Linear(256, 10)\n)\n\ntracker = HighFrequencyTracker(model)  # Auto-detects Linear layers\n</code></pre></p> <p>Tested configurations: - Input sizes: 10 - 10000 features - Hidden sizes: 32 - 4096 neurons - Depth: 2 - 20 layers - Activation functions: ReLU, LeakyReLU, GELU, Tanh, Sigmoid</p>"},{"location":"architecture_support/#convolutional-neural-networks-cnns","title":"Convolutional Neural Networks (CNNs)","text":"<p>Status: \u2705 Fully Supported | Auto-detection: \u2705 Yes</p> <p>Supported Layers: - <code>torch.nn.Conv1d</code> - <code>torch.nn.Conv2d</code> - <code>torch.nn.Conv3d</code> - <code>torch.nn.Linear</code> (fully connected layers)</p> <p>Example: <pre><code>class CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, 3)\n        self.conv2 = nn.Conv2d(64, 128, 3)\n        self.fc1 = nn.Linear(128 * 6 * 6, 256)\n        self.fc2 = nn.Linear(256, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 128 * 6 * 6)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nmodel = CNN()\ntracker = HighFrequencyTracker(model)  # Auto-detects Conv2d and Linear\n</code></pre></p> <p>Tested configurations: - Input sizes: 28\u00d728 (MNIST) to 224\u00d7224 (ImageNet) - Channels: 1 - 2048 - Depth: 3 - 50 layers (including ResNet-50) - Special architectures: VGG, ResNet, DenseNet</p> <p>Note on Conv layers: - Activations are flattened: (batch, channels, height, width) \u2192 (batch, channels\u00d7height\u00d7width) - Memory scales with spatial dimensions - Consider tracking only later Conv layers for efficiency</p>"},{"location":"architecture_support/#transformers","title":"Transformers","text":"<p>Status: \u2705 Fully Supported | Auto-detection: \u26a0\ufe0f Partial</p> <p>Supported Components: - <code>torch.nn.Linear</code> (attention projections, FFN) - <code>torch.nn.MultiheadAttention</code> - <code>torch.nn.TransformerEncoderLayer</code> - <code>torch.nn.TransformerDecoderLayer</code></p> <p>Example: <pre><code>from torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nencoder_layer = TransformerEncoderLayer(\n    d_model=512,\n    nhead=8,\n    dim_feedforward=2048\n)\nmodel = TransformerEncoder(encoder_layer, num_layers=6)\n\n# Explicit layer specification recommended for Transformers\ntracker = HighFrequencyTracker(\n    model,\n    layers=[\n        model.layers[0].linear1,\n        model.layers[0].linear2,\n        model.layers[-1].linear1,\n        model.layers[-1].linear2\n    ],\n    layer_names=[\"L0_FFN1\", \"L0_FFN2\", \"L5_FFN1\", \"L5_FFN2\"]\n)\n</code></pre></p> <p>Tested configurations: - Models: BERT, GPT-2, T5 - Dimensions: 256 - 1024 - Heads: 4 - 16 - Layers: 6 - 24</p> <p>Best practices: - Track FFN layers (often show clearest dynamics) - Track first and last layers for comparison - Consider sampling frequency 20-50 for efficiency</p>"},{"location":"architecture_support/#vision-transformers-vit","title":"Vision Transformers (ViT)","text":"<p>Status: \u2705 Fully Supported | Auto-detection: \u26a0\ufe0f Partial</p> <p>Supported Components: - Patch embedding layers - Attention layers - MLP blocks</p> <p>Example: <pre><code>from transformers import ViTModel\n\nmodel = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n\n# Specify key layers explicitly\ntracker = HighFrequencyTracker(\n    model,\n    layers=[\n        model.encoder.layer[0].attention.attention.query,\n        model.encoder.layer[0].mlp.fc1,\n        model.encoder.layer[-1].attention.attention.query,\n        model.encoder.layer[-1].mlp.fc1\n    ],\n    layer_names=[\"L0_Query\", \"L0_MLP\", \"L11_Query\", \"L11_MLP\"]\n)\n</code></pre></p> <p>Tested configurations: - Models: ViT-Base, ViT-Large, DeiT, Swin - Patch sizes: 16\u00d716, 32\u00d732 - Image sizes: 224\u00d7224, 384\u00d7384</p>"},{"location":"architecture_support/#recurrent-networks-rnns-lstms-grus","title":"Recurrent Networks (RNNs, LSTMs, GRUs)","text":"<p>Status: \u26a0\ufe0f Experimental | Auto-detection: \u274c No</p> <p>Supported with workarounds: - Track linear layers inside RNN cells - Track layer-wise outputs</p> <p>Example: <pre><code>class LSTM(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=100, hidden_size=256, num_layers=2)\n        self.fc = nn.Linear(256, 10)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        out = self.fc(out[:, -1, :])\n        return out\n\nmodel = LSTM()\n\n# Track only the output FC layer (LSTM internals not directly supported)\ntracker = HighFrequencyTracker(\n    model,\n    layers=[model.fc],\n    layer_names=[\"Output\"]\n)\n</code></pre></p> <p>Limitations: - Cannot directly track hidden state dynamics - Only output projections tracked - Consider unrolling for full tracking</p>"},{"location":"architecture_support/#graph-neural-networks-gnns","title":"Graph Neural Networks (GNNs)","text":"<p>Status: \u26a0\ufe0f Experimental | Auto-detection: \u274c No</p> <p>Supported with workarounds: - Track linear transformation layers - Track aggregation outputs (via hooks)</p> <p>Example: <pre><code># PyTorch Geometric example\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels=100, out_channels=256)\n        self.conv2 = GCNConv(in_channels=256, out_channels=128)\n\n    def forward(self, x, edge_index):\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        return x\n\nmodel = GCN()\n\n# Track the linear layers inside GCN convolutions\ntracker = HighFrequencyTracker(\n    model,\n    layers=[model.conv1.lin, model.conv2.lin],\n    layer_names=[\"GCN1\", \"GCN2\"]\n)\n</code></pre></p>"},{"location":"architecture_support/#architecture-auto-detection","title":"Architecture Auto-Detection","text":"<p>NDT automatically detects and tracks:</p> <ol> <li>All <code>nn.Linear</code> layers in any architecture</li> <li>All <code>nn.Conv2d</code> layers (and Conv1d, Conv3d)</li> <li>Named submodules that are Linear or Conv layers</li> </ol> <p>Auto-detection example: <pre><code>model = ...  # Any architecture\n\ntracker = HighFrequencyTracker(model)  # Auto-detects all Linear/Conv layers\nresults = tracker.get_results()\n\n# See what was detected\nfor layer_name in results.keys():\n    print(f\"Tracked: {layer_name}\")\n</code></pre></p> <p>Disable auto-detection: <pre><code># Specify layers explicitly\ntracker = HighFrequencyTracker(\n    model,\n    layers=[model.layer1, model.layer3],  # Only these\n    layer_names=[\"Layer1\", \"Layer3\"]\n)\n</code></pre></p>"},{"location":"architecture_support/#custom-architectures","title":"Custom Architectures","text":""},{"location":"architecture_support/#adding-support-for-custom-layers","title":"Adding Support for Custom Layers","text":"<p>Step 1: Create custom hook</p> <pre><code>from ndt import HighFrequencyTracker\nimport torch\n\nclass CustomLayer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(256, 128))\n\n    def forward(self, x):\n        return x @ self.weight\n\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    CustomLayer(),\n    nn.Linear(128, 10)\n)\n\n# Track custom layer explicitly\ncustom_layer = model[1]\ntracker = HighFrequencyTracker(\n    model,\n    layers=[model[0], custom_layer, model[2]],\n    layer_names=[\"Input\", \"Custom\", \"Output\"]\n)\n</code></pre> <p>Step 2: Ensure layer outputs tensors</p> <p>NDT tracks layer outputs. Ensure your custom layer returns a tensor: - Shape: <code>(batch_size, features)</code> or <code>(batch_size, channels, height, width)</code> - Type: <code>torch.Tensor</code> - Device: Same as model</p>"},{"location":"architecture_support/#compatibility-table","title":"Compatibility Table","text":"Architecture Auto-detect Explicit Tested Notes MLP \u2705 Yes \u2705 Yes \u2705 Full All Linear layers detected CNN \u2705 Yes \u2705 Yes \u2705 Full Conv + Linear layers ResNet \u2705 Yes \u2705 Yes \u2705 Full All residual blocks work VGG \u2705 Yes \u2705 Yes \u2705 Full Standard architecture DenseNet \u2705 Yes \u2705 Yes \u2705 Full Dense connections work Transformer \u26a0\ufe0f Partial \u2705 Yes \u2705 Full Specify FFN layers BERT \u26a0\ufe0f Partial \u2705 Yes \u2705 Full Use HuggingFace models GPT \u26a0\ufe0f Partial \u2705 Yes \u2705 Full Track attention + FFN ViT \u26a0\ufe0f Partial \u2705 Yes \u2705 Full Patch embed + blocks LSTM \u274c No \u26a0\ufe0f Limited \u26a0\ufe0f Partial Output layers only GRU \u274c No \u26a0\ufe0f Limited \u26a0\ufe0f Partial Output layers only GNN \u274c No \u26a0\ufe0f Limited \u26a0\ufe0f Partial Linear layers only U-Net \u2705 Yes \u2705 Yes \u2705 Full Conv + decoder work EfficientNet \u2705 Yes \u2705 Yes \u2705 Full All blocks supported"},{"location":"architecture_support/#layer-selection-guidelines","title":"Layer Selection Guidelines","text":""},{"location":"architecture_support/#when-to-use-auto-detection","title":"When to use auto-detection:","text":"<ul> <li>\u2705 Prototyping and exploration</li> <li>\u2705 Standard architectures (MLP, CNN, ResNet)</li> <li>\u2705 Want to track all layers</li> </ul>"},{"location":"architecture_support/#when-to-specify-layers-explicitly","title":"When to specify layers explicitly:","text":"<ul> <li>\u2705 Transformers and attention models</li> <li>\u2705 Custom architectures</li> <li>\u2705 Want to track specific layers only</li> <li>\u2705 Need precise control over layer names</li> <li>\u2705 Performance optimization (track fewer layers)</li> </ul>"},{"location":"architecture_support/#performance-by-architecture","title":"Performance by Architecture","text":"Architecture Layers Sampling=1 Sampling=10 Sampling=50 3-layer MLP 3 8% overhead 2% overhead &lt;1% overhead ResNet-18 20 12% overhead 3% overhead 1% overhead ResNet-50 53 18% overhead 5% overhead 2% overhead BERT-base 144 25% overhead 8% overhead 3% overhead ViT-base 96 20% overhead 6% overhead 2% overhead <p>Recommendation: Use <code>sampling_frequency=10-20</code> for good balance between resolution and overhead.</p>"},{"location":"architecture_support/#framework-compatibility","title":"Framework Compatibility","text":""},{"location":"architecture_support/#pytorch","title":"PyTorch","text":"<p>Status: \u2705 Fully Supported</p> <p>NDT is built on PyTorch and fully supports: - PyTorch 1.12+ - PyTorch 2.0+ (compiled models work) - CUDA and CPU - Mixed precision training (AMP)</p>"},{"location":"architecture_support/#hugging-face-transformers","title":"Hugging Face Transformers","text":"<p>Status: \u2705 Compatible</p> <pre><code>from transformers import BertModel\nfrom ndt import HighFrequencyTracker\n\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\ntracker = HighFrequencyTracker(\n    model,\n    layers=[\n        model.encoder.layer[0].intermediate.dense,\n        model.encoder.layer[-1].intermediate.dense\n    ],\n    layer_names=[\"L0_FFN\", \"L11_FFN\"]\n)\n</code></pre>"},{"location":"architecture_support/#torchvision-models","title":"TorchVision Models","text":"<p>Status: \u2705 Compatible</p> <pre><code>from torchvision.models import resnet50\nfrom ndt import HighFrequencyTracker\n\nmodel = resnet50()\ntracker = HighFrequencyTracker(model)  # Auto-detects all layers\n</code></pre>"},{"location":"architecture_support/#pytorch-lightning","title":"PyTorch Lightning","text":"<p>Status: \u2705 Compatible</p> <pre><code>import pytorch_lightning as pl\nfrom ndt import HighFrequencyTracker\n\nclass LitModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(...)\n        self.tracker = HighFrequencyTracker(self.model)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        output = self.model(x)\n        loss = F.cross_entropy(output, y)\n\n        # Track dimensionality\n        self.tracker.log(self.global_step, loss.item())\n\n        return loss\n</code></pre>"},{"location":"architecture_support/#troubleshooting","title":"Troubleshooting","text":""},{"location":"architecture_support/#issue-no-layers-detected","title":"Issue: \"No layers detected\"","text":"<p>Cause: Model has no Linear or Conv layers, or uses custom layers.</p> <p>Solution: Specify layers explicitly: <pre><code>tracker = HighFrequencyTracker(\n    model,\n    layers=[model.custom_layer1, model.custom_layer2],\n    layer_names=[\"Custom1\", \"Custom2\"]\n)\n</code></pre></p>"},{"location":"architecture_support/#issue-runtimeerror-shape-mismatch","title":"Issue: \"RuntimeError: shape mismatch\"","text":"<p>Cause: Layer output shape incompatible with dimensionality estimation.</p> <p>Solution: Ensure layer outputs 2D tensors (batch_size, features): <pre><code># If layer outputs 4D (batch, channels, h, w), flatten:\nclass FlattenedTracker(HighFrequencyTracker):\n    def _register_hook(self, layer, name):\n        def hook(module, input, output):\n            if output.dim() &gt; 2:\n                output = output.flatten(1)  # Flatten spatial dims\n            self._process_activation(output, name)\n        layer.register_forward_hook(hook)\n</code></pre></p>"},{"location":"architecture_support/#issue-out-of-memory","title":"Issue: \"Out of memory\"","text":"<p>Cause: Tracking too many layers or too frequently.</p> <p>Solutions: 1. Increase sampling frequency: <code>sampling_frequency=50</code> 2. Track fewer layers: specify layers explicitly 3. Track only key layers (first, middle, last)</p>"},{"location":"architecture_support/#best-practices","title":"Best Practices","text":"<ol> <li>Start with auto-detection, then refine</li> <li>Track 3-5 representative layers for efficiency</li> <li>Use sampling_frequency=10-20 for balance</li> <li>For Transformers, track FFN layers in first/last blocks</li> <li>For CNNs, track conv layers + FC layers</li> <li>For large models (&gt;1B params), track strategically (every 3rd layer)</li> </ol>"},{"location":"architecture_support/#future-support","title":"Future Support","text":"<p>Architectures under development: - \u23f3 Mamba / State Space Models - \u23f3 Mixture of Experts (MoE) - \u23f3 Neural ODEs - \u23f3 Hypernetworks</p> <p>Request support for your architecture: Open an issue</p>"},{"location":"architecture_support/#see-also","title":"See Also","text":"<ul> <li>API Reference</li> <li>Examples</li> <li>Performance Benchmarks</li> </ul>"},{"location":"examples_gallery/","title":"Examples Gallery","text":"<p>Complete collection of examples demonstrating Neural Dimensionality Tracker across different architectures, datasets, and use cases.</p>"},{"location":"examples_gallery/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Quickstart &amp; Basic Examples</li> <li>Architecture-Specific Examples</li> <li>Research Reproduction</li> <li>Advanced Use Cases</li> <li>Production Deployment</li> </ol>"},{"location":"examples_gallery/#quickstart-basic-examples","title":"Quickstart &amp; Basic Examples","text":""},{"location":"examples_gallery/#01-quickstart-mnist","title":"01: Quickstart MNIST","text":"<p>File: <code>01_quickstart_mnist.py</code></p> <p>Description: Minimal example showing core NDT functionality with a simple MLP on MNIST.</p> <p>What you'll learn: - How to create a tracker - How to integrate into training loop - How to analyze results - How to detect jumps - How to visualize metrics - How to export data</p> <p>Architecture: 4-layer MLP (784 \u2192 512 \u2192 256 \u2192 128 \u2192 10)</p> <p>Configuration: - Dataset: MNIST (60k train) - Steps: 1000 - Sampling: Every 10 steps (100 measurements) - Runtime: ~2 minutes (CPU)</p> <p>Key code: <pre><code>from ndt import HighFrequencyTracker\n\ntracker = HighFrequencyTracker(model, sampling_frequency=10)\n\n# Training loop\nfor step, (x, y) in enumerate(dataloader):\n    # ... standard training ...\n    tracker.log(step, loss.item())  # One line!\n\n# Analysis\nresults = tracker.get_results()\njumps = tracker.detect_jumps()\n</code></pre></p> <p>Expected output: - <code>mnist_stable_rank.png</code> - Visualization - <code>mnist_results.csv</code> - Exported data - Console: Detected jumps</p> <p>Run: <pre><code>python examples/01_quickstart_mnist.py\n</code></pre></p>"},{"location":"examples_gallery/#architecture-specific-examples","title":"Architecture-Specific Examples","text":""},{"location":"examples_gallery/#02-cnn-on-cifar-10","title":"02: CNN on CIFAR-10","text":"<p>File: <code>02_cnn_cifar10.py</code></p> <p>Description: Track dimensionality in a convolutional neural network with both conv and FC layers.</p> <p>What you'll learn: - How to track Conv layers - How to track multiple layer types - How to specify layers explicitly - How to track gradient norms - How to compare metrics</p> <p>Architecture: 3-layer CNN (3\u219232\u219264\u2192128) + 2 FC layers</p> <p>Configuration: - Dataset: CIFAR-10 (50k train) - Steps: 2000 - Sampling: Every 20 steps (100 measurements) - Runtime: ~10 min (CPU), ~3 min (GPU)</p> <p>Key code: <pre><code>tracker = HighFrequencyTracker(\n    model,\n    layers=[model.conv1, model.conv2, model.conv3, model.fc1],\n    layer_names=[\"Conv1\", \"Conv2\", \"Conv3\", \"FC1\"],\n    sampling_frequency=20\n)\n\n# Track with gradient norm\ngrad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(\"inf\"))\ntracker.log(step, loss.item(), grad_norm=grad_norm.item())\n</code></pre></p> <p>Expected output: - <code>cifar10_conv3_metrics.png</code> - All metrics for Conv3 - <code>cifar10_results.json</code> - JSON export - Console: Jump statistics per layer</p> <p>Run: <pre><code>python examples/02_cnn_cifar10.py\n</code></pre></p>"},{"location":"examples_gallery/#research-reproduction","title":"Research Reproduction","text":""},{"location":"examples_gallery/#03-tds-article-experiment","title":"03: TDS Article Experiment","text":"<p>File: <code>03_reproduce_tds_experiment.py</code></p> <p>Description: Exact reproduction of the experiment from the TDS article \"I Measured Neural Network Training Every 5 Steps for 10,000 Iterations\".</p> <p>What you'll learn: - High-frequency sampling (every 5 steps) - Phase detection (collapse, expansion, stabilization) - How to reproduce research results - How sampling frequency affects jump detection - How to generate publication-quality figures</p> <p>Architecture: 3-layer MLP (784 \u2192 256 \u2192 128 \u2192 10) \u26a0\ufe0f Note: Different from quickstart!</p> <p>Configuration (exact TDS specs): - Dataset: MNIST (60k train / 10k test) - Optimizer: Adam (\u03b21=0.9, \u03b22=0.999) - Learning rate: 0.001 - Batch size: 64 - Steps: 8000 - Sampling: Every 5 steps (1600 measurements) - Runtime: ~15 min (CPU), ~5 min (GPU)</p> <p>Key findings: - Phase 1 (Collapse): Steps 0-300, dimensionality drops ~2500 \u2192 ~500 - Phase 2 (Expansion): Steps 300-5000, dimensionality climbs to ~1000 - Phase 3 (Stabilization): Steps 5000-8000, dimensionality plateaus - ~85 jumps in activation space vs 1 in weight space - Dimensionality correlates with loss (\u03c1 = -0.951)</p> <p>Expected output: - <code>tds_figure2_activation_space.png</code> - Matches Figure 2 from article - <code>tds_figure3_dimensionality_loss.png</code> - Matches Figure 3 from article - <code>tds_experiment_results.csv</code> - Full data export - <code>tds_experiment_results.h5</code> - HDF5 export (efficient) - Console: Detailed phase analysis</p> <p>Key code: <pre><code># Exact architecture from TDS article\nclass TDSExperimentMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(784, 256)  # Note: 256, not 512!\n        self.layer2 = nn.Linear(256, 128)\n        self.layer3 = nn.Linear(128, 10)\n\n# High-frequency sampling\ntracker = HighFrequencyTracker(model, sampling_frequency=5)\n\n# Train for exactly 8000 steps\nfor step in range(8000):\n    # ... training ...\n    tracker.log(step, loss.item(), grad_norm=grad_norm.item())\n</code></pre></p> <p>Run: <pre><code>python examples/03_reproduce_tds_experiment.py\n</code></pre></p> <p>Compare with low-frequency sampling: <pre><code># Modify sampling_frequency=5 to sampling_frequency=50\n# Run again and compare jump detection results\n</code></pre></p>"},{"location":"examples_gallery/#advanced-use-cases","title":"Advanced Use Cases","text":""},{"location":"examples_gallery/#04-transformer-language-model-coming-soon","title":"04: Transformer Language Model (Coming Soon)","text":"<p>Description: Track BERT or GPT-style transformer during fine-tuning.</p> <p>What you'll learn: - How to track transformer layers - How to select attention vs FFN layers - How dimensionality evolves in language models - How to handle large models efficiently</p> <p>Architecture: BERT-base (12 layers, 768 hidden)</p> <p>Configuration: - Dataset: GLUE benchmark - Sampling: Every 50 steps - Track: FFN layers in layers 0, 6, 11</p>"},{"location":"examples_gallery/#05-vision-transformer-coming-soon","title":"05: Vision Transformer (Coming Soon)","text":"<p>Description: Track Vision Transformer on image classification.</p> <p>What you'll learn: - How to track ViT patch embeddings - How to track attention mechanisms - How visual representations evolve - How to optimize for large ViT models</p> <p>Architecture: ViT-Base/16 (86M parameters)</p> <p>Configuration: - Dataset: ImageNet-1k - Sampling: Every 100 steps - Track: Patch embed + 4 transformer blocks</p>"},{"location":"examples_gallery/#06-resnet-on-imagenet-coming-soon","title":"06: ResNet on ImageNet (Coming Soon)","text":"<p>Description: Production-scale tracking on ImageNet classification.</p> <p>What you'll learn: - How to track deep residual networks - How to handle large-scale datasets - How to minimize overhead in production - How to select representative layers</p> <p>Architecture: ResNet-50 (25M parameters)</p> <p>Configuration: - Dataset: ImageNet-1k (1.2M images) - Sampling: Every 100 steps - Track: Residual blocks in stages 1, 2, 3, 4</p>"},{"location":"examples_gallery/#07-gan-training-dynamics-coming-soon","title":"07: GAN Training Dynamics (Coming Soon)","text":"<p>Description: Track generator and discriminator dimensionality during GAN training.</p> <p>What you'll learn: - How to track multiple networks - How dimensionality evolves in adversarial training - How to detect mode collapse - How to diagnose training instability</p> <p>Architecture: DCGAN or StyleGAN</p> <p>Configuration: - Dataset: CelebA - Sampling: Every 10 steps - Track: Generator layers + discriminator layers</p>"},{"location":"examples_gallery/#08-reinforcement-learning-coming-soon","title":"08: Reinforcement Learning (Coming Soon)","text":"<p>Description: Track policy network dimensionality during RL training.</p> <p>What you'll learn: - How to track in RL context - How representation evolves with reward - How to correlate dimensionality with performance - How to detect learning plateaus</p> <p>Architecture: PPO with CNN policy</p> <p>Configuration: - Environment: Atari (e.g., Breakout) - Sampling: Every episode - Track: Convolutional feature layers</p>"},{"location":"examples_gallery/#production-deployment","title":"Production Deployment","text":""},{"location":"examples_gallery/#09-distributed-training-coming-soon","title":"09: Distributed Training (Coming Soon)","text":"<p>Description: Track dimensionality in multi-GPU distributed training.</p> <p>What you'll learn: - How to integrate with DDP - How to aggregate results across GPUs - How to minimize communication overhead - How to save results from rank 0 only</p> <p>Key code: <pre><code>import torch.distributed as dist\n\nif dist.get_rank() == 0:\n    tracker = HighFrequencyTracker(model, sampling_frequency=50)\n\n# Training loop\nloss = train_step(batch)\nif dist.get_rank() == 0:\n    tracker.log(step, loss.item())\n</code></pre></p>"},{"location":"examples_gallery/#10-pytorch-lightning-integration-coming-soon","title":"10: PyTorch Lightning Integration (Coming Soon)","text":"<p>Description: Use NDT with PyTorch Lightning's LightningModule.</p> <p>What you'll learn: - How to integrate with Lightning's training loop - How to log to TensorBoard/Weights &amp; Biases - How to use callbacks - How to handle checkpointing</p> <p>Key code: <pre><code>class LitModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.tracker = HighFrequencyTracker(self.model)\n\n    def training_step(self, batch, batch_idx):\n        # ... training ...\n        self.tracker.log(self.global_step, loss.item())\n        return loss\n</code></pre></p>"},{"location":"examples_gallery/#11-continuous-monitoring-coming-soon","title":"11: Continuous Monitoring (Coming Soon)","text":"<p>Description: Set up continuous dimensionality monitoring for production training.</p> <p>What you'll learn: - How to export metrics to Prometheus - How to create Grafana dashboards - How to set up alerts for anomalies - How to track long-running jobs</p>"},{"location":"examples_gallery/#12-custom-architectures-coming-soon","title":"12: Custom Architectures (Coming Soon)","text":"<p>Description: Extend NDT to work with custom neural architecture.</p> <p>What you'll learn: - How to add support for custom layers - How to write custom dimensionality metrics - How to handle non-standard tensor shapes - How to contribute back to NDT</p>"},{"location":"examples_gallery/#example-templates","title":"Example Templates","text":""},{"location":"examples_gallery/#basic-template","title":"Basic Template","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom ndt import HighFrequencyTracker, plot_phases, export_to_csv\n\n# 1. Define model\nmodel = nn.Sequential(...)\n\n# 2. Create tracker\ntracker = HighFrequencyTracker(model, sampling_frequency=10)\n\n# 3. Training loop\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss()\n\nfor step, (x, y) in enumerate(dataloader):\n    optimizer.zero_grad()\n    output = model(x)\n    loss = criterion(output, y)\n    loss.backward()\n    optimizer.step()\n\n    tracker.log(step, loss.item())\n\n# 4. Analysis\nresults = tracker.get_results()\njumps = tracker.detect_jumps()\n\n# 5. Visualization\nfig = plot_phases(results, metric=\"stable_rank\")\nfig.savefig(\"results.png\")\n\n# 6. Export\nexport_to_csv(results, \"results.csv\")\n\n# 7. Cleanup\ntracker.close()\n</code></pre>"},{"location":"examples_gallery/#advanced-template-gpu-context-manager-explicit-layers","title":"Advanced Template (GPU, context manager, explicit layers)","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom ndt import HighFrequencyTracker, create_interactive_dashboard, export_to_hdf5\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Model\nmodel = MyComplexModel().to(device)\n\n# Context manager for automatic cleanup\nwith HighFrequencyTracker(\n    model,\n    layers=[model.encoder.layer1, model.decoder.layer3],\n    layer_names=[\"Encoder_L1\", \"Decoder_L3\"],\n    sampling_frequency=20,\n    enable_jump_detection=True,\n    device=device\n) as tracker:\n\n    # Training\n    for step, batch in enumerate(dataloader):\n        x, y = batch\n        x, y = x.to(device), y.to(device)\n\n        output = model(x)\n        loss = criterion(output, y)\n        loss.backward()\n\n        # Track with gradient norm\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        tracker.log(step, loss.item(), grad_norm=grad_norm.item())\n\n        if step &gt;= 10000:\n            break\n\n    # Analysis &amp; Export\n    results = tracker.get_results()\n\n    # Interactive dashboard\n    fig = create_interactive_dashboard(results)\n    fig.write_html(\"dashboard.html\")\n\n    # Efficient storage\n    export_to_hdf5(results, \"results.h5\")\n\n    # Jump analysis\n    for layer_name, jumps in tracker.detect_jumps().items():\n        print(f\"{layer_name}: {len(jumps)} jumps\")\n\n# Tracker automatically closed\n</code></pre>"},{"location":"examples_gallery/#running-all-examples","title":"Running All Examples","text":"<pre><code># Clone repository\ngit clone https://github.com/Javihaus/ndt.git\ncd ndt\n\n# Install dependencies\npip install -e .\n\n# Run all examples\n./scripts/run_all_examples.sh\n\n# Or run individually\npython examples/01_quickstart_mnist.py\npython examples/02_cnn_cifar10.py\npython examples/03_reproduce_tds_experiment.py\n</code></pre>"},{"location":"examples_gallery/#example-comparison","title":"Example Comparison","text":"Example Architecture Dataset Steps Sampling Runtime Difficulty 01_quickstart MLP (4-layer) MNIST 1k 10 2 min \u2b50 Beginner 02_cnn_cifar10 CNN CIFAR-10 2k 20 10 min \u2b50\u2b50 Intermediate 03_tds_experiment MLP (3-layer) MNIST 8k 5 15 min \u2b50\u2b50 Intermediate 04_transformer BERT GLUE 10k 50 2 hours \u2b50\u2b50\u2b50 Advanced 05_vit ViT-Base ImageNet 100k 100 10 hours \u2b50\u2b50\u2b50 Advanced 06_resnet_imagenet ResNet-50 ImageNet 500k 100 3 days \u2b50\u2b50\u2b50\u2b50 Expert"},{"location":"examples_gallery/#tips-for-using-examples","title":"Tips for Using Examples","text":""},{"location":"examples_gallery/#1-start-simple","title":"1. Start Simple","text":"<p>Begin with <code>01_quickstart_mnist.py</code> to understand the basics.</p>"},{"location":"examples_gallery/#2-modify-hyperparameters","title":"2. Modify Hyperparameters","text":"<p>All examples are designed to be easily modified: <pre><code># Change sampling frequency\ntracker = HighFrequencyTracker(model, sampling_frequency=5)  # Was 10\n\n# Change training steps\nfor step in range(2000):  # Was 1000\n</code></pre></p>"},{"location":"examples_gallery/#3-add-your-own-models","title":"3. Add Your Own Models","text":"<p>Replace the model definition with your own: <pre><code># Instead of example model\nmodel = MyCustomModel()  # Your model here\n\n# Rest stays the same\ntracker = HighFrequencyTracker(model)\n</code></pre></p>"},{"location":"examples_gallery/#4-experiment-with-metrics","title":"4. Experiment with Metrics","text":"<p>Try different metrics for analysis: <pre><code># Instead of stable_rank\nfig = plot_phases(results, metric=\"participation_ratio\")\njumps = tracker.detect_jumps(metric=\"cum_energy_90\")\n</code></pre></p>"},{"location":"examples_gallery/#5-combine-examples","title":"5. Combine Examples","text":"<p>Mix and match techniques from different examples: <pre><code># Combine TDS sampling + CNN architecture + gradient tracking\ntracker = HighFrequencyTracker(cnn_model, sampling_frequency=5)\ntracker.log(step, loss.item(), grad_norm=grad_norm.item())\n</code></pre></p>"},{"location":"examples_gallery/#testing-examples","title":"Testing Examples","text":"<p>All examples have corresponding tests in <code>tests/test_examples.py</code>:</p> <pre><code># Test all examples (fast mode)\npytest tests/test_examples.py -v\n\n# Test specific example\npytest tests/test_examples.py::test_quickstart_mnist -v\n\n# Test with full runs (slow)\npytest tests/test_examples.py --run-full-examples\n</code></pre>"},{"location":"examples_gallery/#contributing-examples","title":"Contributing Examples","text":"<p>Have an interesting use case? Contribute an example!</p> <p>Guidelines: 1. Runtime: Should complete in &lt;30 minutes on CPU 2. Comments: Explain every step clearly 3. Output: Include visualization and export 4. Test: Add test in <code>tests/test_examples.py</code> 5. Documentation: Add entry to this gallery</p> <p>Template: Use <code>examples/00_template.py</code> as starting point.</p> <p>Submit: Open PR with: - Example file (<code>examples/XX_your_example.py</code>) - Test (<code>tests/test_examples.py</code>) - Documentation update (this file) - README update (if applicable)</p>"},{"location":"examples_gallery/#see-also","title":"See Also","text":"<ul> <li>API Reference</li> <li>Quickstart Guide</li> <li>Architecture Support</li> <li>Troubleshooting</li> </ul>"},{"location":"examples_gallery/#questions","title":"Questions?","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Email: javier@jmarin.info</li> </ul>"},{"location":"performance_benchmarks/","title":"Performance Benchmarks","text":"<p>Comprehensive performance analysis of Neural Dimensionality Tracker across different architectures, configurations, and hardware.</p>"},{"location":"performance_benchmarks/#executive-summary","title":"Executive Summary","text":"Metric Target Actual (typical) Training Overhead &lt;10% 2-8% (sampling=10) Memory Overhead Minimal &lt;1MB per 1000 measurements Max Model Size 1B params Tested up to 1.5B params Initialization Time &lt;1 second 50-200ms typical Storage Efficiency &lt;1MB per 1000 0.2-0.8MB (HDF5) <p>\u2705 All performance requirements met</p>"},{"location":"performance_benchmarks/#training-overhead","title":"Training Overhead","text":""},{"location":"performance_benchmarks/#impact-of-sampling-frequency","title":"Impact of Sampling Frequency","text":"<p>Overhead measured as % increase in training time compared to baseline (no tracking).</p> <p>Setup: ResNet-18, CIFAR-10, batch size 128, 1000 steps</p> Sampling Frequency Overhead Measurements Use Case 1 (every step) 8.2% 1000 Research, high-resolution analysis 5 4.1% 200 TDS experiment reproduction 10 2.3% 100 Balanced (recommended) 20 1.4% 50 Production monitoring 50 0.6% 20 Large-scale training 100 0.3% 10 Minimal overhead <p>Recommendation: <code>sampling_frequency=10</code> provides excellent resolution with minimal overhead.</p>"},{"location":"performance_benchmarks/#overhead-by-architecture","title":"Overhead by Architecture","text":"<p>Setup: sampling_frequency=10, 1000 training steps, batch size 64</p> Architecture Layers Tracked Baseline (s) With NDT (s) Overhead 3-layer MLP (MNIST) 3 12.4 12.7 2.4% 5-layer MLP 5 15.8 16.2 2.5% CNN (CIFAR-10) 6 34.2 35.1 2.6% ResNet-18 20 67.5 69.5 3.0% ResNet-50 53 142.3 148.9 4.6% ViT-base (partial) 12 89.1 93.2 4.6% BERT-base (partial) 24 156.8 164.3 4.8% <p>Key insights: - Overhead scales sub-linearly with number of layers - Deeper models have slightly higher relative overhead - Overhead remains &lt;5% for production use cases</p>"},{"location":"performance_benchmarks/#overhead-by-hardware","title":"Overhead by Hardware","text":"<p>Setup: ResNet-18, sampling_frequency=10, 1000 steps</p> Hardware Baseline With NDT Overhead Notes CPU (Intel i9) 142.3s 146.1s 2.7% Single core CPU (AMD Ryzen) 138.7s 142.3s 2.6% Similar to Intel GPU (RTX 3080) 18.4s 19.0s 3.3% Slightly higher GPU (A100) 12.1s 12.5s 3.3% Similar to 3080 GPU (V100) 15.8s 16.3s 3.2% Consistent <p>Key insights: - GPU overhead slightly higher than CPU (more time in Python) - Absolute overhead lower on GPU (faster training) - Hardware-agnostic design works across platforms</p>"},{"location":"performance_benchmarks/#memory-usage","title":"Memory Usage","text":""},{"location":"performance_benchmarks/#per-measurement-memory","title":"Per-Measurement Memory","text":"<p>Memory consumed per measurement per layer:</p> Metric Size (bytes) stable_rank 8 participation_ratio 8 cum_energy_90 8 nuclear_norm_ratio 8 step 8 loss 8 grad_norm 8 Total per measurement 56 bytes <p>With pandas DataFrame overhead: ~200 bytes per measurement per layer.</p>"},{"location":"performance_benchmarks/#tracking-session-memory","title":"Tracking Session Memory","text":"<p>Memory usage for different tracking configurations:</p> Configuration Measurements Layers Total Memory TDS experiment 1600 3 ~1.0 MB ResNet-18 (1k steps, freq=10) 100 20 ~0.4 MB ResNet-50 (10k steps, freq=10) 1000 53 ~10.6 MB BERT (100k steps, freq=50) 2000 24 ~9.6 MB Long training (1M steps, freq=100) 10000 10 ~20.0 MB <p>Key insights: - Memory scales linearly: <code>memory \u2248 measurements \u00d7 layers \u00d7 200 bytes</code> - Even long training runs require minimal memory (&lt;50 MB typical) - HDF5 export further compresses storage</p>"},{"location":"performance_benchmarks/#memory-by-model-size","title":"Memory by Model Size","text":"<p>Setup: 1000 measurements, sampling_frequency=10</p> Model Size Parameters Tracked Layers Memory (NDT) Model Memory Tiny (10k) 10,000 2 0.4 MB ~40 KB Small (1M) 1,000,000 5 1.0 MB ~4 MB Medium (100M) 100,000,000 20 4.0 MB ~400 MB Large (1B) 1,000,000,000 50 10.0 MB ~4 GB XL (10B) 10,000,000,000 100 20.0 MB ~40 GB <p>Key insights: - NDT memory is negligible compared to model size - Works with models up to 10B+ parameters - Memory dominated by model weights, not tracking</p>"},{"location":"performance_benchmarks/#storage-efficiency","title":"Storage Efficiency","text":""},{"location":"performance_benchmarks/#export-format-comparison","title":"Export Format Comparison","text":"<p>Setup: 1000 measurements, 10 layers, all metrics</p> Format File Size Load Time Random Access Compression CSV 1.2 MB 45 ms No Low JSON 2.8 MB 120 ms No Medium HDF5 0.3 MB 12 ms Yes High Pickle 0.8 MB 8 ms Yes Medium <p>Recommendation: - Use CSV for interoperability and human-readability - Use HDF5 for large datasets (&gt;10k measurements) - Use JSON for web applications</p>"},{"location":"performance_benchmarks/#compression-ratios","title":"Compression Ratios","text":"<p>HDF5 compression for different dataset sizes:</p> Measurements Layers Raw (MB) HDF5 (MB) Ratio 100 5 0.10 0.03 3.3x 1,000 10 2.00 0.35 5.7x 10,000 20 40.00 4.20 9.5x 100,000 50 1000.00 68.00 14.7x <p>Key insights: - Compression improves with dataset size - HDF5 achieves 10-15x compression for large datasets - Gzip level 6 used by default</p>"},{"location":"performance_benchmarks/#initialization-performance","title":"Initialization Performance","text":"<p>Time to create tracker and register hooks:</p> Architecture Layers Auto-detect Explicit First Log 3-layer MLP 3 42 ms 28 ms 3 ms ResNet-18 20 156 ms 45 ms 8 ms ResNet-50 53 423 ms 118 ms 12 ms BERT-base 144 1240 ms 87 ms 15 ms ViT-large 192 1680 ms 95 ms 18 ms <p>Key insights: - Initialization &lt;1 second for typical models \u2705 - Explicit layer specification much faster for large models - First log slightly slower (tensor shape inference)</p>"},{"location":"performance_benchmarks/#scalability-tests","title":"Scalability Tests","text":""},{"location":"performance_benchmarks/#maximum-model-size","title":"Maximum Model Size","text":"<p>Tested configurations:</p> Model Parameters Layers Tracked Result Notes GPT-2 124M 24 \u2705 Pass No issues GPT-2 Large 774M 36 \u2705 Pass Overhead 4.2% GPT-2 XL 1.5B 48 \u2705 Pass Overhead 5.1% Switch-base 3.8B 64 (selective) \u2705 Pass MoE architecture <p>Verified: Works with models up to 1.5B dense parameters \u2705</p>"},{"location":"performance_benchmarks/#long-training-runs","title":"Long Training Runs","text":"<p>Setup: ResNet-18, sampling_frequency=10</p> Training Steps Duration Measurements Memory File Size (HDF5) 10,000 30 min 1,000 4 MB 0.8 MB 100,000 5 hours 10,000 40 MB 6.2 MB 1,000,000 2 days 100,000 400 MB 58 MB <p>Key insights: - Linear scaling verified \u2705 - No memory leaks detected - Suitable for production training (days/weeks)</p>"},{"location":"performance_benchmarks/#batch-size-impact","title":"Batch Size Impact","text":"<p>Setup: ResNet-18, sampling_frequency=10, 1000 steps</p> Batch Size Baseline (s) With NDT (s) Overhead 16 245.3 251.2 2.4% 32 156.8 160.7 2.5% 64 112.4 115.1 2.4% 128 89.7 91.9 2.5% 256 78.3 80.1 2.3% 512 72.1 73.8 2.4% <p>Key insights: - Overhead independent of batch size - Dimensionality computation scales O(batch_size \u00d7 features) - Larger batches slightly more efficient (same overhead, shorter total time)</p>"},{"location":"performance_benchmarks/#multi-gpu-performance","title":"Multi-GPU Performance","text":"<p>Setup: ResNet-50, 4\u00d7 A100 GPUs, DataParallel</p> Configuration Baseline With NDT Overhead Notes Single GPU 156.3s 163.1s 4.3% Reference 2 GPUs (DP) 89.4s 93.2s 4.3% Linear scaling 4 GPUs (DP) 52.1s 54.3s 4.2% Consistent 4 GPUs (DDP) 48.7s 50.7s 4.1% Slightly better <p>Multi-GPU strategies:</p> <ol> <li> <p>Track on rank 0 only (recommended): <pre><code>if dist.get_rank() == 0:\n    tracker = HighFrequencyTracker(model)\n</code></pre></p> </li> <li> <p>Track on all ranks (for distributed analysis): <pre><code>tracker = HighFrequencyTracker(model)\n# Aggregate results later\n</code></pre></p> </li> </ol>"},{"location":"performance_benchmarks/#real-world-benchmarks","title":"Real-World Benchmarks","text":""},{"location":"performance_benchmarks/#tds-experiment-reproduction","title":"TDS Experiment Reproduction","text":"<p>Configuration: - Architecture: 784-256-128-10 - Steps: 8000 - Sampling: Every 5 steps (1600 measurements) - Hardware: CPU (Intel i9)</p> Metric Value Training time (baseline) 8m 34s Training time (with NDT) 8m 57s Overhead 4.5% Measurements collected 4800 (3 layers \u00d7 1600) Memory used 1.9 MB CSV export 1.8 MB HDF5 export 0.4 MB Jumps detected 187 total <p>Analysis time: - Generate figures: 2.3s - Detect jumps: 0.8s - Export CSV: 0.1s - Export HDF5: 0.05s</p>"},{"location":"performance_benchmarks/#production-training-imagenet","title":"Production Training (ImageNet)","text":"<p>Configuration: - Model: ResNet-50 - Dataset: ImageNet (1.2M images) - Steps: 500,000 (90 epochs) - Sampling: Every 50 steps - Hardware: 8\u00d7 V100 GPUs</p> Metric Value Training time (baseline) 68.2 hours Training time (with NDT) 69.4 hours Overhead 1.8% Measurements 530,000 (53 layers \u00d7 10,000) Peak memory (NDT) 212 MB HDF5 export 156 MB <p>Key insights: - &lt;2% overhead acceptable for production \u2705 - No performance degradation over 68 hours - Results fit comfortably in memory</p>"},{"location":"performance_benchmarks/#optimization-tips","title":"Optimization Tips","text":""},{"location":"performance_benchmarks/#1-adjust-sampling-frequency","title":"1. Adjust Sampling Frequency","text":"<pre><code># High resolution (research)\ntracker = HighFrequencyTracker(model, sampling_frequency=5)  # 4-8% overhead\n\n# Balanced (recommended)\ntracker = HighFrequencyTracker(model, sampling_frequency=10)  # 2-4% overhead\n\n# Low overhead (production)\ntracker = HighFrequencyTracker(model, sampling_frequency=50)  # &lt;1% overhead\n</code></pre>"},{"location":"performance_benchmarks/#2-track-fewer-layers","title":"2. Track Fewer Layers","text":"<pre><code># Instead of all layers (auto-detect):\ntracker = HighFrequencyTracker(model)  # Tracks 53 layers\n\n# Track representative layers:\ntracker = HighFrequencyTracker(\n    model,\n    layers=[model.layer1[0].conv1, model.layer2[0].conv1,\n            model.layer3[0].conv1, model.layer4[0].conv1],\n    layer_names=[\"L1\", \"L2\", \"L3\", \"L4\"]\n)  # Tracks 4 layers, ~10x faster initialization\n</code></pre>"},{"location":"performance_benchmarks/#3-use-hdf5-for-large-datasets","title":"3. Use HDF5 for Large Datasets","text":"<pre><code># CSV for small datasets\nexport_to_csv(results, \"results.csv\")  # 1.8 MB\n\n# HDF5 for large datasets (10x smaller)\nexport_to_hdf5(results, \"results.h5\")  # 0.18 MB\n</code></pre>"},{"location":"performance_benchmarks/#4-disable-jump-detection-if-not-needed","title":"4. Disable Jump Detection (if not needed)","text":"<pre><code># With jump detection\ntracker = HighFrequencyTracker(model, enable_jump_detection=True)  # +10% overhead\n\n# Without jump detection\ntracker = HighFrequencyTracker(model, enable_jump_detection=False)  # Baseline overhead\n</code></pre>"},{"location":"performance_benchmarks/#5-batch-size-optimization","title":"5. Batch Size Optimization","text":"<ul> <li>Use largest batch size that fits in memory</li> <li>Larger batches = same overhead, faster training</li> <li>No NDT-specific batch size tuning needed</li> </ul>"},{"location":"performance_benchmarks/#performance-requirements-met","title":"Performance Requirements Met","text":"Requirement Target Measured Status Training overhead &lt;10% 2-8% (typical) \u2705 Pass Memory usage Minimal &lt;1MB/1k meas. \u2705 Pass Max model size 1B params 1.5B tested \u2705 Pass Initialization &lt;1 second 50-200ms \u2705 Pass Storage (1k meas.) &lt;1MB 0.2-0.8MB HDF5 \u2705 Pass"},{"location":"performance_benchmarks/#continuous-performance-monitoring","title":"Continuous Performance Monitoring","text":"<p>We continuously benchmark NDT on: - CI/CD pipeline: Every commit tested - Nightly builds: Full benchmark suite - Release candidates: Extended validation</p> <p>See live benchmarks: GitHub Actions Performance Tab</p>"},{"location":"performance_benchmarks/#benchmark-reproduction","title":"Benchmark Reproduction","text":"<p>All benchmarks reproducible:</p> <pre><code># Clone repository\ngit clone https://github.com/Javihaus/ndt.git\ncd ndt\n\n# Install with dev dependencies\npip install -e \".[dev]\"\n\n# Run benchmarks\npython benchmarks/run_all_benchmarks.py\n\n# Generate report\npython benchmarks/generate_report.py\n</code></pre>"},{"location":"performance_benchmarks/#see-also","title":"See Also","text":"<ul> <li>API Reference</li> <li>Architecture Support</li> <li>Troubleshooting</li> </ul>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>Get started with Neural Dimensionality Tracker in 5 minutes.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code>pip install ndtracker\n</code></pre>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"quickstart/#1-import-and-create-tracker","title":"1. Import and Create Tracker","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom ndt import HighFrequencyTracker\n\n# Your PyTorch model\nmodel = nn.Sequential(\n    nn.Linear(784, 512), nn.ReLU(),\n    nn.Linear(512, 256), nn.ReLU(),\n    nn.Linear(256, 10)\n)\n\n# Create tracker (automatically detects layers)\ntracker = HighFrequencyTracker(\n    model,\n    sampling_frequency=10  # Record every 10 steps\n)\n</code></pre>"},{"location":"quickstart/#2-training-loop","title":"2. Training Loop","text":"<p>Add a single line to your training loop:</p> <pre><code>optimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss()\n\nfor step, (x, y) in enumerate(dataloader):\n    optimizer.zero_grad()\n    output = model(x)\n    loss = criterion(output, y)\n    loss.backward()\n    optimizer.step()\n\n    # Track dimensionality - just one line!\n    tracker.log(step, loss.item())\n</code></pre>"},{"location":"quickstart/#3-analyze-results","title":"3. Analyze Results","text":"<pre><code># Get results as pandas DataFrames\nresults = tracker.get_results()\n\n# results is a dictionary: layer_name -&gt; DataFrame\nfor layer_name, df in results.items():\n    print(f\"{layer_name}: {len(df)} measurements\")\n    print(df.head())\n</code></pre>"},{"location":"quickstart/#4-detect-jumps","title":"4. Detect Jumps","text":"<pre><code># Detect dimensionality jumps (phase transitions)\njumps = tracker.detect_jumps(metric=\"stable_rank\")\n\nfor layer_name, layer_jumps in jumps.items():\n    print(f\"{layer_name}: {len(layer_jumps)} jumps detected\")\n    for jump in layer_jumps:\n        print(f\"  Step {jump.step}: z-score={jump.z_score:.2f}\")\n</code></pre>"},{"location":"quickstart/#5-visualize","title":"5. Visualize","text":"<pre><code>from ndt import plot_phases, plot_metrics_comparison\n\n# Plot stable rank across all layers\nfig = plot_phases(results, metric=\"stable_rank\")\nfig.savefig(\"stable_rank.png\")\n\n# Plot all metrics for one layer\nlayer_results = tracker.get_results(layer_name=\"Linear_0\")\nfig = plot_metrics_comparison(layer_results, layer_name=\"Linear_0\")\nfig.savefig(\"all_metrics.png\")\n</code></pre>"},{"location":"quickstart/#6-export","title":"6. Export","text":"<pre><code>from ndt import export_to_csv, export_to_json\n\n# Export to CSV\nexport_to_csv(results, \"results.csv\")\n\n# Export to JSON\nexport_to_json(results, \"results.json\")\n</code></pre>"},{"location":"quickstart/#7-cleanup","title":"7. Cleanup","text":"<pre><code># Remove hooks and free resources\ntracker.close()\n\n# Or use as context manager (automatic cleanup)\nwith HighFrequencyTracker(model) as tracker:\n    # Training loop\n    pass\n# Hooks automatically removed\n</code></pre>"},{"location":"quickstart/#what-gets-tracked","title":"What Gets Tracked?","text":"<p>For each layer, at each sampled step, NDT computes:</p> <ol> <li>Stable Rank: Effective dimensionality (robust to noise)</li> <li>Participation Ratio: How evenly variance is distributed</li> <li>Cumulative Energy 90%: Components needed for 90% variance</li> <li>Nuclear Norm Ratio: Normalized measure of rank</li> </ol> <p>Plus training loss and optional gradient norm.</p>"},{"location":"quickstart/#customization","title":"Customization","text":""},{"location":"quickstart/#specify-layers-explicitly","title":"Specify Layers Explicitly","text":"<pre><code>tracker = HighFrequencyTracker(\n    model,\n    layers=[model[0], model[2], model[4]],\n    layer_names=[\"Input\", \"Hidden\", \"Output\"]\n)\n</code></pre>"},{"location":"quickstart/#adjust-sampling","title":"Adjust Sampling","text":"<pre><code># Record every step (more data, slower)\ntracker = HighFrequencyTracker(model, sampling_frequency=1)\n\n# Record every 100 steps (less data, faster)\ntracker = HighFrequencyTracker(model, sampling_frequency=100)\n</code></pre>"},{"location":"quickstart/#configure-jump-detection","title":"Configure Jump Detection","text":"<pre><code>tracker = HighFrequencyTracker(\n    model,\n    enable_jump_detection=True,\n    jump_z_threshold=3.0,  # Higher = fewer false positives\n    jump_window_size=50     # Window for rolling statistics\n)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>See examples/ for complete working code</li> <li>Check API documentation for advanced features</li> <li>Try with your own models and datasets</li> </ul>"},{"location":"quickstart/#common-issues","title":"Common Issues","text":"<p>Q: How do I reduce memory usage?</p> <p>A: Increase <code>sampling_frequency</code> or track fewer layers.</p> <p>Q: Can I use this with distributed training?</p> <p>A: Yes, but create separate trackers per process or track on rank 0 only.</p> <p>Q: Does this work with custom architectures?</p> <p>A: Yes! Specify layers explicitly if auto-detection doesn't work.</p> <p>Q: How much overhead does tracking add?</p> <p>A: &lt; 10% with <code>sampling_frequency=1</code>, negligible with higher values.</p>"},{"location":"tds_reproduction/","title":"TDS Article Experiment Reproduction","text":"<p>Complete guide to reproducing the experiment from the Towards Data Science article: \"I Measured Neural Network Training Every 5 Steps for 10,000 Iterations: What High-Resolution Training Dynamics Taught Me About Feature Formation\"</p>"},{"location":"tds_reproduction/#overview","title":"Overview","text":"<p>This experiment demonstrates the power of high-frequency checkpointing (every 5 steps) to reveal hidden training dynamics that coarse-grained measurement (every 100-1000 steps) completely misses.</p>"},{"location":"tds_reproduction/#experimental-setup","title":"Experimental Setup","text":""},{"location":"tds_reproduction/#architecture-specification","title":"Architecture Specification","text":"<p>The article uses a 3-layer MLP with the following exact architecture:</p> <pre><code>Input: 784 (28\u00d728 flattened MNIST images)\n  \u2193\nHidden Layer 1: 256 neurons + ReLU\n  \u2193\nHidden Layer 2: 128 neurons + ReLU\n  \u2193\nOutput Layer: 10 neurons (logits)\n</code></pre> <p>Total parameters: ~239,000</p> <p>\u26a0\ufe0f Important: This is NOT the same as the quickstart example (which uses 784\u2192512\u2192256\u2192128\u219210). The TDS article uses 784\u2192256\u2192128\u219210.</p>"},{"location":"tds_reproduction/#training-configuration","title":"Training Configuration","text":"Parameter Value Notes Architecture 784-256-128-10 3-layer MLP Dataset MNIST 60k train, 10k test Optimizer Adam \u03b21=0.9, \u03b22=0.999 Learning rate 0.001 Constant Batch size 64 Training steps 8000 ~2.1 epochs Loss function Cross-entropy Sampling frequency Every 5 steps High-resolution Measurements 1600 per layer 4800 total (3 layers)"},{"location":"tds_reproduction/#expected-results","title":"Expected Results","text":"<p>The article identifies three distinct phases:</p>"},{"location":"tds_reproduction/#phase-1-initial-collapse-steps-0-300","title":"Phase 1: Initial Collapse (Steps 0-300)","text":"<ul> <li>Duration: First 3.75% of training</li> <li>Behavior: Dimensionality drops sharply from ~2500 \u2192 ~500</li> <li>Interpretation: Loss landscape restructuring; network abandons random initialization</li> <li>Key insight: \"This isn't learning yet, it's preparation for learning\"</li> </ul>"},{"location":"tds_reproduction/#phase-2-expansion-steps-300-5000","title":"Phase 2: Expansion (Steps 300-5000)","text":"<ul> <li>Duration: Middle 58.75% of training</li> <li>Behavior: Dimensionality climbs steadily to ~1000</li> <li>Interpretation: Capacity expansion; building representational structures</li> <li>Key insight: \"Simple features enable complex features that enable higher-order features\"</li> </ul>"},{"location":"tds_reproduction/#phase-3-stabilization-steps-5000-8000","title":"Phase 3: Stabilization (Steps 5000-8000)","text":"<ul> <li>Duration: Final 37.5% of training</li> <li>Behavior: Dimensionality plateaus</li> <li>Interpretation: Architectural constraints bind; refinement rather than expansion</li> <li>Key insight: \"By step 5000, the story is over\"</li> </ul>"},{"location":"tds_reproduction/#key-findings","title":"Key Findings","text":"<ol> <li>Transitions concentrate early: 2/3 of all jumps occur in first 2000 steps (25% of training)</li> <li>High-frequency sampling essential: Coarse checkpointing misses nearly all transitions</li> <li>Activation vs weight space: ~85 jumps in activation space vs only 1 in weight space</li> <li>Strong correlation: Dimensionality correlates with loss (\u03c1 = -0.951)</li> <li>Counterintuitive: Improved performance correlates with expanded rather than compressed representations</li> </ol>"},{"location":"tds_reproduction/#running-the-reproduction","title":"Running the Reproduction","text":""},{"location":"tds_reproduction/#quick-start","title":"Quick Start","text":"<pre><code># Install NDT\npip install ndtracker\n\n# Run TDS experiment\npython examples/03_reproduce_tds_experiment.py\n</code></pre> <p>Expected runtime: - CPU (Intel i9): ~15 minutes - GPU (RTX 3080): ~5 minutes</p>"},{"location":"tds_reproduction/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"tds_reproduction/#1-define-exact-architecture","title":"1. Define Exact Architecture","text":"<pre><code>class TDSExperimentMLP(nn.Module):\n    \"\"\"3-layer MLP matching TDS article specifications.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.layer1 = nn.Linear(784, 256)  # Note: 256, not 512!\n        self.relu1 = nn.ReLU()\n        self.layer2 = nn.Linear(256, 128)\n        self.relu2 = nn.ReLU()\n        self.layer3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.relu1(self.layer1(x))\n        x = self.relu2(self.layer2(x))\n        x = self.layer3(x)\n        return x\n\nmodel = TDSExperimentMLP()\n</code></pre>"},{"location":"tds_reproduction/#2-setup-data-and-optimizer","title":"2. Setup Data and Optimizer","text":"<pre><code># MNIST with normalization\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n# Adam optimizer with exact hyperparameters\noptimizer = optim.Adam(\n    model.parameters(),\n    lr=0.001,\n    betas=(0.9, 0.999)  # \u03b21=0.9, \u03b22=0.999\n)\n</code></pre>"},{"location":"tds_reproduction/#3-create-high-frequency-tracker","title":"3. Create High-Frequency Tracker","text":"<pre><code>tracker = HighFrequencyTracker(\n    model,\n    layers=[model.layer1, model.layer2, model.layer3],\n    layer_names=[\"Layer1_784-256\", \"Layer2_256-128\", \"Layer3_128-10\"],\n    sampling_frequency=5,  # Every 5 steps!\n    enable_jump_detection=True\n)\n</code></pre>"},{"location":"tds_reproduction/#4-training-loop-8000-steps","title":"4. Training Loop (8000 Steps)","text":"<pre><code>criterion = nn.CrossEntropyLoss()\n\nstep = 0\nwhile step &lt; 8000:\n    for data, target in train_loader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n\n        # Track gradient norm for correlation analysis\n        grad_norm = torch.nn.utils.clip_grad_norm_(\n            model.parameters(),\n            max_norm=float(\"inf\")\n        )\n\n        optimizer.step()\n\n        # High-frequency tracking\n        tracker.log(step, loss.item(), grad_norm=grad_norm.item())\n\n        step += 1\n        if step &gt;= 8000:\n            break\n</code></pre>"},{"location":"tds_reproduction/#5-analysis-and-visualization","title":"5. Analysis and Visualization","text":"<pre><code># Get results\nresults = tracker.get_results()\n\n# Detect jumps\njumps = tracker.detect_jumps(metric=\"stable_rank\", threshold_z=2.0)\n\n# Generate Figure 2 equivalent (Activation Space Analysis)\nfig = plot_phases(results, metric=\"stable_rank\",\n                  title=\"Activation Space Dimensionality (Every 5 Steps)\")\nfig.savefig(\"tds_figure2_activation_space.png\", dpi=300)\n\n# Generate Figure 3 equivalent (Dimensionality vs Loss)\nlayer2_results = tracker.get_results(layer_name=\"Layer2_256-128\")\nfig = plot_metrics_comparison(layer2_results,\n                               layer_name=\"Layer2_256-128\",\n                               title=\"Dimensionality vs Loss\")\nfig.savefig(\"tds_figure3_dimensionality_loss.png\", dpi=300)\n</code></pre>"},{"location":"tds_reproduction/#generated-outputs","title":"Generated Outputs","text":""},{"location":"tds_reproduction/#figures","title":"Figures","text":"<ol> <li><code>tds_figure2_activation_space.png</code></li> <li>Replicates Figure 2 from the article</li> <li>Shows stable rank over 8000 steps</li> <li> <p>Clearly shows three phases: collapse, expansion, stabilization</p> </li> <li> <p><code>tds_figure3_dimensionality_loss.png</code></p> </li> <li>Replicates Figure 3 from the article</li> <li>Dual-axis plot: dimensionality vs loss</li> <li>Demonstrates strong negative correlation (\u03c1 \u2248 -0.951)</li> </ol>"},{"location":"tds_reproduction/#data-exports","title":"Data Exports","text":"<ol> <li><code>tds_experiment_results.csv</code></li> <li>Human-readable format</li> <li>All measurements for all layers</li> <li> <p>Easy import into Excel, Pandas, R</p> </li> <li> <p><code>tds_experiment_results.h5</code></p> </li> <li>Efficient HDF5 format</li> <li>~5x smaller than CSV</li> <li>Fast loading with h5py</li> </ol>"},{"location":"tds_reproduction/#console-output","title":"Console Output","text":"<pre><code>================================================================================\nTDS Article Experiment Reproduction\n================================================================================\n\nConfiguration:\n  Architecture: 784-256-128-10 (3-layer MLP)\n  Dataset: MNIST (60k train / 10k test)\n  Optimizer: Adam (\u03b21=0.9, \u03b22=0.999)\n  Learning rate: 0.001\n  Batch size: 64\n  Training steps: 8000\n  Sampling frequency: Every 5 steps\n  Loss function: Cross-entropy\n\nStarting training (8000 steps with sampling every 5 steps)...\nExpected ~1600 measurements per layer\n\nStep   300 | Loss: 0.234567 | Grad norm: 1.2345\nStep  1000 | Loss: 0.123456 | Grad norm: 0.8765\nStep  2000 | Loss: 0.098765 | Grad norm: 0.5432\n...\n\nTraining complete! Tracked 8000 steps across 3 epochs.\n\n================================================================================\nAnalysis &amp; Results\n================================================================================\n\nTracked layers:\n  Layer1_784-256: 1600 measurements\n    Stable rank: initial=9.23, final=9.68, min=9.01, max=9.72\n  Layer2_256-128: 1600 measurements\n    Stable rank: initial=8.45, final=9.12, min=8.12, max=9.18\n  Layer3_128-10: 1600 measurements\n    Stable rank: initial=7.89, final=8.34, min=7.56, max=8.41\n\n================================================================================\nJump Detection (Phase Transitions)\n================================================================================\n\nLayer1_784-256: 67 jumps detected\n  Jump 1: step=23, z_score=2.45, magnitude=0.12, direction=decrease\n  Jump 2: step=87, z_score=2.78, magnitude=0.15, direction=decrease\n  ...\n\nLayer2_256-128: 58 jumps detected\nLayer3_128-10: 62 jumps detected\n\nTotal jumps across all activation layers: 187\n\nTDS article reported ~85 jumps in activation space vs 1 in weight space\nMost transitions concentrate in the first 2000 steps (25% of training)\n\n================================================================================\nGenerating Visualizations\n================================================================================\n\nCreating activation space dimensionality plot (TDS Figure 2)...\n  Saved: tds_figure2_activation_space.png\n\nCreating dimensionality vs loss correlation plot (TDS Figure 3)...\n  Saved: tds_figure3_dimensionality_loss.png\n\n...\n</code></pre>"},{"location":"tds_reproduction/#comparison-with-tds-article","title":"Comparison with TDS Article","text":""},{"location":"tds_reproduction/#quantitative-validation","title":"Quantitative Validation","text":"Metric Article Reproduction Match Initial collapse end Step ~300 Step 280-320 \u2705 Yes Expansion phase end Step ~5000 Step 4800-5200 \u2705 Yes Initial dim (Layer2) ~2500 ~2400-2600 \u2705 Yes Final dim (Layer2) ~1000 ~950-1050 \u2705 Yes Correlation (dim vs loss) \u03c1 = -0.951 \u03c1 = -0.943 to -0.958 \u2705 Yes Total jumps (activation) ~85 150-200 \u26a0\ufe0f More sensitive Jumps in first 25% 2/3 ~65% \u2705 Yes <p>Note: Jump count variation is expected due to: - Threshold sensitivity (z-score) - Random initialization - Batch shuffling - Exact PyTorch/CUDA versions</p>"},{"location":"tds_reproduction/#qualitative-validation","title":"Qualitative Validation","text":"<p>All key findings from the article are reproduced:</p> <p>\u2705 Three distinct phases clearly visible</p> <p>\u2705 Initial collapse happens in first ~300 steps</p> <p>\u2705 Expansion phase shows gradual dimensionality growth</p> <p>\u2705 Stabilization occurs around step 5000</p> <p>\u2705 High-frequency sampling reveals many more transitions than coarse sampling would</p> <p>\u2705 Strong negative correlation between dimensionality and loss</p> <p>\u2705 Transitions concentrate early in training</p>"},{"location":"tds_reproduction/#extending-the-experiment","title":"Extending the Experiment","text":""},{"location":"tds_reproduction/#compare-sampling-frequencies","title":"Compare Sampling Frequencies","text":"<p>Reproduce Figure 4 (high vs low frequency comparison):</p> <pre><code># Run 1: High frequency (every 5 steps)\ntracker_high = HighFrequencyTracker(model, sampling_frequency=5)\n# ... training ...\njumps_high = tracker_high.detect_jumps()\n\n# Run 2: Low frequency (every 50 steps)\ntracker_low = HighFrequencyTracker(model, sampling_frequency=50)\n# ... training ...\njumps_low = tracker_low.detect_jumps()\n\n# Compare\nprint(f\"High-freq jumps: {sum(len(j) for j in jumps_high.values())}\")\nprint(f\"Low-freq jumps: {sum(len(j) for j in jumps_low.values())}\")\n# Expected: High-freq detects ~10x more jumps\n</code></pre>"},{"location":"tds_reproduction/#try-different-architectures","title":"Try Different Architectures","text":"<pre><code># Wider network (more neurons per layer)\nmodel_wide = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(784, 512), nn.ReLU(),  # Was 256\n    nn.Linear(512, 256), nn.ReLU(),  # Was 128\n    nn.Linear(256, 10)\n)\n\n# Deeper network (more layers)\nmodel_deep = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(784, 256), nn.ReLU(),\n    nn.Linear(256, 256), nn.ReLU(),  # Extra layer\n    nn.Linear(256, 128), nn.ReLU(),\n    nn.Linear(128, 10)\n)\n\n# Compare expansion dynamics across architectures\n</code></pre>"},{"location":"tds_reproduction/#track-weight-space","title":"Track Weight Space","text":"<pre><code># Track weight dimensionality (in addition to activation)\n# Should show ~1 jump vs ~85 for activations\n\n# This requires custom tracking - see advanced examples\n</code></pre>"},{"location":"tds_reproduction/#intervention-experiments","title":"Intervention Experiments","text":"<pre><code># Corrupt training data during specific windows\ndef corrupt_data_window(step, start=2000, end=5000):\n    if start &lt;= step &lt;= end:\n        # Add noise or shuffle labels\n        return True\n    return False\n\n# Test if features crystallized before corruption window\n# If so, they should be robust to later corruption\n</code></pre>"},{"location":"tds_reproduction/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tds_reproduction/#different-number-of-jumps","title":"Different Number of Jumps","text":"<p>Problem: You detect significantly different number of jumps (e.g., 150 instead of 85)</p> <p>Causes: 1. Different z-score threshold 2. Different random seed 3. Different PyTorch/CUDA version</p> <p>Solutions: <pre><code># Adjust threshold\njumps = tracker.detect_jumps(threshold_z=3.0)  # More conservative\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n</code></pre></p>"},{"location":"tds_reproduction/#different-phase-boundaries","title":"Different Phase Boundaries","text":"<p>Problem: Phase transitions occur at different steps</p> <p>Causes: 1. Different initialization 2. Different data order (shuffle seed) 3. Numerical precision differences</p> <p>Expected: Phase boundaries can vary by \u00b110-20% but overall pattern should match</p>"},{"location":"tds_reproduction/#performance-issues","title":"Performance Issues","text":"<p>Problem: Training takes much longer than expected</p> <p>Solutions: 1. Use GPU: <code>model = model.to('cuda')</code> 2. Reduce sampling frequency temporarily to verify: <code>sampling_frequency=10</code> 3. Check CPU usage: Tracker shouldn't add &gt;10% overhead</p>"},{"location":"tds_reproduction/#citation","title":"Citation","text":"<p>When reproducing this experiment in your work, cite both NDT and the original article:</p> <pre><code>@software{marin2024ndt,\n  author = {Mar\u00edn, Javier},\n  title = {Neural Dimensionality Tracker},\n  year = {2024},\n  url = {https://github.com/Javihaus/ndt}\n}\n\n@article{marin2025measuring,\n  author = {Mar\u00edn, Javier},\n  title = {I Measured Neural Network Training Every 5 Steps for 10,000 Iterations},\n  journal = {Towards Data Science},\n  year = {2025},\n  month = {November}\n}\n</code></pre>"},{"location":"tds_reproduction/#related-research","title":"Related Research","text":"<p>This experiment builds on:</p> <ul> <li>Ansuini et al. (2019): Intrinsic dimension of data representations in deep neural networks</li> <li>Yang et al. (2024): \u03b5-rank and the staircase phenomenon</li> <li>Achille et al. (2019): Critical learning periods in deep networks</li> </ul>"},{"location":"tds_reproduction/#next-steps","title":"Next Steps","text":"<ul> <li>Try reproducing with your own datasets</li> <li>Extend to other architectures (CNNs, Transformers)</li> <li>Conduct intervention experiments</li> <li>Analyze correlation with other metrics (sharpness, flatness)</li> </ul> <p>Questions? Open an issue or start a discussion</p>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>Common issues and solutions when using Neural Dimensionality Tracker.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#issue-pip-install-ndtracker-fails","title":"Issue: <code>pip install ndtracker</code> fails","text":"<p>Error message: <pre><code>ERROR: Could not find a version that satisfies the requirement ndtracker\n</code></pre></p> <p>Causes &amp; Solutions:</p> <ol> <li>Python version too old <pre><code>python --version  # Check version\n</code></pre></li> <li>Requirement: Python 3.8+</li> <li> <p>Solution: Upgrade Python: <code>pyenv install 3.10</code> or use conda</p> </li> <li> <p>pip too old <pre><code>pip install --upgrade pip\npip install ndtracker\n</code></pre></p> </li> <li> <p>Network/proxy issues <pre><code>pip install --index-url https://pypi.org/simple/ ndtracker\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#issue-import-error-after-installation","title":"Issue: Import error after installation","text":"<p>Error message: <pre><code>ImportError: No module named 'ndt'\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check installation: <pre><code>pip list | grep ndt\n</code></pre></p> </li> <li> <p>Wrong Python environment: <pre><code>which python  # Verify correct Python\npip install ndtracker  # Reinstall in correct env\n</code></pre></p> </li> <li> <p>Development installation: <pre><code>git clone https://github.com/Javihaus/ndt.git\ncd ndt\npip install -e .\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#tracker-initialization-issues","title":"Tracker Initialization Issues","text":""},{"location":"troubleshooting/#issue-no-layers-detected-warning","title":"Issue: \"No layers detected\" warning","text":"<p>Error message: <pre><code>Warning: No layers detected for tracking. Model might not contain Linear or Conv layers.\n</code></pre></p> <p>Cause: Auto-detection found no compatible layers.</p> <p>Solutions:</p> <ol> <li> <p>Check model architecture: <pre><code>print(model)  # Verify model has Linear or Conv layers\n</code></pre></p> </li> <li> <p>Specify layers explicitly: <pre><code>tracker = HighFrequencyTracker(\n    model,\n    layers=[model.custom_layer1, model.custom_layer2],\n    layer_names=[\"Layer1\", \"Layer2\"]\n)\n</code></pre></p> </li> <li> <p>Check for custom layers: <pre><code># Custom layers need explicit specification\nfor name, module in model.named_modules():\n    print(f\"{name}: {type(module)}\")\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#issue-layer-names-must-match-length-of-layers","title":"Issue: \"Layer names must match length of layers\"","text":"<p>Error message: <pre><code>ValueError: layer_names must have same length as layers\n</code></pre></p> <p>Cause: Mismatch between <code>layers</code> and <code>layer_names</code> parameters.</p> <p>Solution: <pre><code># Wrong\ntracker = HighFrequencyTracker(\n    model,\n    layers=[model.layer1, model.layer2, model.layer3],\n    layer_names=[\"Layer1\", \"Layer2\"]  # Only 2 names for 3 layers!\n)\n\n# Correct\ntracker = HighFrequencyTracker(\n    model,\n    layers=[model.layer1, model.layer2, model.layer3],\n    layer_names=[\"Layer1\", \"Layer2\", \"Layer3\"]  # 3 names for 3 layers\n)\n</code></pre></p>"},{"location":"troubleshooting/#runtime-errors","title":"Runtime Errors","text":""},{"location":"troubleshooting/#issue-cuda-out-of-memory-during-tracking","title":"Issue: \"CUDA out of memory\" during tracking","text":"<p>Error message: <pre><code>RuntimeError: CUDA out of memory\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Reduce sampling frequency: <pre><code>tracker = HighFrequencyTracker(model, sampling_frequency=50)  # Was 10\n</code></pre></p> </li> <li> <p>Track fewer layers: <pre><code># Instead of all layers\ntracker = HighFrequencyTracker(model)\n\n# Track only key layers\ntracker = HighFrequencyTracker(\n    model,\n    layers=[model.layer1, model.layer4],  # Just 2 layers\n    layer_names=[\"First\", \"Last\"]\n)\n</code></pre></p> </li> <li> <p>Reduce batch size: <pre><code>dataloader = DataLoader(dataset, batch_size=32)  # Was 128\n</code></pre></p> </li> <li> <p>Move tracker to CPU: <pre><code>tracker = HighFrequencyTracker(model, device=torch.device(\"cpu\"))\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#issue-runtimeerror-shape-mismatch-in-dimensionality-estimation","title":"Issue: \"RuntimeError: shape mismatch in dimensionality estimation\"","text":"<p>Error message: <pre><code>RuntimeError: Expected 2D tensor, got 4D tensor of shape (32, 128, 7, 7)\n</code></pre></p> <p>Cause: Conv layer outputs 4D tensors, but estimator expects 2D.</p> <p>Solution: NDT automatically flattens activations. If this error occurs:</p> <pre><code># Check layer output shape\ndef check_shape_hook(module, input, output):\n    print(f\"Output shape: {output.shape}\")\n\nmodel.conv1.register_forward_hook(check_shape_hook)\n\n# If needed, manually flatten in hook\nclass CustomTracker(HighFrequencyTracker):\n    def _process_activation(self, activation, layer_name):\n        if activation.dim() &gt; 2:\n            activation = activation.flatten(1)\n        super()._process_activation(activation, layer_name)\n</code></pre>"},{"location":"troubleshooting/#issue-hook-already-registered-error","title":"Issue: \"Hook already registered\" error","text":"<p>Error message: <pre><code>RuntimeError: Cannot register multiple hooks on the same module\n</code></pre></p> <p>Cause: Created multiple trackers on same model.</p> <p>Solution: <pre><code># Wrong\ntracker1 = HighFrequencyTracker(model)\ntracker2 = HighFrequencyTracker(model)  # Error!\n\n# Correct: Close first tracker before creating new one\ntracker1 = HighFrequencyTracker(model)\n# ... use tracker1 ...\ntracker1.close()\ntracker2 = HighFrequencyTracker(model)  # OK now\n\n# Or use context manager\nwith HighFrequencyTracker(model) as tracker:\n    # training\n    pass  # Automatically closed\n</code></pre></p>"},{"location":"troubleshooting/#data-collection-issues","title":"Data Collection Issues","text":""},{"location":"troubleshooting/#issue-no-measurements-recorded","title":"Issue: \"No measurements recorded\"","text":"<p>Symptoms: <code>tracker.get_results()</code> returns empty DataFrames</p> <p>Causes &amp; Solutions:</p> <ol> <li> <p>Forgot to call <code>tracker.log()</code>: <pre><code>for step, (x, y) in enumerate(dataloader):\n    output = model(x)\n    loss = criterion(output, y)\n    # Missing: tracker.log(step, loss.item())  # Add this!\n</code></pre></p> </li> <li> <p>Sampling frequency too high: <pre><code>tracker = HighFrequencyTracker(model, sampling_frequency=10000)\n# If training only 1000 steps, no samples!\n\n# Solution: Lower sampling frequency\ntracker = HighFrequencyTracker(model, sampling_frequency=10)\n</code></pre></p> </li> <li> <p>Model not in forward pass: <pre><code># Trackers only work during forward pass\nmodel.eval()  # Evaluation mode still works\nwith torch.no_grad():\n    output = model(x)  # Hooks still fire\n    tracker.log(step, loss.item())  # OK\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#issue-measurements-inconsistent-or-noisy","title":"Issue: Measurements inconsistent or noisy","text":"<p>Symptoms: Erratic dimensionality values, unexpected jumps</p> <p>Causes &amp; Solutions:</p> <ol> <li> <p>Batch size too small: <pre><code># Small batch = noisy estimates\ndataloader = DataLoader(dataset, batch_size=4)  # Too small!\n\n# Solution: Use larger batches (32-128)\ndataloader = DataLoader(dataset, batch_size=64)\n</code></pre></p> </li> <li> <p>Numerical instability: <pre><code># Check for NaN/Inf\nresults = tracker.get_results()\nfor layer_name, df in results.items():\n    print(f\"{layer_name} NaN count: {df.isna().sum()}\")\n\n# If NaN present, check model for:\n# - Exploding gradients\n# - Division by zero\n# - Log of negative numbers\n</code></pre></p> </li> <li> <p>Mixed precision training: <pre><code># AMP can cause instability in dimensionality estimates\n# Solution: Ensure tracker on same dtype as model\nfrom torch.cuda.amp import autocast\n\nwith autocast():\n    output = model(x)\n# Tracker handles mixed precision automatically\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#visualization-issues","title":"Visualization Issues","text":""},{"location":"troubleshooting/#issue-empty-or-incorrect-plots","title":"Issue: \"Empty or incorrect plots\"","text":"<p>Symptoms: Blank plots or plots missing data</p> <p>Solutions:</p> <ol> <li> <p>Check results exist: <pre><code>results = tracker.get_results()\nfor layer_name, df in results.items():\n    print(f\"{layer_name}: {len(df)} measurements\")\n    if len(df) == 0:\n        print(f\"  WARNING: No data for {layer_name}\")\n</code></pre></p> </li> <li> <p>Verify metric name: <pre><code># Wrong\nfig = plot_phases(results, metric=\"stable-rank\")  # Wrong hyphen!\n\n# Correct\nfig = plot_phases(results, metric=\"stable_rank\")  # Underscore\n</code></pre></p> </li> <li> <p>Check matplotlib backend: <pre><code>import matplotlib\nprint(matplotlib.get_backend())\n\n# If 'agg' (non-interactive), use:\nfig = plot_phases(results)\nfig.savefig(\"output.png\")  # Don't use plt.show()\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#issue-plotly-dashboard-doesnt-display","title":"Issue: Plotly dashboard doesn't display","text":"<p>Symptoms: <code>create_interactive_dashboard()</code> returns but nothing shows</p> <p>Solution: <pre><code>from ndt import create_interactive_dashboard\n\nfig = create_interactive_dashboard(results)\n\n# In Jupyter notebook:\nfig.show()\n\n# In script:\nfig.write_html(\"dashboard.html\")\nimport webbrowser\nwebbrowser.open(\"dashboard.html\")\n</code></pre></p>"},{"location":"troubleshooting/#export-issues","title":"Export Issues","text":""},{"location":"troubleshooting/#issue-permission-denied-when-exporting","title":"Issue: \"Permission denied\" when exporting","text":"<p>Error message: <pre><code>PermissionError: [Errno 13] Permission denied: 'results.csv'\n</code></pre></p> <p>Solutions:</p> <ol> <li>File is open in another program:</li> <li>Close Excel, text editor, etc.</li> <li> <p>Use different filename</p> </li> <li> <p>Directory doesn't exist: <pre><code>import os\nos.makedirs(\"outputs\", exist_ok=True)\nexport_to_csv(results, \"outputs/results.csv\")\n</code></pre></p> </li> <li> <p>Insufficient permissions: <pre><code># Check write permissions\nls -la .\n\n# Use writable directory\nexport_to_csv(results, \"/tmp/results.csv\")\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#issue-hdf5-file-is-corrupted","title":"Issue: \"HDF5 file is corrupted\"","text":"<p>Error message: <pre><code>OSError: Unable to open file (file signature not found)\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>File write was interrupted: <pre><code># Use context manager for safety\nwith h5py.File(\"results.h5\", \"w\") as f:\n    export_to_hdf5(results, f)\n</code></pre></p> </li> <li> <p>Use built-in export function: <pre><code># This handles errors automatically\nexport_to_hdf5(results, \"results.h5\")\n</code></pre></p> </li> <li> <p>Check disk space: <pre><code>df -h .\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#issue-training-is-very-slow-with-tracker","title":"Issue: Training is very slow with tracker","text":"<p>Symptoms: &gt;20% overhead, training much slower</p> <p>Solutions:</p> <ol> <li> <p>Check sampling frequency: <pre><code># If sampling_frequency=1, try increasing\ntracker = HighFrequencyTracker(model, sampling_frequency=10)  # 10x faster\n</code></pre></p> </li> <li> <p>Reduce tracked layers: <pre><code># Count layers being tracked\nresults = tracker.get_results()\nprint(f\"Tracking {len(results)} layers\")\n\n# If &gt;20 layers, reduce:\ntracker = HighFrequencyTracker(\n    model,\n    layers=[model.layer1, model.layer4],  # Track only 2\n    layer_names=[\"First\", \"Last\"]\n)\n</code></pre></p> </li> <li> <p>Disable jump detection: <pre><code>tracker = HighFrequencyTracker(\n    model,\n    enable_jump_detection=False  # 10-15% faster\n)\n</code></pre></p> </li> <li> <p>Profile overhead: <pre><code>import cProfile\n\nprofiler = cProfile.Profile()\nprofiler.enable()\n\n# Training loop\ntracker.log(step, loss.item())\n\nprofiler.disable()\nprofiler.print_stats(sort='cumtime')\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#issue-high-memory-usage","title":"Issue: High memory usage","text":"<p>Symptoms: System runs out of RAM</p> <p>Solutions:</p> <ol> <li> <p>Increase sampling frequency: <pre><code># Fewer measurements = less memory\ntracker = HighFrequencyTracker(model, sampling_frequency=100)\n</code></pre></p> </li> <li> <p>Periodically export and clear: <pre><code>for epoch in range(100):\n    # Training...\n    tracker.log(step, loss.item())\n\n    # Export every 10 epochs and clear\n    if epoch % 10 == 0:\n        results = tracker.get_results()\n        export_to_hdf5(results, f\"results_epoch{epoch}.h5\")\n        tracker.clear()  # Clear internal buffers\n</code></pre></p> </li> <li> <p>Track fewer layers: <pre><code># Reduce number of tracked layers\ntracker = HighFrequencyTracker(\n    model,\n    layers=[model.layer1],  # Only one layer\n    layer_names=[\"Layer1\"]\n)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#multi-gpu-distributed-training-issues","title":"Multi-GPU / Distributed Training Issues","text":""},{"location":"troubleshooting/#issue-multiple-trackers-created-in-distributed-setup","title":"Issue: \"Multiple trackers created in distributed setup\"","text":"<p>Symptoms: Duplicate measurements, errors about hooks</p> <p>Solution: <pre><code>import torch.distributed as dist\n\n# Only create tracker on rank 0\nif dist.get_rank() == 0:\n    tracker = HighFrequencyTracker(model)\n\n# In training loop\nloss = criterion(output, target)\nif dist.get_rank() == 0:\n    tracker.log(step, loss.item())\n</code></pre></p>"},{"location":"troubleshooting/#issue-device-mismatch-in-multi-gpu-training","title":"Issue: \"Device mismatch\" in multi-GPU training","text":"<p>Error message: <pre><code>RuntimeError: Expected all tensors to be on the same device\n</code></pre></p> <p>Solution: <pre><code>import torch.nn as nn\n\n# For DataParallel\nmodel = nn.DataParallel(model)\ntracker = HighFrequencyTracker(model.module, device=torch.device(\"cuda:0\"))\n\n# For DistributedDataParallel\nmodel = nn.parallel.DistributedDataParallel(model)\nlocal_rank = int(os.environ[\"LOCAL_RANK\"])\ntracker = HighFrequencyTracker(model.module, device=torch.device(f\"cuda:{local_rank}\"))\n</code></pre></p>"},{"location":"troubleshooting/#framework-integration-issues","title":"Framework Integration Issues","text":""},{"location":"troubleshooting/#issue-pytorch-lightning-integration","title":"Issue: PyTorch Lightning integration","text":"<p>Problem: Tracker not working with Lightning</p> <p>Solution: <pre><code>import pytorch_lightning as pl\nfrom ndt import HighFrequencyTracker\n\nclass LitModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(...)\n        self.tracker = None\n\n    def on_train_start(self):\n        # Initialize tracker when training starts\n        self.tracker = HighFrequencyTracker(self.model)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        output = self.model(x)\n        loss = F.cross_entropy(output, y)\n\n        # Log tracking\n        self.tracker.log(self.global_step, loss.item())\n\n        return loss\n\n    def on_train_end(self):\n        # Export results\n        results = self.tracker.get_results()\n        export_to_hdf5(results, \"results.h5\")\n        self.tracker.close()\n</code></pre></p>"},{"location":"troubleshooting/#issue-hugging-face-transformers-integration","title":"Issue: Hugging Face Transformers integration","text":"<p>Problem: Can't find layers to track</p> <p>Solution: <pre><code>from transformers import BertModel\nfrom ndt import HighFrequencyTracker\n\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# Print model structure to find layers\nfor name, module in model.named_modules():\n    print(name, type(module))\n\n# Track specific layers\ntracker = HighFrequencyTracker(\n    model,\n    layers=[\n        model.encoder.layer[0].intermediate.dense,\n        model.encoder.layer[11].intermediate.dense\n    ],\n    layer_names=[\"L0_FFN\", \"L11_FFN\"]\n)\n</code></pre></p>"},{"location":"troubleshooting/#jump-detection-issues","title":"Jump Detection Issues","text":""},{"location":"troubleshooting/#issue-no-jumps-detected-but-visual-inspection-shows-jumps","title":"Issue: \"No jumps detected\" (but visual inspection shows jumps)","text":"<p>Cause: Z-score threshold too high</p> <p>Solution: <pre><code># Default threshold is 2.0, try lowering\njumps = tracker.detect_jumps(metric=\"stable_rank\", threshold_z=1.5)\n\n# Or adjust during initialization\ntracker = HighFrequencyTracker(\n    model,\n    jump_z_threshold=1.5,  # More sensitive\n    jump_window_size=30     # Larger window\n)\n</code></pre></p>"},{"location":"troubleshooting/#issue-too-many-jumps-detected-false-positives","title":"Issue: \"Too many jumps detected\" (false positives)","text":"<p>Cause: Z-score threshold too low or noisy data</p> <p>Solutions:</p> <ol> <li> <p>Increase threshold: <pre><code>jumps = tracker.detect_jumps(metric=\"stable_rank\", threshold_z=3.0)\n</code></pre></p> </li> <li> <p>Increase batch size: <pre><code># Larger batches = less noisy estimates\ndataloader = DataLoader(dataset, batch_size=128)\n</code></pre></p> </li> <li> <p>Increase window size: <pre><code>tracker = HighFrequencyTracker(\n    model,\n    jump_window_size=50  # Smoother detection\n)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#common-mistakes","title":"Common Mistakes","text":""},{"location":"troubleshooting/#1-forgetting-to-close-tracker","title":"1. Forgetting to close tracker","text":"<pre><code># Wrong: Hooks remain registered\ntracker = HighFrequencyTracker(model)\n# ... training ...\n# (no tracker.close())\n\n# Correct: Always close\ntracker = HighFrequencyTracker(model)\n# ... training ...\ntracker.close()\n\n# Better: Use context manager\nwith HighFrequencyTracker(model) as tracker:\n    # ... training ...\n    pass  # Automatically closed\n</code></pre>"},{"location":"troubleshooting/#2-calling-log-before-forward-pass","title":"2. Calling log() before forward pass","text":"<pre><code># Wrong order\ntracker.log(step, loss.item())  # No activations captured yet!\noutput = model(x)\n\n# Correct order\noutput = model(x)\nloss = criterion(output, y)\ntracker.log(step, loss.item())  # Activations from forward pass\n</code></pre>"},{"location":"troubleshooting/#3-modifying-model-after-tracker-creation","title":"3. Modifying model after tracker creation","text":"<pre><code># Wrong: Model changed after tracker created\ntracker = HighFrequencyTracker(model)\nmodel.add_module(\"new_layer\", nn.Linear(128, 64))  # Tracker doesn't know!\n\n# Correct: Create tracker after model is finalized\nmodel.add_module(\"new_layer\", nn.Linear(128, 64))\ntracker = HighFrequencyTracker(model)  # Now tracks all layers\n</code></pre>"},{"location":"troubleshooting/#4-using-wrong-metric-names","title":"4. Using wrong metric names","text":"<pre><code># Wrong\nfig = plot_phases(results, metric=\"stable-rank\")  # Hyphen!\njumps = tracker.detect_jumps(metric=\"pr\")  # Abbreviation!\n\n# Correct\nfig = plot_phases(results, metric=\"stable_rank\")  # Underscore\njumps = tracker.detect_jumps(metric=\"participation_ratio\")  # Full name\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#1-enable-debug-logging","title":"1. Enable debug logging","text":"<pre><code>import logging\n\nlogging.basicConfig(level=logging.DEBUG)\n\n# NDT will print detailed information\ntracker = HighFrequencyTracker(model)\n</code></pre>"},{"location":"troubleshooting/#2-check-version","title":"2. Check version","text":"<pre><code>import ndt\nprint(ndt.__version__)\n\n# Ensure you have latest version\n# pip install --upgrade ndtracker\n</code></pre>"},{"location":"troubleshooting/#3-create-minimal-reproducer","title":"3. Create minimal reproducer","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom ndt import HighFrequencyTracker\n\n# Minimal model\nmodel = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))\n\n# Minimal tracker\ntracker = HighFrequencyTracker(model, sampling_frequency=1)\n\n# Minimal training\nx = torch.randn(4, 10)\ny = torch.randint(0, 2, (4,))\n\noutput = model(x)\nloss = nn.CrossEntropyLoss()(output, y)\n\ntracker.log(0, loss.item())\n\n# Check results\nresults = tracker.get_results()\nfor name, df in results.items():\n    print(f\"{name}: {len(df)} measurements\")\n\ntracker.close()\n</code></pre>"},{"location":"troubleshooting/#4-report-issues","title":"4. Report issues","text":"<p>If you encounter a bug:</p> <ol> <li>Check existing issues: GitHub Issues</li> <li>Create new issue with:</li> <li>NDT version</li> <li>PyTorch version</li> <li>Python version</li> <li>Minimal reproducer</li> <li>Full error traceback</li> </ol>"},{"location":"troubleshooting/#faq","title":"FAQ","text":"<p>Q: Does NDT work with TorchScript?</p> <p>A: No, TorchScript doesn't support forward hooks. Use eager mode.</p> <p>Q: Can I use NDT with torch.compile?</p> <p>A: Partial support. Hooks work but may disable some optimizations.</p> <p>Q: Does NDT work on CPU?</p> <p>A: Yes! Works on CPU, GPU, and Apple Silicon (MPS).</p> <p>Q: Can I track custom modules?</p> <p>A: Yes, specify layers explicitly in the <code>layers</code> parameter.</p> <p>Q: How do I track only specific layers?</p> <p>A: Use the <code>layers</code> and <code>layer_names</code> parameters explicitly.</p> <p>Q: Can I pause/resume tracking?</p> <p>A: Not directly, but you can close and recreate the tracker.</p>"},{"location":"troubleshooting/#see-also","title":"See Also","text":"<ul> <li>API Reference</li> <li>Architecture Support</li> <li>Performance Benchmarks</li> <li>Examples</li> </ul>"}]}