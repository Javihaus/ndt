"""
Generate Demonstration Results for TAP Experiments

This script creates realistic mock data to demonstrate what the experimental
framework would produce. Useful for:
1. Showing expected output format
2. Testing analysis pipelines
3. Demonstrating visualization
4. Documenting expected findings

Run this when you want to see what the experiments produce without
waiting for full training runs.
"""

import numpy as np
import json
from pathlib import Path
try:
    from scipy.optimize import curve_fit
    from sklearn.linear_model import LinearRegression
    import pandas as pd
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False
    print("Warning: scipy/sklearn not available, using simplified fitting")


# ============================================================================
# CONFIGURATION
# ============================================================================

np.random.seed(42)

OUTPUT_DIR = Path('./experiments/new/results_demo')
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Architecture configurations
ARCHITECTURES = {
    'mlp_shallow_2': {'depth': 2, 'width': 96, 'num_params': 45000},
    'mlp_medium_5': {'depth': 5, 'width': 64, 'num_params': 55000},
    'mlp_deep_10': {'depth': 10, 'width': 32, 'num_params': 48000},
    'mlp_wide': {'depth': 4, 'width': 256, 'num_params': 180000},
    'cnn_shallow': {'depth': 3, 'width': 48, 'num_params': 65000},
    'cnn_deep': {'depth': 5, 'width': 128, 'num_params': 250000},
    'transformer_shallow': {'depth': 2, 'width': 128, 'num_params': 95000},
    'transformer_deep': {'depth': 6, 'width': 128, 'num_params': 280000},
}

DATASETS = ['mnist', 'cifar10']


# ============================================================================
# GENERATE PHASE 1: CALIBRATION DATA
# ============================================================================

def generate_dimensionality_curve(depth, width, num_steps=1000, alpha=None):
    """
    Generate realistic dimensionality curve using TAP model.

    D(t+1) = D(t) + Î± Â· ||âˆ‡L|| Â· D(t) Â· (1 - D(t)/D_max)
    """
    # Estimate Î± from architecture (deeper/wider â†’ larger Î±)
    if alpha is None:
        alpha = 0.0001 * (depth ** 0.5) * (width ** 0.3)

    # Estimate D_max
    D_max = np.sqrt(width) * depth

    # Generate gradient norms (decrease over training)
    grad_norms = 2.0 * np.exp(-np.arange(num_steps) / 300) + 0.1

    # Numerical integration of TAP model
    D = np.zeros(num_steps)
    D[0] = 0.5  # Initial dimensionality

    for t in range(num_steps - 1):
        growth = alpha * grad_norms[t] * D[t] * (1 - D[t] / D_max)
        D[t+1] = D[t] + growth + np.random.randn() * 0.05  # Add noise
        D[t+1] = np.clip(D[t+1], 0, D_max)

    # Generate loss curve
    loss = 2.3 * np.exp(-np.arange(num_steps) / 200) + 0.1

    # Generate accuracy curve
    accuracy = 0.1 + 0.85 * (1 - np.exp(-np.arange(num_steps) / 250))

    return {
        'dimensionality': D.tolist(),
        'grad_norms': grad_norms.tolist(),
        'loss': loss.tolist(),
        'accuracy': accuracy.tolist(),
        'alpha_true': alpha,
        'D_max': D_max
    }


def generate_phase1_data():
    """Generate Phase 1 calibration data for all architectures Ã— datasets."""

    print("Generating Phase 1 calibration data...")

    phase1_dir = OUTPUT_DIR / 'phase1'
    phase1_dir.mkdir(exist_ok=True)

    all_results = []

    for arch_name, arch_params in ARCHITECTURES.items():
        for dataset in DATASETS:
            print(f"  {arch_name} on {dataset}")

            # Generate curves
            curves = generate_dimensionality_curve(
                arch_params['depth'],
                arch_params['width'],
                num_steps=1000
            )

            # Create measurements
            measurements = []
            steps = list(range(0, 1000, 5))

            for step in steps:
                idx = step
                measurements.append({
                    'step': step,
                    'loss': curves['loss'][idx],
                    'grad_norm': curves['grad_norms'][idx],
                    'layer_metrics': {
                        'layer_0': {
                            'stable_rank': curves['dimensionality'][idx],
                            'participation_ratio': curves['dimensionality'][idx] * 0.95,
                            'effective_rank_90': curves['dimensionality'][idx] * 0.85,
                            'nuclear_norm_ratio': curves['dimensionality'][idx] * 1.1
                        }
                    }
                })

            result = {
                'arch_name': arch_name,
                'dataset_name': dataset,
                'architecture_params': arch_params,
                'measurements': measurements,
                'final_accuracy': curves['accuracy'][-1],
                'num_steps': 1000,
                'measurement_interval': 5
            }

            # Save individual result
            output_file = phase1_dir / f'{arch_name}_{dataset}.json'
            with open(output_file, 'w') as f:
                json.dump(result, f, indent=2)

            all_results.append(result)

    # Save combined
    with open(phase1_dir / 'phase1_all_results.json', 'w') as f:
        json.dump(all_results, f, indent=2)

    print(f"âœ“ Phase 1 data generated: {len(all_results)} experiments")
    return all_results


# ============================================================================
# GENERATE PHASE 1 ANALYSIS
# ============================================================================

def generate_phase1_analysis():
    """Generate Phase 1 analysis with Î± extraction."""

    print("\nGenerating Phase 1 analysis...")

    phase1_dir = OUTPUT_DIR / 'phase1'
    analysis_dir = OUTPUT_DIR / 'phase1_analysis'
    analysis_dir.mkdir(exist_ok=True)

    # Load Phase 1 data
    result_files = list(phase1_dir.glob('*_*.json'))

    summary_data = []

    for result_file in result_files:
        if result_file.name == 'phase1_all_results.json':
            continue

        with open(result_file) as f:
            data = json.load(f)

        arch_params = data['architecture_params']
        measurements = data['measurements']

        # Extract dimensionality curve
        steps = [m['step'] for m in measurements]
        D = [m['layer_metrics']['layer_0']['stable_rank'] for m in measurements]

        # Fit logistic curve to extract Î±
        if HAS_SCIPY:
            def logistic(t, D_max, alpha, t0):
                return D_max / (1 + np.exp(-alpha * (t - t0)))

            try:
                popt, _ = curve_fit(logistic, steps, D,
                                   p0=[max(D) * 1.2, 0.01, 500],
                                   maxfev=10000)
                D_max_fit, alpha_fit, t0_fit = popt

                # Calculate RÂ²
                predictions = logistic(np.array(steps), *popt)
                ss_res = np.sum((np.array(D) - predictions) ** 2)
                ss_tot = np.sum((np.array(D) - np.mean(D)) ** 2)
                r2 = 1 - (ss_res / ss_tot)
            except:
                alpha_fit = 0.001 * arch_params['depth'] ** 0.5
                D_max_fit = max(D)
                r2 = 0.85
        else:
            # Simple estimate without scipy
            alpha_fit = 0.001 * arch_params['depth'] ** 0.5
            D_max_fit = max(D)
            r2 = 0.85

        # Detect jumps
        gradients = np.gradient(D)
        mean_grad = np.mean(gradients)
        std_grad = np.std(gradients)
        num_jumps = sum(1 for g in gradients if (g - mean_grad) / (std_grad + 1e-10) > 2.0)

        summary_data.append({
            'arch_name': data['arch_name'],
            'dataset_name': data['dataset_name'],
            'depth': arch_params['depth'],
            'width': arch_params['width'],
            'num_params': arch_params['num_params'],
            'connectivity': arch_params['num_params'] / (arch_params['depth'] + arch_params['width']),
            'alpha': abs(alpha_fit),
            'D_max': D_max_fit,
            'R2': r2,
            't0': 500,
            'num_jumps': num_jumps,
            'final_accuracy': data['final_accuracy']
        })

    # Save summary CSV
    if HAS_SCIPY:
        df = pd.DataFrame(summary_data)
        df.to_csv(analysis_dir / 'alpha_summary.csv', index=False)
    else:
        # Manual CSV writing
        with open(analysis_dir / 'alpha_summary.csv', 'w') as f:
            if summary_data:
                keys = summary_data[0].keys()
                f.write(','.join(keys) + '\n')
                for row in summary_data:
                    f.write(','.join(str(row[k]) for k in keys) + '\n')

        # Create simple dataframe-like structure
        class SimpleDF:
            def __init__(self, data):
                self.data = data
                self._columns = list(data[0].keys()) if data else []

            def __len__(self):
                return len(self.data)

            def groupby(self, col):
                groups = {}
                for row in self.data:
                    key = row[col]
                    if key not in groups:
                        groups[key] = []
                    groups[key].append(row)
                return SimpleGroupBy(groups, self._columns)

            def __getitem__(self, cols):
                if isinstance(cols, list):
                    return [[row[c] for c in cols] for row in self.data]
                return [row[cols] for row in self.data]

        class SimpleGroupBy:
            def __init__(self, groups, columns):
                self.groups = groups
                self.columns = columns

            def agg(self, agg_dict):
                result = []
                for key, rows in self.groups.items():
                    row_result = {}
                    for col, func in agg_dict.items():
                        if func == 'first':
                            row_result[col] = rows[0][col]
                        elif func == 'mean':
                            row_result[col] = np.mean([r[col] for r in rows])
                    row_result['arch_name'] = key
                    result.append(row_result)
                agg_result = SimpleDF(result)
                # Add reset_index method to the result
                agg_result.reset_index = lambda: agg_result
                return agg_result

            def reset_index(self):
                return SimpleDF([row for rows in self.groups.values() for row in rows])

        df = SimpleDF(summary_data)

    # Fit Î± = f(architecture) models
    print("\n  Fitting Î± = f(architecture) relationship...")

    # Group by architecture and average across datasets
    arch_alphas = df.groupby('arch_name').agg({
        'depth': 'first',
        'width': 'first',
        'num_params': 'first',
        'connectivity': 'first',
        'alpha': 'mean',
        'R2': 'mean'
    }).reset_index()

    # Linear model: Î± = c0 + c1*depth + c2*width + c3*connectivity
    if HAS_SCIPY:
        X = np.array([[r['depth'], r['width'], r['connectivity']] for r in arch_alphas.data])
        y = np.array([r['alpha'] for r in arch_alphas.data])

        model_linear = LinearRegression()
        model_linear.fit(X, y)
        y_pred = model_linear.predict(X)

        r2_linear = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)
    else:
        # Simple correlation-based estimate
        data_list = arch_alphas.data if hasattr(arch_alphas, 'data') else arch_alphas
        depths = [r['depth'] for r in data_list]
        widths = [r['width'] for r in data_list]
        connectivities = [r['connectivity'] for r in data_list]
        alphas = [r['alpha'] for r in data_list]

        # Simple linear regression (least squares)
        X = np.column_stack([depths, widths, connectivities])
        X_with_intercept = np.column_stack([np.ones(len(depths)), X])
        y = np.array(alphas)

        # Solve normal equations: (X^T X)^{-1} X^T y
        coeffs = np.linalg.lstsq(X_with_intercept, y, rcond=None)[0]

        class SimpleModel:
            def __init__(self, coeffs):
                self.intercept_ = coeffs[0]
                self.coef_ = coeffs[1:]

            def predict(self, X):
                return self.intercept_ + np.dot(X, self.coef_)

        model_linear = SimpleModel(coeffs)
        y_pred = model_linear.predict(X)
        r2_linear = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)

    alpha_models = {
        'linear': {
            'R2': r2_linear,
            'coefficients': {
                'depth': model_linear.coef_[0],
                'width': model_linear.coef_[1],
                'connectivity': model_linear.coef_[2],
                'intercept': model_linear.intercept_
            }
        },
        'data': arch_alphas.data if hasattr(arch_alphas, 'data') else list(arch_alphas)
    }

    # Save models (as JSON since we can't pickle without sklearn objects)
    with open(analysis_dir / 'alpha_models.json', 'w') as f:
        json.dump(alpha_models, f, indent=2)

    print(f"  Î± model RÂ²: {r2_linear:.4f}")
    print(f"  Coefficients: depth={model_linear.coef_[0]:.6f}, "
          f"width={model_linear.coef_[1]:.6f}, connectivity={model_linear.coef_[2]:.6f}")

    # Create report
    report = []
    # Get statistics manually
    num_archs = len(set([r['arch_name'] for r in summary_data]))
    num_datasets = len(set([r['dataset_name'] for r in summary_data]))
    r2_values = [r['R2'] for r in summary_data]
    mean_r2 = np.mean(r2_values)
    r2_above_08 = sum(1 for r2 in r2_values if r2 > 0.8)

    report.append("# Phase 1 Calibration: Analysis Report\n\n")
    report.append(f"## Summary Statistics\n\n")
    report.append(f"- Total experiments: {len(summary_data)}\n")
    report.append(f"- Unique architectures: {num_archs}\n")
    report.append(f"- Datasets: {num_datasets}\n")
    report.append(f"- Mean RÂ²: {mean_r2:.4f}\n")
    report.append(f"- Experiments with RÂ² > 0.8: {r2_above_08} ({r2_above_08/len(summary_data)*100:.1f}%)\n\n")

    report.append(f"## Î± = f(architecture) Relationship\n\n")
    report.append(f"### Linear Model\n")
    report.append(f"- RÂ²: {r2_linear:.4f}\n")
    report.append(f"- Formula: Î± = {model_linear.intercept_:.6f} + "
                 f"{model_linear.coef_[0]:.6f}Â·depth + "
                 f"{model_linear.coef_[1]:.6f}Â·width + "
                 f"{model_linear.coef_[2]:.6f}Â·connectivity\n\n")

    # Get summary stats
    alphas = [r['alpha'] for r in summary_data]
    jumps = [r['num_jumps'] for r in summary_data]

    report.append(f"## Key Findings\n\n")
    report.append(f"1. **Î± range**: [{min(alphas):.6f}, {max(alphas):.6f}]\n")
    report.append(f"2. **Mean Î±**: {np.mean(alphas):.6f}\n")
    report.append(f"3. **Std Î±**: {np.std(alphas):.6f}\n")
    report.append(f"4. **Mean jumps per experiment**: {np.mean(jumps):.2f}\n")

    with open(analysis_dir / 'phase1_report.md', 'w') as f:
        f.writelines(report)

    print(f"âœ“ Phase 1 analysis complete: {len(summary_data)} experiments analyzed")

    return alpha_models, df


# ============================================================================
# GENERATE PHASE 2: PREDICTIONS
# ============================================================================

def generate_phase2_predictions(alpha_models):
    """Generate Phase 2 prediction results."""

    print("\nGenerating Phase 2 prediction experiments...")

    phase2_dir = OUTPUT_DIR / 'phase2'
    phase2_dir.mkdir(exist_ok=True)

    # Test architectures (some new, some from Phase 1)
    test_archs = ['mlp_shallow_2', 'mlp_deep_10', 'cnn_shallow']

    results = []

    for arch_name in test_archs:
        for dataset in ['mnist', 'cifar10']:
            arch_params = ARCHITECTURES[arch_name]

            # Estimate Î± using fitted model
            coeffs = alpha_models['linear']['coefficients']
            alpha_pred = (coeffs['intercept'] +
                         coeffs['depth'] * arch_params['depth'] +
                         coeffs['width'] * arch_params['width'] +
                         coeffs['connectivity'] * (arch_params['num_params'] /
                                                   (arch_params['depth'] + arch_params['width'])))
            alpha_pred = max(alpha_pred, 1e-6)

            # Generate actual curve
            actual_curves = generate_dimensionality_curve(
                arch_params['depth'],
                arch_params['width'],
                num_steps=1000
            )

            # Generate predicted curve (slightly different)
            pred_curves = generate_dimensionality_curve(
                arch_params['depth'],
                arch_params['width'],
                num_steps=1000,
                alpha=alpha_pred * 0.95  # Slight difference
            )

            # Compute RÂ²
            actual_D = np.array(actual_curves['dimensionality'])
            pred_D = np.array(pred_curves['dimensionality'])

            ss_res = np.sum((actual_D - pred_D) ** 2)
            ss_tot = np.sum((actual_D - np.mean(actual_D)) ** 2)
            r2 = 1 - (ss_res / ss_tot)

            success = bool(r2 > 0.8)

            result = {
                'arch_name': arch_name,
                'dataset_name': dataset,
                'architecture_params': arch_params,
                'alpha_predicted': float(alpha_pred),
                'predictions': {
                    'simple': {'R2': float(r2 * 0.7)},
                    'refined': {'R2': float(r2), 'curve': pred_D.tolist()}
                },
                'actual': {
                    'steps': list(range(0, 1000, 5)),
                    'dimensionality': actual_D[::5].tolist(),
                    'grad_norms': actual_curves['grad_norms'][::5]
                },
                'success': success,
                'success_criterion': 'RÂ² > 0.8'
            }

            # Save
            output_file = phase2_dir / f'{arch_name}_{dataset}.json'
            with open(output_file, 'w') as f:
                json.dump(result, f, indent=2)

            results.append(result)

            status = "âœ“" if success else "âœ—"
            print(f"  {status} {arch_name} on {dataset}: RÂ² = {r2:.4f}")

    # Save summary
    with open(phase2_dir / 'phase2_summary.json', 'w') as f:
        json.dump(results, f, indent=2)

    successes = sum(1 for r in results if r['success'])
    print(f"\nâœ“ Phase 2 complete: {successes}/{len(results)} successful predictions")

    return results


# ============================================================================
# GENERATE PHASE 3: MONITORING
# ============================================================================

def generate_phase3_monitoring():
    """Generate Phase 3 monitoring demonstration."""

    print("\nGenerating Phase 3 monitoring demonstration...")

    phase3_dir = OUTPUT_DIR / 'phase3'
    phase3_dir.mkdir(exist_ok=True)

    # Generate monitoring data
    arch_params = ARCHITECTURES['mlp_medium_5']
    curves = generate_dimensionality_curve(
        arch_params['depth'],
        arch_params['width'],
        num_steps=2000
    )

    # Create monitoring log
    monitor_log = {
        'architecture_params': arch_params,
        'alpha_estimated': curves['alpha_true'],
        'warnings': [
            {'step': 500, 'message': 'âš ï¸ Gradient norm declining rapidly'},
            {'step': 1200, 'message': 'âš ï¸ No jumps detected for 400 steps'},
        ],
        'jumps_detected': [
            {'step': 150, 'magnitude': 2.5},
            {'step': 380, 'magnitude': 3.1},
            {'step': 720, 'magnitude': 2.8},
        ],
        'recommendations': [
            'ðŸ’¡ Network approaching capacity limits at step 1500',
            'ðŸ’¡ Consider early stopping around step 1600'
        ],
        'history': {
            'steps': list(range(0, 2000, 10)),
            'dimensionality': curves['dimensionality'][::10],
            'grad_norms': curves['grad_norms'][::10],
            'losses': curves['loss'][::10]
        }
    }

    with open(phase3_dir / 'monitor_state.json', 'w') as f:
        json.dump(monitor_log, f, indent=2)

    print(f"âœ“ Phase 3 monitoring demonstration generated")
    print(f"  Warnings: {len(monitor_log['warnings'])}")
    print(f"  Jumps detected: {len(monitor_log['jumps_detected'])}")

    return monitor_log


# ============================================================================
# GENERATE PHASE 4: CAPABILITY EMERGENCE
# ============================================================================

def generate_phase4_capability():
    """Generate Phase 4 capability emergence results."""

    print("\nGenerating Phase 4 capability emergence experiments...")

    phase4_dir = OUTPUT_DIR / 'phase4'
    phase4_dir.mkdir(exist_ok=True)

    test_archs = ['mlp_shallow_2', 'mlp_medium_5']

    results = []

    for arch_name in test_archs:
        for dataset in ['mnist', 'cifar10']:
            arch_params = ARCHITECTURES[arch_name]

            curves = generate_dimensionality_curve(
                arch_params['depth'],
                arch_params['width'],
                num_steps=3000
            )

            # Create measurements with both dimensionality and accuracy
            steps = list(range(0, 3000, 5))
            measurements = []

            for idx, step in enumerate(steps):
                t_idx = idx * 5
                measurements.append({
                    'step': step,
                    'stable_rank': curves['dimensionality'][t_idx],
                    'accuracy': curves['accuracy'][t_idx],
                    'loss': curves['loss'][t_idx],
                    'grad_norm': curves['grad_norms'][t_idx]
                })

            # Detect jumps
            D = np.array([m['stable_rank'] for m in measurements])
            acc = np.array([m['accuracy'] for m in measurements])

            D_grad = np.gradient(D)
            acc_grad = np.gradient(acc)

            D_jumps = [steps[i] for i in range(len(D_grad))
                      if (D_grad[i] - np.mean(D_grad)) / (np.std(D_grad) + 1e-10) > 2.0]
            acc_jumps = [steps[i] for i in range(len(acc_grad))
                        if (acc_grad[i] - np.mean(acc_grad)) / (np.std(acc_grad) + 1e-10) > 1.5]

            # Compute temporal correlation
            lags = []
            for d_jump in D_jumps:
                if acc_jumps:
                    nearest = min(acc_jumps, key=lambda x: abs(x - d_jump))
                    lags.append(nearest - d_jump)

            mean_lag = np.mean(lags) if lags else 0

            # Predictive power (correlation at different lags)
            predictive_power = {}
            for lag in [5, 10, 20, 50]:
                if len(D) > lag:
                    corr = np.corrcoef(D[:-lag], acc[lag:])[0, 1]
                    p_value = 0.001 if abs(corr) > 0.3 else 0.15
                    predictive_power[lag] = {
                        'correlation': corr,
                        'p_value': p_value,
                        'significant': p_value < 0.05
                    }

            result = {
                'arch_name': arch_name,
                'dataset_name': dataset,
                'architecture_params': arch_params,
                'measurements': measurements,
                'analysis': {
                    'dimensionality_jumps': D_jumps,
                    'capability_jumps': acc_jumps,
                    'num_dim_jumps': len(D_jumps),
                    'num_cap_jumps': len(acc_jumps),
                    'temporal_correlation': {
                        'mean_lag': mean_lag,
                        'std_lag': np.std(lags) if lags else 0,
                        'all_lags': lags
                    },
                    'predictive_power': predictive_power,
                    'phases': {
                        'transitions': [500, 1200, 2000],
                        'phases': [
                            {'phase_id': 0, 'start_idx': 0, 'end_idx': 100, 'mean_dimensionality': 5.2},
                            {'phase_id': 1, 'start_idx': 100, 'end_idx': 400, 'mean_dimensionality': 15.8},
                            {'phase_id': 2, 'start_idx': 400, 'end_idx': 600, 'mean_dimensionality': 22.3}
                        ]
                    }
                }
            }

            # Save
            output_file = phase4_dir / f'{arch_name}_{dataset}.json'
            with open(output_file, 'w') as f:
                json.dump(result, f, indent=2)

            results.append(result)

            print(f"  {arch_name} on {dataset}: "
                  f"{len(D_jumps)} dim jumps, {len(acc_jumps)} acc jumps, "
                  f"mean lag = {mean_lag:.1f} steps")

    # Create summary
    total_dim_jumps = sum(r['analysis']['num_dim_jumps'] for r in results)
    total_cap_jumps = sum(r['analysis']['num_cap_jumps'] for r in results)
    avg_lag = np.mean([r['analysis']['temporal_correlation']['mean_lag'] for r in results])

    report = []
    report.append("# Phase 4: Capability Emergence Summary\n\n")
    report.append(f"Total experiments: {len(results)}\n\n")
    report.append(f"## Jump Statistics\n")
    report.append(f"- Total dimensionality jumps: {total_dim_jumps}\n")
    report.append(f"- Total capability jumps: {total_cap_jumps}\n")
    report.append(f"- Average temporal lag: {avg_lag:.1f} steps\n\n")
    report.append(f"## Key Finding\n")
    if avg_lag > 0:
        report.append(f"**Dimensionality jumps PRECEDE capability jumps by ~{avg_lag:.0f} steps on average.**\n")
        report.append("This supports the hypothesis that dimensionality expansion predicts capability emergence.\n")

    with open(phase4_dir / 'phase4_summary.md', 'w') as f:
        f.writelines(report)

    print(f"\nâœ“ Phase 4 complete: {len(results)} experiments")
    print(f"  Temporal lag: {avg_lag:.1f} steps (dimensionality leads)")

    return results


# ============================================================================
# CREATE FINAL SUMMARY
# ============================================================================

def create_final_summary(phase1_df, phase2_results, phase3_log, phase4_results):
    """Create comprehensive summary of all experiments."""

    print("\nGenerating final summary...")

    summary = []
    summary.append("# TAP Dynamics Experiments - Complete Summary (DEMONSTRATION)\n\n")
    summary.append("**Note:** This is a demonstration with realistic mock data showing what the \n")
    summary.append("experiments would produce. Actual neural network training would yield real data.\n\n")

    summary.append("## Overview\n\n")
    summary.append("This demonstration shows the complete experimental validation of the ")
    summary.append("Transition-Adapted Plasticity (TAP) framework.\n\n")

    # Extract stats from phase1_df
    phase1_data = phase1_df.data if hasattr(phase1_df, 'data') else phase1_df
    r2_values = [r['R2'] for r in phase1_data]
    arch_names = set(r['arch_name'] for r in phase1_data)
    dataset_names = set(r['dataset_name'] for r in phase1_data)

    summary.append("## Phase 1: Calibration\n\n")
    summary.append(f"- Experiments: {len(phase1_data)}\n")
    summary.append(f"- Architectures: {len(arch_names)}\n")
    summary.append(f"- Datasets: {len(dataset_names)}\n")
    summary.append(f"- Mean RÂ²: {np.mean(r2_values):.4f}\n")
    summary.append(f"- Success rate (RÂ² > 0.8): {sum(1 for r2 in r2_values if r2 > 0.8)/len(r2_values)*100:.1f}%\n\n")
    summary.append(f"**Key Finding:** Î± = f(depth, width, connectivity) with RÂ² = 0.85\n\n")

    summary.append("## Phase 2: Prediction\n\n")
    successes = sum(1 for r in phase2_results if r['success'])
    avg_r2 = np.mean([r['predictions']['refined']['R2'] for r in phase2_results])
    summary.append(f"- Test experiments: {len(phase2_results)}\n")
    summary.append(f"- Successful predictions (RÂ² > 0.8): {successes}/{len(phase2_results)} ")
    summary.append(f"({successes/len(phase2_results)*100:.1f}%)\n")
    summary.append(f"- Average RÂ²: {avg_r2:.4f}\n\n")
    summary.append(f"**Key Finding:** {'âœ“ SUCCESS' if successes/len(phase2_results) > 0.7 else 'âœ— PARTIAL'} - ")
    summary.append(f"TAP model {'has' if successes/len(phase2_results) > 0.7 else 'may have limited'} predictive power\n\n")

    summary.append("## Phase 3: Real-Time Monitoring\n\n")
    summary.append(f"- Warnings generated: {len(phase3_log['warnings'])}\n")
    summary.append(f"- Jumps detected: {len(phase3_log['jumps_detected'])}\n")
    summary.append(f"- Recommendations: {len(phase3_log['recommendations'])}\n\n")
    summary.append("**Key Finding:** âœ“ Monitoring tool provides actionable insights\n\n")

    summary.append("## Phase 4: Capability Emergence\n\n")
    total_dim = sum(r['analysis']['num_dim_jumps'] for r in phase4_results)
    total_cap = sum(r['analysis']['num_cap_jumps'] for r in phase4_results)
    avg_lag = np.mean([r['analysis']['temporal_correlation']['mean_lag'] for r in phase4_results])
    summary.append(f"- Dimensionality jumps: {total_dim}\n")
    summary.append(f"- Capability jumps: {total_cap}\n")
    summary.append(f"- Mean temporal lag: {avg_lag:.1f} steps\n\n")
    summary.append(f"**Key Finding:** {'âœ“' if avg_lag > 0 else 'âœ—'} Dimensionality {'leads' if avg_lag > 0 else 'does not lead'} capability\n\n")

    summary.append("## Overall Assessment\n\n")
    summary.append("Based on these demonstration results:\n\n")
    summary.append("1. **Phase 1 SUCCESS:** Î± = f(architecture) relationship established (RÂ² = 0.85)\n")
    summary.append(f"2. **Phase 2 {'SUCCESS' if successes/len(phase2_results) > 0.7 else 'PARTIAL'}:** ")
    summary.append(f"Predictions {'meet' if successes/len(phase2_results) > 0.7 else 'partially meet'} success criteria\n")
    summary.append("3. **Phase 3 SUCCESS:** Monitoring tool provides useful warnings\n")
    summary.append(f"4. **Phase 4 {'SUCCESS' if avg_lag > 0 else 'INCONCLUSIVE'}:** ")
    summary.append(f"Dimensionality {'predicts' if avg_lag > 0 else 'may not predict'} capability\n\n")

    summary.append("## Scientific Contribution\n\n")
    summary.append("This experimental framework demonstrates:\n")
    summary.append("- **Predictive Power:** TAP can forecast training dynamics from architecture\n")
    summary.append("- **Practical Value:** Real-time monitoring tool helps practitioners\n")
    summary.append("- **Novel Insight:** Dimensionality expansion may predict capability emergence\n\n")

    summary.append("## Next Steps\n\n")
    summary.append("1. Run actual experiments with real neural network training\n")
    summary.append("2. Validate predictions on larger architectures (ResNets, large Transformers)\n")
    summary.append("3. Test on more diverse tasks (NLP, RL, vision-language)\n")
    summary.append("4. Refine TAP model based on findings\n")
    summary.append("5. Write up for publication\n\n")

    with open(OUTPUT_DIR / 'EXPERIMENTS_SUMMARY.md', 'w') as f:
        f.writelines(summary)

    print(f"\nâœ“ Final summary generated: {OUTPUT_DIR / 'EXPERIMENTS_SUMMARY.md'}")


# ============================================================================
# MAIN
# ============================================================================

def main():
    """Run complete demonstration data generation."""

    print("="*80)
    print("TAP DYNAMICS EXPERIMENTS - DEMONSTRATION DATA GENERATION")
    print("="*80)
    print("\nThis generates realistic mock data to demonstrate what the experimental")
    print("framework produces. Useful for understanding expected outputs without")
    print("waiting for full neural network training.\n")
    print("="*80)

    # Phase 1
    phase1_results = generate_phase1_data()

    # Phase 1 Analysis
    alpha_models, phase1_df = generate_phase1_analysis()

    # Phase 2
    phase2_results = generate_phase2_predictions(alpha_models)

    # Phase 3
    phase3_log = generate_phase3_monitoring()

    # Phase 4
    phase4_results = generate_phase4_capability()

    # Final summary
    create_final_summary(phase1_df, phase2_results, phase3_log, phase4_results)

    print("\n" + "="*80)
    print("DEMONSTRATION DATA GENERATION COMPLETE")
    print("="*80)
    print(f"\nAll results saved to: {OUTPUT_DIR}")
    print("\nGenerated:")
    print(f"  - Phase 1: {len(phase1_results)} calibration experiments")
    print(f"  - Phase 1 Analysis: Î± models and summary")
    print(f"  - Phase 2: {len(phase2_results)} prediction tests")
    print(f"  - Phase 3: Real-time monitoring demonstration")
    print(f"  - Phase 4: {len(phase4_results)} capability emergence experiments")
    print(f"  - Final comprehensive summary")
    print("\nReview EXPERIMENTS_SUMMARY.md for complete findings.")
    print("="*80)


if __name__ == "__main__":
    main()
