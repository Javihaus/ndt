# Running Experiments on Your Local Machine
## With Your Downloaded Datasets

---

## ğŸš¨ **IMPORTANT: Cloud vs Local Environment**

**Current Situation:**
- âœ… You have datasets on YOUR local computer in `experiments/new/data/`
- âŒ This cloud environment (where I run) **CANNOT access** your local files
- âœ… I've created all the code needed to run experiments
- ğŸ¯ **You need to run experiments on YOUR machine**

---

## ğŸ“ **Verify Your Setup**

Make sure you have this structure on YOUR local computer:

```
your-ndt-repo/
â””â”€â”€ experiments/new/
    â”œâ”€â”€ data/               # Your datasets
    â”‚   â”œâ”€â”€ cifar-10/
    â”‚   â”‚   â””â”€â”€ cifar-10-batches-py/
    â”‚   â”‚       â”œâ”€â”€ data_batch_1
    â”‚   â”‚       â”œâ”€â”€ data_batch_2
    â”‚   â”‚       â”œâ”€â”€ ...
    â”‚   â”‚       â””â”€â”€ test_batch
    â”‚   â”œâ”€â”€ imagenet/
    â”‚   â”‚   â””â”€â”€ train-00001-of-00021.parquet
    â”‚   â”œâ”€â”€ glue/
    â”‚   â”‚   â””â”€â”€ mnli/
    â”‚   â”‚       â””â”€â”€ train-00000-of-00001.parquet
    â”‚   â””â”€â”€ conceptual_captions/
    â”‚       â””â”€â”€ [your files]
    â”‚
    â”œâ”€â”€ modern_dataset_loaders.py     # â† I created this
    â”œâ”€â”€ run_experiments_parallel.py   # â† I created this
    â””â”€â”€ phase1_calibration.py         # â† Updated by me
```

---

## ğŸš€ **Running Experiments Locally (Parallel)**

### **Option 1: Parallel Execution (FAST - ~10-20 hours)**

```bash
cd /path/to/your/ndt/experiments/new

# Run 4 experiments in parallel (recommended)
python run_experiments_parallel.py --num-processes 4 --num-steps 2000

# Or more aggressive (if you have 8+ CPU cores)
python run_experiments_parallel.py --num-processes 8 --num-steps 2000
```

**Benefits:**
- âœ… 4x faster than sequential
- âœ… ~10-20 hours for all 30 experiments
- âœ… Uses multiple CPU cores

**Requirements:**
- At least 4 CPU cores
- 8-16GB RAM

---

### **Option 2: Sequential Execution (SLOW - ~40-80 hours)**

```bash
cd /path/to/your/ndt/experiments/new

# Run one at a time
python phase1_calibration.py --num-steps 2000 --output-dir results/phase1_full
```

---

## ğŸ“Š **What Will Happen**

### **30 Real Experiments Will Run:**

| Architecture Type | Dataset | Count |
|------------------|---------|-------|
| MLPs (8 variants) | MNIST + CIFAR-10 | 16 |
| CNNs (3 variants) | CIFAR-10 | 3 |
| ResNet18 | CIFAR-10 + ImageNet | 2 |
| Transformers (5 variants) | GLUE + CIFAR-10 | 9 |
| **TOTAL** | | **30** |

### **Each Experiment:**
- 2000 training steps
- 400 dimensionality measurements (every 5 steps)
- Real gradient descent on real data
- Saves ~500KB-1.5MB JSON result

---

## â±ï¸ **Time Estimates**

| Method | Time | Notes |
|--------|------|-------|
| **Parallel (4 cores)** | 10-20 hours | **Recommended** |
| **Parallel (8 cores)** | 5-10 hours | If you have powerful CPU |
| Sequential | 40-80 hours | Not recommended |

**Per experiment:** ~30-60 minutes

---

## ğŸ” **Monitoring Progress**

While running, you'll see:

```
======================================================================
PARALLEL EXPERIMENT RUNNER
======================================================================
Total experiments: 30
Parallel processes: 4
...

[1/30] âœ“ Completed mlp_shallow_2 Ã— cifar10 in 45.2 min
[2/30] â­  Skipped mlp_shallow_2 Ã— mnist (already exists)
[3/30] âœ“ Completed cnn_shallow Ã— cifar10 in 52.1 min
...
```

**Results saved to:** `experiments/new/results/phase1_full/`

---

## ğŸ› **If Something Fails**

### **Dataset Not Found:**
```
âœ— Failed mlp_shallow_2 Ã— cifar10: [Errno 2] No such file or directory
```

**Fix:** Check that `experiments/new/data/cifar-10/cifar-10-batches-py/data_batch_1` exists

### **Out of Memory:**
```
RuntimeError: CUDA out of memory
```

**Fix:** Reduce number of parallel processes:
```bash
python run_experiments_parallel.py --num-processes 2
```

### **Crashes:**
The runner is designed to **resume from where it left off**. Just run the command again - it will skip completed experiments.

---

## ğŸ’¾ **After Completion**

### **You'll Have:**
- âœ… 30 JSON result files (~20-40MB total)
- âœ… Real training data for TAP validation
- âœ… Ready for Phase 2-4 analysis

### **Next Steps:**
1. Commit results to git:
```bash
git add experiments/new/results/phase1_full/*.json
git commit -m "Add Phase 1 results: 30 experiments with modern datasets"
git push origin claude/review-repo-cla-01QN2r8oCF5Ao3Z1BGhtCgSE
```

2. Run Phase 1 analysis:
```bash
python phase1_analysis.py --results-dir results/phase1_full
```

---

## â“ **Questions?**

- **Can I run just a subset?** Edit `EXPERIMENT_PLAN` in `run_experiments_parallel.py`
- **Can I use GPU?** Yes, it will auto-detect and use if available
- **Can I stop/resume?** Yes, it skips completed experiments automatically

---

## ğŸ¯ **Start Now**

```bash
cd /path/to/your/ndt/experiments/new
python run_experiments_parallel.py --num-processes 4 --num-steps 2000
```

**Let it run overnight!** â˜•
