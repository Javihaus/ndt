{
  "arch": "transformer_shallow",
  "dataset": "mnist",
  "jump_analysis": {
    "num_jumps": 9,
    "pattern": "early_cascade",
    "characteristics": "Concentrated burst of jumps in early training",
    "early_jumps": 7,
    "late_jumps": 2,
    "avg_magnitude": 0.10103468190093237,
    "max_magnitude": 0.16393126003764427,
    "avg_z_score": 4.421800333071534,
    "avg_gap": 8.625,
    "top_5_jumps": [
      {
        "index": 0,
        "magnitude": 0.16393126003764427,
        "z_score": 7.260119476518395
      },
      {
        "index": 1,
        "magnitude": 0.15636237017467028,
        "z_score": 6.91855995723859
      },
      {
        "index": 2,
        "magnitude": 0.14198219516997856,
        "z_score": 6.269629169410738
      },
      {
        "index": 3,
        "magnitude": 0.12225964413778573,
        "z_score": 5.379614230432581
      },
      {
        "index": 4,
        "magnitude": 0.09921764893484628,
        "z_score": 4.339803512183978
      }
    ]
  },
  "growth_analysis": {
    "alpha": 5.012531328320803e-05,
    "D_max": 13.179426382273936,
    "R2": 0.26101689841129794,
    "t0": 7.287672420531192e-10,
    "growth_type": "very_slow",
    "growth_description": "Very slow expansion, possible bottleneck",
    "fit_quality": "poor",
    "fit_description": "Significant deviation from TAP model - likely jumps/transitions"
  },
  "mechanistic_interpretation": {
    "interpretations": [
      "Transformers show discrete attention pattern transitions rather than gradual feature refinement."
    ],
    "hypotheses": [
      "Jumps may correspond to attention heads specializing for different aspects of the input."
    ],
    "suggested_investigations": [
      "Feature visualization at jump indices: [0, 1, 2, 3, 4]",
      "Compare activation patterns before/after largest jumps",
      "Analyze which neurons become active at each transition"
    ]
  }
}