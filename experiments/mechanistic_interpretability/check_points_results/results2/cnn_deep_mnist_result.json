{
  "experiment_name": "cnn_deep_mnist",
  "measurements": [
    {
      "step": 0,
      "loss": 2.2819700241088867,
      "grad_norm": 1.1963886978103524
    },
    {
      "step": 5,
      "loss": 2.1322858333587646,
      "grad_norm": 1.2610715823838161
    },
    {
      "step": 10,
      "loss": 1.7064021825790405,
      "grad_norm": 2.8790841616473544
    },
    {
      "step": 15,
      "loss": 1.246328592300415,
      "grad_norm": 6.516571171713251
    },
    {
      "step": 20,
      "loss": 0.7842127084732056,
      "grad_norm": 7.685547097237464
    },
    {
      "step": 25,
      "loss": 0.6279209852218628,
      "grad_norm": 9.730472629411997
    },
    {
      "step": 30,
      "loss": 0.4258177876472473,
      "grad_norm": 7.884722368758782
    },
    {
      "step": 35,
      "loss": 0.7868310809135437,
      "grad_norm": 9.414826973201292
    },
    {
      "step": 40,
      "loss": 0.34494543075561523,
      "grad_norm": 6.147271885101312
    },
    {
      "step": 45,
      "loss": 0.2837234139442444,
      "grad_norm": 6.608483745651781
    },
    {
      "step": 50,
      "loss": 0.41693800687789917,
      "grad_norm": 7.598367564981542
    },
    {
      "step": 55,
      "loss": 0.2110801637172699,
      "grad_norm": 4.756312479704529
    },
    {
      "step": 60,
      "loss": 0.4800364077091217,
      "grad_norm": 5.760114513347001
    },
    {
      "step": 65,
      "loss": 0.1180015355348587,
      "grad_norm": 3.575997621254422
    },
    {
      "step": 70,
      "loss": 0.3657858371734619,
      "grad_norm": 3.8558549303821605
    },
    {
      "step": 75,
      "loss": 0.24559181928634644,
      "grad_norm": 3.1255304369629426
    },
    {
      "step": 80,
      "loss": 0.14453786611557007,
      "grad_norm": 2.956915949405792
    },
    {
      "step": 85,
      "loss": 0.1779966652393341,
      "grad_norm": 5.611982529342406
    },
    {
      "step": 90,
      "loss": 0.09145035594701767,
      "grad_norm": 2.021702548167455
    },
    {
      "step": 95,
      "loss": 0.09265017509460449,
      "grad_norm": 2.262131267253499
    },
    {
      "step": 100,
      "loss": 0.3122689723968506,
      "grad_norm": 4.923629279593412
    },
    {
      "step": 105,
      "loss": 0.17765802145004272,
      "grad_norm": 2.833892885273029
    },
    {
      "step": 110,
      "loss": 0.1756618767976761,
      "grad_norm": 1.9493646126471103
    },
    {
      "step": 115,
      "loss": 0.18221385776996613,
      "grad_norm": 3.937272915853745
    },
    {
      "step": 120,
      "loss": 0.20412620902061462,
      "grad_norm": 4.3385623430764495
    },
    {
      "step": 125,
      "loss": 0.20808179676532745,
      "grad_norm": 2.76750852036488
    },
    {
      "step": 130,
      "loss": 0.10534921288490295,
      "grad_norm": 2.1114136323824284
    },
    {
      "step": 135,
      "loss": 0.16683249175548553,
      "grad_norm": 2.7788967893027747
    },
    {
      "step": 140,
      "loss": 0.08306333422660828,
      "grad_norm": 2.4651754302996296
    },
    {
      "step": 145,
      "loss": 0.14388513565063477,
      "grad_norm": 3.53193462334364
    },
    {
      "step": 150,
      "loss": 0.1609123796224594,
      "grad_norm": 3.75640081306723
    },
    {
      "step": 155,
      "loss": 0.03687775135040283,
      "grad_norm": 1.0666859788382737
    },
    {
      "step": 160,
      "loss": 0.060731854289770126,
      "grad_norm": 2.1045453938176184
    },
    {
      "step": 165,
      "loss": 0.12321016937494278,
      "grad_norm": 2.6200887998838187
    },
    {
      "step": 170,
      "loss": 0.07562903314828873,
      "grad_norm": 2.025786177535266
    },
    {
      "step": 175,
      "loss": 0.049498580396175385,
      "grad_norm": 1.3773882734999001
    },
    {
      "step": 180,
      "loss": 0.20569291710853577,
      "grad_norm": 4.2922634362182475
    },
    {
      "step": 185,
      "loss": 0.19248896837234497,
      "grad_norm": 2.1860844156221715
    },
    {
      "step": 190,
      "loss": 0.055673833936452866,
      "grad_norm": 1.336353962951321
    },
    {
      "step": 195,
      "loss": 0.10268458724021912,
      "grad_norm": 3.055170143203846
    },
    {
      "step": 200,
      "loss": 0.24710863828659058,
      "grad_norm": 4.635759846487911
    },
    {
      "step": 205,
      "loss": 0.0893656313419342,
      "grad_norm": 2.8967688394709197
    },
    {
      "step": 210,
      "loss": 0.1130460649728775,
      "grad_norm": 2.9128966863236205
    },
    {
      "step": 215,
      "loss": 0.10154454410076141,
      "grad_norm": 2.1455984445246927
    },
    {
      "step": 220,
      "loss": 0.09119680523872375,
      "grad_norm": 2.0010484700700517
    },
    {
      "step": 225,
      "loss": 0.16172517836093903,
      "grad_norm": 3.702437446072185
    },
    {
      "step": 230,
      "loss": 0.0816788375377655,
      "grad_norm": 2.7208039317299826
    },
    {
      "step": 235,
      "loss": 0.16255025565624237,
      "grad_norm": 2.2710378664922977
    },
    {
      "step": 240,
      "loss": 0.1864134669303894,
      "grad_norm": 4.0403436013105205
    },
    {
      "step": 245,
      "loss": 0.1219433918595314,
      "grad_norm": 2.700279260024572
    },
    {
      "step": 250,
      "loss": 0.22248020768165588,
      "grad_norm": 4.21720091897473
    },
    {
      "step": 255,
      "loss": 0.05160777270793915,
      "grad_norm": 1.9834437973670034
    },
    {
      "step": 260,
      "loss": 0.17663954198360443,
      "grad_norm": 2.966932502519338
    },
    {
      "step": 265,
      "loss": 0.03952528536319733,
      "grad_norm": 1.0508514283010704
    },
    {
      "step": 270,
      "loss": 0.09055563807487488,
      "grad_norm": 2.3122524440227266
    },
    {
      "step": 275,
      "loss": 0.048694901168346405,
      "grad_norm": 1.723680872896446
    },
    {
      "step": 280,
      "loss": 0.1669396609067917,
      "grad_norm": 2.1105559182714684
    },
    {
      "step": 285,
      "loss": 0.05732107162475586,
      "grad_norm": 1.4128578999409078
    },
    {
      "step": 290,
      "loss": 0.14556780457496643,
      "grad_norm": 2.46036025708386
    },
    {
      "step": 295,
      "loss": 0.2268683761358261,
      "grad_norm": 1.9182761853181838
    },
    {
      "step": 300,
      "loss": 0.06170637905597687,
      "grad_norm": 1.150902552855167
    },
    {
      "step": 305,
      "loss": 0.07413707673549652,
      "grad_norm": 1.196867737111253
    },
    {
      "step": 310,
      "loss": 0.08656739443540573,
      "grad_norm": 1.9700142276158743
    },
    {
      "step": 315,
      "loss": 0.10466448962688446,
      "grad_norm": 1.5690497775322974
    },
    {
      "step": 320,
      "loss": 0.07559115439653397,
      "grad_norm": 1.7546642108036723
    },
    {
      "step": 325,
      "loss": 0.05632958561182022,
      "grad_norm": 1.4822863325411333
    },
    {
      "step": 330,
      "loss": 0.08344286680221558,
      "grad_norm": 1.4833215171940317
    },
    {
      "step": 335,
      "loss": 0.07779692858457565,
      "grad_norm": 1.406120353640831
    },
    {
      "step": 340,
      "loss": 0.056650035083293915,
      "grad_norm": 1.1995106098571973
    },
    {
      "step": 345,
      "loss": 0.04357658326625824,
      "grad_norm": 1.0567235241460218
    },
    {
      "step": 350,
      "loss": 0.06800907105207443,
      "grad_norm": 1.685577111412159
    },
    {
      "step": 355,
      "loss": 0.06994814425706863,
      "grad_norm": 1.5233969618021135
    },
    {
      "step": 360,
      "loss": 0.019203990697860718,
      "grad_norm": 0.7392250115495179
    },
    {
      "step": 365,
      "loss": 0.034320056438446045,
      "grad_norm": 1.644631802520827
    },
    {
      "step": 370,
      "loss": 0.04753733426332474,
      "grad_norm": 1.4426580388220065
    },
    {
      "step": 375,
      "loss": 0.18082298338413239,
      "grad_norm": 3.6939467460238866
    },
    {
      "step": 380,
      "loss": 0.10660506784915924,
      "grad_norm": 2.058013600750371
    },
    {
      "step": 385,
      "loss": 0.016671596094965935,
      "grad_norm": 0.6811006271859411
    },
    {
      "step": 390,
      "loss": 0.07436623424291611,
      "grad_norm": 1.7273656084837077
    },
    {
      "step": 395,
      "loss": 0.021181220188736916,
      "grad_norm": 1.121328129588617
    },
    {
      "step": 400,
      "loss": 0.008608441799879074,
      "grad_norm": 0.3286728977060186
    },
    {
      "step": 405,
      "loss": 0.11612018942832947,
      "grad_norm": 2.7556670046661838
    },
    {
      "step": 410,
      "loss": 0.15298302471637726,
      "grad_norm": 3.2036200046608942
    },
    {
      "step": 415,
      "loss": 0.023442428559064865,
      "grad_norm": 1.2562759347361847
    },
    {
      "step": 420,
      "loss": 0.062289126217365265,
      "grad_norm": 1.9801969927961118
    },
    {
      "step": 425,
      "loss": 0.11522587388753891,
      "grad_norm": 3.007395658419301
    },
    {
      "step": 430,
      "loss": 0.06669668853282928,
      "grad_norm": 2.1198502616309973
    },
    {
      "step": 435,
      "loss": 0.047473255544900894,
      "grad_norm": 0.9328888797185179
    },
    {
      "step": 440,
      "loss": 0.05962618067860603,
      "grad_norm": 1.0873437883350952
    },
    {
      "step": 445,
      "loss": 0.03879118710756302,
      "grad_norm": 1.0995433467941353
    },
    {
      "step": 450,
      "loss": 0.01917373389005661,
      "grad_norm": 0.8477714264888851
    },
    {
      "step": 455,
      "loss": 0.05570881441235542,
      "grad_norm": 1.3459370456385678
    },
    {
      "step": 460,
      "loss": 0.02160249650478363,
      "grad_norm": 1.034452670687014
    },
    {
      "step": 465,
      "loss": 0.03179500997066498,
      "grad_norm": 0.9883979060129215
    },
    {
      "step": 470,
      "loss": 0.07607358694076538,
      "grad_norm": 1.6498298653730925
    },
    {
      "step": 475,
      "loss": 0.04579497501254082,
      "grad_norm": 1.4870352121238062
    },
    {
      "step": 480,
      "loss": 0.08997170627117157,
      "grad_norm": 1.6379206711043925
    },
    {
      "step": 485,
      "loss": 0.033627189695835114,
      "grad_norm": 1.1329938092986467
    },
    {
      "step": 490,
      "loss": 0.0761953592300415,
      "grad_norm": 1.8353498043459626
    },
    {
      "step": 495,
      "loss": 0.013406077399849892,
      "grad_norm": 0.5824088097409736
    },
    {
      "step": 500,
      "loss": 0.07570837438106537,
      "grad_norm": 1.7459791456649625
    },
    {
      "step": 505,
      "loss": 0.10254299640655518,
      "grad_norm": 2.2232158005492657
    },
    {
      "step": 510,
      "loss": 0.10337214171886444,
      "grad_norm": 1.9341038446028853
    },
    {
      "step": 515,
      "loss": 0.04575096443295479,
      "grad_norm": 1.6942005115220544
    },
    {
      "step": 520,
      "loss": 0.04648049548268318,
      "grad_norm": 1.2126565802746947
    },
    {
      "step": 525,
      "loss": 0.010891718789935112,
      "grad_norm": 0.45205794523645737
    },
    {
      "step": 530,
      "loss": 0.1911558359861374,
      "grad_norm": 2.080644939263085
    },
    {
      "step": 535,
      "loss": 0.10681270062923431,
      "grad_norm": 2.4959151381083777
    },
    {
      "step": 540,
      "loss": 0.15190036594867706,
      "grad_norm": 2.5455476213556727
    },
    {
      "step": 545,
      "loss": 0.028689246624708176,
      "grad_norm": 1.0237928053166743
    },
    {
      "step": 550,
      "loss": 0.085662841796875,
      "grad_norm": 1.4012217589648763
    },
    {
      "step": 555,
      "loss": 0.04340188950300217,
      "grad_norm": 1.4299238421847846
    },
    {
      "step": 560,
      "loss": 0.26574811339378357,
      "grad_norm": 2.0872587788159316
    },
    {
      "step": 565,
      "loss": 0.06285625696182251,
      "grad_norm": 1.6860508603292181
    },
    {
      "step": 570,
      "loss": 0.09140630066394806,
      "grad_norm": 2.4308051328742613
    },
    {
      "step": 575,
      "loss": 0.12800998985767365,
      "grad_norm": 2.7404814183302237
    },
    {
      "step": 580,
      "loss": 0.06484530121088028,
      "grad_norm": 1.8815834937404088
    },
    {
      "step": 585,
      "loss": 0.03078635036945343,
      "grad_norm": 1.001180066130578
    },
    {
      "step": 590,
      "loss": 0.06225600466132164,
      "grad_norm": 1.489649972274935
    },
    {
      "step": 595,
      "loss": 0.041683271527290344,
      "grad_norm": 1.188702884270189
    },
    {
      "step": 600,
      "loss": 0.1268034428358078,
      "grad_norm": 1.7886490854230837
    },
    {
      "step": 605,
      "loss": 0.031903594732284546,
      "grad_norm": 0.9398267454999994
    },
    {
      "step": 610,
      "loss": 0.006685634609311819,
      "grad_norm": 0.29626273489411825
    },
    {
      "step": 615,
      "loss": 0.1458912342786789,
      "grad_norm": 2.2553798901085034
    },
    {
      "step": 620,
      "loss": 0.08443772792816162,
      "grad_norm": 2.8384338179900905
    },
    {
      "step": 625,
      "loss": 0.004468307830393314,
      "grad_norm": 0.11523875281551188
    },
    {
      "step": 630,
      "loss": 0.012388002127408981,
      "grad_norm": 0.5659492443244762
    },
    {
      "step": 635,
      "loss": 0.12135432660579681,
      "grad_norm": 2.260830622026549
    },
    {
      "step": 640,
      "loss": 0.02037179470062256,
      "grad_norm": 1.0430052369134513
    },
    {
      "step": 645,
      "loss": 0.05947592854499817,
      "grad_norm": 1.7299661120138632
    },
    {
      "step": 650,
      "loss": 0.09318041801452637,
      "grad_norm": 1.8402305659832288
    },
    {
      "step": 655,
      "loss": 0.12961381673812866,
      "grad_norm": 1.8166076347410862
    },
    {
      "step": 660,
      "loss": 0.016398299485445023,
      "grad_norm": 0.5124429240813154
    },
    {
      "step": 665,
      "loss": 0.07067887485027313,
      "grad_norm": 1.8396508556338798
    },
    {
      "step": 670,
      "loss": 0.11568747460842133,
      "grad_norm": 2.0101959858182066
    },
    {
      "step": 675,
      "loss": 0.11782945692539215,
      "grad_norm": 2.672043451939843
    },
    {
      "step": 680,
      "loss": 0.10352934151887894,
      "grad_norm": 2.13705416791528
    },
    {
      "step": 685,
      "loss": 0.08646541088819504,
      "grad_norm": 2.3412610413096746
    },
    {
      "step": 690,
      "loss": 0.011804976500570774,
      "grad_norm": 0.39237809952819613
    },
    {
      "step": 695,
      "loss": 0.02190866693854332,
      "grad_norm": 0.46427294326239665
    },
    {
      "step": 700,
      "loss": 0.06752943992614746,
      "grad_norm": 1.6498988950535538
    },
    {
      "step": 705,
      "loss": 0.02205471880733967,
      "grad_norm": 1.1740305153033648
    },
    {
      "step": 710,
      "loss": 0.04985552653670311,
      "grad_norm": 1.5282938598308773
    },
    {
      "step": 715,
      "loss": 0.035104475915431976,
      "grad_norm": 1.6282796597273133
    },
    {
      "step": 720,
      "loss": 0.023538578301668167,
      "grad_norm": 0.9936810857717672
    },
    {
      "step": 725,
      "loss": 0.0525515116751194,
      "grad_norm": 1.683022283465494
    },
    {
      "step": 730,
      "loss": 0.030995884910225868,
      "grad_norm": 1.1044231379501512
    },
    {
      "step": 735,
      "loss": 0.026079244911670685,
      "grad_norm": 0.7590216306981648
    },
    {
      "step": 740,
      "loss": 0.012976977974176407,
      "grad_norm": 0.7571140698094554
    },
    {
      "step": 745,
      "loss": 0.025518441572785378,
      "grad_norm": 0.7922898277344685
    },
    {
      "step": 750,
      "loss": 0.2231927067041397,
      "grad_norm": 2.5631668415920736
    },
    {
      "step": 755,
      "loss": 0.06642644852399826,
      "grad_norm": 1.2516921681454376
    },
    {
      "step": 760,
      "loss": 0.021566227078437805,
      "grad_norm": 0.7418923166732229
    },
    {
      "step": 765,
      "loss": 0.07937051355838776,
      "grad_norm": 1.751739591609398
    },
    {
      "step": 770,
      "loss": 0.019046157598495483,
      "grad_norm": 0.8319402721828901
    },
    {
      "step": 775,
      "loss": 0.002154674381017685,
      "grad_norm": 0.10686058082316034
    },
    {
      "step": 780,
      "loss": 0.07228455692529678,
      "grad_norm": 1.7340579109893164
    },
    {
      "step": 785,
      "loss": 0.058648161590099335,
      "grad_norm": 1.8378783613438947
    },
    {
      "step": 790,
      "loss": 0.05162828788161278,
      "grad_norm": 1.7330371755875116
    },
    {
      "step": 795,
      "loss": 0.04268520697951317,
      "grad_norm": 1.490075545376422
    },
    {
      "step": 800,
      "loss": 0.1129092127084732,
      "grad_norm": 1.7294132406239322
    },
    {
      "step": 805,
      "loss": 0.06524694710969925,
      "grad_norm": 2.1740057165548365
    },
    {
      "step": 810,
      "loss": 0.06536498665809631,
      "grad_norm": 1.4913489615259143
    },
    {
      "step": 815,
      "loss": 0.009642476215958595,
      "grad_norm": 0.3145602533351891
    },
    {
      "step": 820,
      "loss": 0.009438902139663696,
      "grad_norm": 0.34238522164164625
    },
    {
      "step": 825,
      "loss": 0.03812280297279358,
      "grad_norm": 1.4353423297480017
    },
    {
      "step": 830,
      "loss": 0.06523603945970535,
      "grad_norm": 2.186180269502404
    },
    {
      "step": 835,
      "loss": 0.006212538108229637,
      "grad_norm": 0.1701842684557129
    },
    {
      "step": 840,
      "loss": 0.03551653400063515,
      "grad_norm": 1.0980917087909814
    },
    {
      "step": 845,
      "loss": 0.06223154813051224,
      "grad_norm": 1.458901564568007
    },
    {
      "step": 850,
      "loss": 0.02472132071852684,
      "grad_norm": 1.052431079198235
    },
    {
      "step": 855,
      "loss": 0.032081350684165955,
      "grad_norm": 0.980537127062757
    },
    {
      "step": 860,
      "loss": 0.054158151149749756,
      "grad_norm": 1.319489127698786
    },
    {
      "step": 865,
      "loss": 0.07480276376008987,
      "grad_norm": 1.5135530659802847
    },
    {
      "step": 870,
      "loss": 0.05452268198132515,
      "grad_norm": 1.6911830983466105
    },
    {
      "step": 875,
      "loss": 0.04121861234307289,
      "grad_norm": 1.0522559220629724
    },
    {
      "step": 880,
      "loss": 0.14294244349002838,
      "grad_norm": 2.6163344701473643
    },
    {
      "step": 885,
      "loss": 0.05822068825364113,
      "grad_norm": 1.6669397879111152
    },
    {
      "step": 890,
      "loss": 0.10814250260591507,
      "grad_norm": 1.6034786387647788
    },
    {
      "step": 895,
      "loss": 0.04876840487122536,
      "grad_norm": 1.1093022503826566
    },
    {
      "step": 900,
      "loss": 0.016426168382167816,
      "grad_norm": 0.5843590311089553
    },
    {
      "step": 905,
      "loss": 0.025124823674559593,
      "grad_norm": 0.6587694879210112
    },
    {
      "step": 910,
      "loss": 0.043434832245111465,
      "grad_norm": 1.5293645812741783
    },
    {
      "step": 915,
      "loss": 0.05350456014275551,
      "grad_norm": 1.2621811279071942
    },
    {
      "step": 920,
      "loss": 0.04498130455613136,
      "grad_norm": 1.1816599332273334
    },
    {
      "step": 925,
      "loss": 0.00616052933037281,
      "grad_norm": 0.23070884209797657
    },
    {
      "step": 930,
      "loss": 0.005369286052882671,
      "grad_norm": 0.1780219690509113
    },
    {
      "step": 935,
      "loss": 0.04395144432783127,
      "grad_norm": 1.4753276755706382
    },
    {
      "step": 940,
      "loss": 0.02685851790010929,
      "grad_norm": 0.8228182906399761
    },
    {
      "step": 945,
      "loss": 0.09731210768222809,
      "grad_norm": 1.3710374069329194
    },
    {
      "step": 950,
      "loss": 0.011159847490489483,
      "grad_norm": 0.6652007167795525
    },
    {
      "step": 955,
      "loss": 0.023034460842609406,
      "grad_norm": 0.6485895221109811
    },
    {
      "step": 960,
      "loss": 0.07318742573261261,
      "grad_norm": 1.7309945769818473
    },
    {
      "step": 965,
      "loss": 0.023233067244291306,
      "grad_norm": 0.9859211131884164
    },
    {
      "step": 970,
      "loss": 0.008256977424025536,
      "grad_norm": 0.4070810126284972
    },
    {
      "step": 975,
      "loss": 0.03134206309914589,
      "grad_norm": 1.4890328067715293
    },
    {
      "step": 980,
      "loss": 0.04647982493042946,
      "grad_norm": 1.1776558711057283
    },
    {
      "step": 985,
      "loss": 0.00528348283842206,
      "grad_norm": 0.19167434095012148
    },
    {
      "step": 990,
      "loss": 0.035325758159160614,
      "grad_norm": 1.1548948067363112
    },
    {
      "step": 995,
      "loss": 0.02173931710422039,
      "grad_norm": 0.7038157478755734
    },
    {
      "step": 1000,
      "loss": 0.02626267820596695,
      "grad_norm": 0.9815397814170659
    },
    {
      "step": 1005,
      "loss": 0.057496439665555954,
      "grad_norm": 1.5184020910255072
    },
    {
      "step": 1010,
      "loss": 0.08519037812948227,
      "grad_norm": 1.6333602671446712
    },
    {
      "step": 1015,
      "loss": 0.016160277649760246,
      "grad_norm": 0.8736394648904229
    },
    {
      "step": 1020,
      "loss": 0.01731068268418312,
      "grad_norm": 0.7812603617976777
    },
    {
      "step": 1025,
      "loss": 0.03409788757562637,
      "grad_norm": 1.4208420159474648
    },
    {
      "step": 1030,
      "loss": 0.01439752895385027,
      "grad_norm": 0.579813312397044
    },
    {
      "step": 1035,
      "loss": 0.0332789309322834,
      "grad_norm": 1.2026762049633597
    },
    {
      "step": 1040,
      "loss": 0.058810971677303314,
      "grad_norm": 1.8038661910172313
    },
    {
      "step": 1045,
      "loss": 0.1020544096827507,
      "grad_norm": 1.7129809629332966
    },
    {
      "step": 1050,
      "loss": 0.009953999891877174,
      "grad_norm": 0.4445905388767416
    },
    {
      "step": 1055,
      "loss": 0.09039755910634995,
      "grad_norm": 2.2014636380147605
    },
    {
      "step": 1060,
      "loss": 0.0230401661247015,
      "grad_norm": 1.0624752062011906
    },
    {
      "step": 1065,
      "loss": 0.0035110292956233025,
      "grad_norm": 0.15320420939368493
    },
    {
      "step": 1070,
      "loss": 0.024861091747879982,
      "grad_norm": 1.1628300583531057
    },
    {
      "step": 1075,
      "loss": 0.027506619691848755,
      "grad_norm": 0.9652632384581381
    },
    {
      "step": 1080,
      "loss": 0.017708798870444298,
      "grad_norm": 0.5630425052627405
    },
    {
      "step": 1085,
      "loss": 0.008270367048680782,
      "grad_norm": 0.29850156532624966
    },
    {
      "step": 1090,
      "loss": 0.033626116812229156,
      "grad_norm": 1.3424692991269112
    },
    {
      "step": 1095,
      "loss": 0.019704965874552727,
      "grad_norm": 0.9938053968548839
    },
    {
      "step": 1100,
      "loss": 0.0030115251429378986,
      "grad_norm": 0.10179957692579576
    },
    {
      "step": 1105,
      "loss": 0.00479543162509799,
      "grad_norm": 0.11473496176587283
    },
    {
      "step": 1110,
      "loss": 0.01830391399562359,
      "grad_norm": 0.7165018751392201
    },
    {
      "step": 1115,
      "loss": 0.027673248201608658,
      "grad_norm": 1.1073199684418837
    },
    {
      "step": 1120,
      "loss": 0.026071541011333466,
      "grad_norm": 1.0213543949638235
    },
    {
      "step": 1125,
      "loss": 0.02038429118692875,
      "grad_norm": 0.8653380392858161
    },
    {
      "step": 1130,
      "loss": 0.05793537199497223,
      "grad_norm": 1.338569708074194
    },
    {
      "step": 1135,
      "loss": 0.018020406365394592,
      "grad_norm": 0.6245973927877485
    },
    {
      "step": 1140,
      "loss": 0.01178425457328558,
      "grad_norm": 0.32415308723261615
    },
    {
      "step": 1145,
      "loss": 0.0161733515560627,
      "grad_norm": 0.7638625387266739
    },
    {
      "step": 1150,
      "loss": 0.01764773577451706,
      "grad_norm": 0.5762269240212077
    },
    {
      "step": 1155,
      "loss": 0.06241109222173691,
      "grad_norm": 1.3818229343150776
    },
    {
      "step": 1160,
      "loss": 0.024863654747605324,
      "grad_norm": 0.8851580803352822
    },
    {
      "step": 1165,
      "loss": 0.024538977071642876,
      "grad_norm": 1.1875344967183494
    },
    {
      "step": 1170,
      "loss": 0.0443025603890419,
      "grad_norm": 1.5511502746855006
    },
    {
      "step": 1175,
      "loss": 0.013940204866230488,
      "grad_norm": 0.5380487755819453
    },
    {
      "step": 1180,
      "loss": 0.3759426474571228,
      "grad_norm": 1.979140209737737
    },
    {
      "step": 1185,
      "loss": 0.06604243814945221,
      "grad_norm": 1.4501059943627197
    },
    {
      "step": 1190,
      "loss": 0.047563355416059494,
      "grad_norm": 1.4159816746333114
    },
    {
      "step": 1195,
      "loss": 0.016437748447060585,
      "grad_norm": 0.5976926273670361
    },
    {
      "step": 1200,
      "loss": 0.003520392579957843,
      "grad_norm": 0.14087260636855012
    },
    {
      "step": 1205,
      "loss": 0.015030174516141415,
      "grad_norm": 0.512096689298024
    },
    {
      "step": 1210,
      "loss": 0.03260432183742523,
      "grad_norm": 0.8777901895494122
    },
    {
      "step": 1215,
      "loss": 0.01413976401090622,
      "grad_norm": 0.7093742385017949
    },
    {
      "step": 1220,
      "loss": 0.02254369854927063,
      "grad_norm": 0.7797056278689295
    },
    {
      "step": 1225,
      "loss": 0.02692398615181446,
      "grad_norm": 0.6424331791812263
    },
    {
      "step": 1230,
      "loss": 0.0041034165769815445,
      "grad_norm": 0.1444039885168456
    },
    {
      "step": 1235,
      "loss": 0.002307793591171503,
      "grad_norm": 0.07829287442539085
    },
    {
      "step": 1240,
      "loss": 0.029661979526281357,
      "grad_norm": 0.876205823730005
    },
    {
      "step": 1245,
      "loss": 0.03267355263233185,
      "grad_norm": 1.1031120482856902
    },
    {
      "step": 1250,
      "loss": 0.0025603719986975193,
      "grad_norm": 0.11481271673017455
    },
    {
      "step": 1255,
      "loss": 0.07582258433103561,
      "grad_norm": 1.5724278006394024
    },
    {
      "step": 1260,
      "loss": 0.14625294506549835,
      "grad_norm": 1.36477674092128
    },
    {
      "step": 1265,
      "loss": 0.011696824803948402,
      "grad_norm": 0.38539012259954625
    },
    {
      "step": 1270,
      "loss": 0.054654061794281006,
      "grad_norm": 1.145696460116746
    },
    {
      "step": 1275,
      "loss": 0.014197949320077896,
      "grad_norm": 0.4454913176110585
    },
    {
      "step": 1280,
      "loss": 0.008937010541558266,
      "grad_norm": 0.2785004591638636
    },
    {
      "step": 1285,
      "loss": 0.02968946099281311,
      "grad_norm": 0.8340003173877468
    },
    {
      "step": 1290,
      "loss": 0.017620444297790527,
      "grad_norm": 0.46903757272381486
    },
    {
      "step": 1295,
      "loss": 0.0058544403873384,
      "grad_norm": 0.2901008155892259
    },
    {
      "step": 1300,
      "loss": 0.03148505091667175,
      "grad_norm": 1.329495143252033
    },
    {
      "step": 1305,
      "loss": 0.030923863872885704,
      "grad_norm": 1.2490838779486249
    },
    {
      "step": 1310,
      "loss": 0.0044219763949513435,
      "grad_norm": 0.15860172900753028
    },
    {
      "step": 1315,
      "loss": 0.084978848695755,
      "grad_norm": 1.503597267415622
    },
    {
      "step": 1320,
      "loss": 0.009293945506215096,
      "grad_norm": 0.28006349029258737
    },
    {
      "step": 1325,
      "loss": 0.026844477280974388,
      "grad_norm": 0.9801707932799426
    },
    {
      "step": 1330,
      "loss": 0.018118133768439293,
      "grad_norm": 0.6040677979704451
    },
    {
      "step": 1335,
      "loss": 0.014053916558623314,
      "grad_norm": 0.651981824933312
    },
    {
      "step": 1340,
      "loss": 0.07190656661987305,
      "grad_norm": 1.999520257997676
    },
    {
      "step": 1345,
      "loss": 0.037229280918836594,
      "grad_norm": 1.2520950139664564
    },
    {
      "step": 1350,
      "loss": 0.05765054002404213,
      "grad_norm": 1.5302764441055432
    },
    {
      "step": 1355,
      "loss": 0.07965510338544846,
      "grad_norm": 1.68209473689883
    },
    {
      "step": 1360,
      "loss": 0.04094424471259117,
      "grad_norm": 1.2182182587377919
    },
    {
      "step": 1365,
      "loss": 0.06437794119119644,
      "grad_norm": 1.7383693591003488
    },
    {
      "step": 1370,
      "loss": 0.010583932511508465,
      "grad_norm": 0.6287803864602212
    },
    {
      "step": 1375,
      "loss": 0.005173305980861187,
      "grad_norm": 0.19187761793756483
    },
    {
      "step": 1380,
      "loss": 0.05872474983334541,
      "grad_norm": 1.507675564534901
    },
    {
      "step": 1385,
      "loss": 0.014695009216666222,
      "grad_norm": 0.4653230235998036
    },
    {
      "step": 1390,
      "loss": 0.014594264328479767,
      "grad_norm": 0.4906123934795597
    },
    {
      "step": 1395,
      "loss": 0.04890528321266174,
      "grad_norm": 1.2602970672577787
    },
    {
      "step": 1400,
      "loss": 0.030582545325160027,
      "grad_norm": 0.8606090616679649
    },
    {
      "step": 1405,
      "loss": 0.017908403649926186,
      "grad_norm": 0.45901499553208464
    },
    {
      "step": 1410,
      "loss": 0.033228784799575806,
      "grad_norm": 1.2404386814449078
    },
    {
      "step": 1415,
      "loss": 0.028694139793515205,
      "grad_norm": 1.2260502216989821
    },
    {
      "step": 1420,
      "loss": 0.012180403806269169,
      "grad_norm": 0.46296679418631553
    },
    {
      "step": 1425,
      "loss": 0.012451179325580597,
      "grad_norm": 0.6893476279978158
    },
    {
      "step": 1430,
      "loss": 0.026522839441895485,
      "grad_norm": 0.8538148552500857
    },
    {
      "step": 1435,
      "loss": 0.12452047318220139,
      "grad_norm": 1.6730606461218767
    },
    {
      "step": 1440,
      "loss": 0.07917597144842148,
      "grad_norm": 1.4971034641337826
    },
    {
      "step": 1445,
      "loss": 0.0592130646109581,
      "grad_norm": 1.4978783422049753
    },
    {
      "step": 1450,
      "loss": 0.006105456035584211,
      "grad_norm": 0.25896795265314776
    },
    {
      "step": 1455,
      "loss": 0.09410794824361801,
      "grad_norm": 1.4384826347919457
    },
    {
      "step": 1460,
      "loss": 0.00850844569504261,
      "grad_norm": 0.43030295826358944
    },
    {
      "step": 1465,
      "loss": 0.04158348590135574,
      "grad_norm": 1.121599833719094
    },
    {
      "step": 1470,
      "loss": 0.011370931752026081,
      "grad_norm": 0.6010149698734514
    },
    {
      "step": 1475,
      "loss": 0.004225360695272684,
      "grad_norm": 0.21264225875740378
    },
    {
      "step": 1480,
      "loss": 0.013861517421901226,
      "grad_norm": 0.7608527174342105
    },
    {
      "step": 1485,
      "loss": 0.049021799117326736,
      "grad_norm": 1.208335499932762
    },
    {
      "step": 1490,
      "loss": 0.01029210165143013,
      "grad_norm": 0.5321477439142097
    },
    {
      "step": 1495,
      "loss": 0.023231854662299156,
      "grad_norm": 0.7855174525644025
    },
    {
      "step": 1500,
      "loss": 0.014094092883169651,
      "grad_norm": 0.5328891939412935
    },
    {
      "step": 1505,
      "loss": 0.003829043824225664,
      "grad_norm": 0.14422114179436446
    },
    {
      "step": 1510,
      "loss": 0.030549660325050354,
      "grad_norm": 1.1145041673166145
    },
    {
      "step": 1515,
      "loss": 0.01860133931040764,
      "grad_norm": 0.9668956001677729
    },
    {
      "step": 1520,
      "loss": 0.07197917252779007,
      "grad_norm": 2.082085436230482
    },
    {
      "step": 1525,
      "loss": 0.013639816083014011,
      "grad_norm": 0.777345353386141
    },
    {
      "step": 1530,
      "loss": 0.00651311781257391,
      "grad_norm": 0.27048203292457773
    },
    {
      "step": 1535,
      "loss": 0.02293136902153492,
      "grad_norm": 0.8746278373593308
    },
    {
      "step": 1540,
      "loss": 0.01067979633808136,
      "grad_norm": 0.4084804324650909
    },
    {
      "step": 1545,
      "loss": 0.045706454664468765,
      "grad_norm": 1.1426841759493305
    },
    {
      "step": 1550,
      "loss": 0.004422767087817192,
      "grad_norm": 0.17271887523109122
    },
    {
      "step": 1555,
      "loss": 0.032065149396657944,
      "grad_norm": 0.8485438821076806
    },
    {
      "step": 1560,
      "loss": 0.0518839955329895,
      "grad_norm": 1.2902160948394568
    },
    {
      "step": 1565,
      "loss": 0.1408359408378601,
      "grad_norm": 2.226503976477576
    },
    {
      "step": 1570,
      "loss": 0.01794060692191124,
      "grad_norm": 0.5526677006231918
    },
    {
      "step": 1575,
      "loss": 0.03836901858448982,
      "grad_norm": 1.054261305826593
    },
    {
      "step": 1580,
      "loss": 0.0021715424954891205,
      "grad_norm": 0.06511306775105394
    },
    {
      "step": 1585,
      "loss": 0.006132413633167744,
      "grad_norm": 0.20656374810293593
    },
    {
      "step": 1590,
      "loss": 0.052176110446453094,
      "grad_norm": 1.3571754114449455
    },
    {
      "step": 1595,
      "loss": 0.09522444754838943,
      "grad_norm": 1.4971120471328838
    },
    {
      "step": 1600,
      "loss": 0.07675982266664505,
      "grad_norm": 1.3664687499691996
    },
    {
      "step": 1605,
      "loss": 0.010453549213707447,
      "grad_norm": 0.37666978486919445
    },
    {
      "step": 1610,
      "loss": 0.04020459204912186,
      "grad_norm": 1.131578536163369
    },
    {
      "step": 1615,
      "loss": 0.05957099795341492,
      "grad_norm": 1.4227586709137898
    },
    {
      "step": 1620,
      "loss": 0.00895996019244194,
      "grad_norm": 0.28809794762534463
    },
    {
      "step": 1625,
      "loss": 0.01748974248766899,
      "grad_norm": 0.6013929389109536
    },
    {
      "step": 1630,
      "loss": 0.0636730045080185,
      "grad_norm": 1.2908796758387104
    },
    {
      "step": 1635,
      "loss": 0.005682955961674452,
      "grad_norm": 0.22407641379152976
    },
    {
      "step": 1640,
      "loss": 0.06155332177877426,
      "grad_norm": 1.2870318646787167
    },
    {
      "step": 1645,
      "loss": 0.048390328884124756,
      "grad_norm": 0.9773894685829506
    },
    {
      "step": 1650,
      "loss": 0.02725672349333763,
      "grad_norm": 1.070314579819805
    },
    {
      "step": 1655,
      "loss": 0.06331444531679153,
      "grad_norm": 1.1103104958498298
    },
    {
      "step": 1660,
      "loss": 0.12521770596504211,
      "grad_norm": 1.6269310875628544
    },
    {
      "step": 1665,
      "loss": 0.08146242797374725,
      "grad_norm": 0.9444641922876528
    },
    {
      "step": 1670,
      "loss": 0.0067797331139445305,
      "grad_norm": 0.2929403144655294
    },
    {
      "step": 1675,
      "loss": 0.01325957477092743,
      "grad_norm": 0.4762438631381204
    },
    {
      "step": 1680,
      "loss": 0.11087502539157867,
      "grad_norm": 1.4657507413169022
    },
    {
      "step": 1685,
      "loss": 0.06021903082728386,
      "grad_norm": 1.3044418004302918
    },
    {
      "step": 1690,
      "loss": 0.059034425765275955,
      "grad_norm": 1.6785876459332716
    },
    {
      "step": 1695,
      "loss": 0.020823786035180092,
      "grad_norm": 0.9089424920988383
    },
    {
      "step": 1700,
      "loss": 0.006027098745107651,
      "grad_norm": 0.34294961775121524
    },
    {
      "step": 1705,
      "loss": 0.002811580430716276,
      "grad_norm": 0.1788532580381153
    },
    {
      "step": 1710,
      "loss": 0.05635502189397812,
      "grad_norm": 0.9242879139372658
    },
    {
      "step": 1715,
      "loss": 0.019246315583586693,
      "grad_norm": 0.5903133064660152
    },
    {
      "step": 1720,
      "loss": 0.05633274465799332,
      "grad_norm": 1.5042845962699591
    },
    {
      "step": 1725,
      "loss": 0.0016773288371041417,
      "grad_norm": 0.08580911884789476
    },
    {
      "step": 1730,
      "loss": 0.004268051125109196,
      "grad_norm": 0.214136421690664
    },
    {
      "step": 1735,
      "loss": 0.11559174209833145,
      "grad_norm": 1.029427788527724
    },
    {
      "step": 1740,
      "loss": 0.046669550240039825,
      "grad_norm": 1.5501555562114908
    },
    {
      "step": 1745,
      "loss": 0.013170764781534672,
      "grad_norm": 0.49103732234297626
    },
    {
      "step": 1750,
      "loss": 0.11421655118465424,
      "grad_norm": 1.4251421563363844
    },
    {
      "step": 1755,
      "loss": 0.05842851102352142,
      "grad_norm": 1.2643952520692512
    },
    {
      "step": 1760,
      "loss": 0.037434257566928864,
      "grad_norm": 1.0821984742338562
    },
    {
      "step": 1765,
      "loss": 0.026096101850271225,
      "grad_norm": 0.9476886534230289
    },
    {
      "step": 1770,
      "loss": 0.003998848609626293,
      "grad_norm": 0.24047132977789534
    },
    {
      "step": 1775,
      "loss": 0.0038216665852814913,
      "grad_norm": 0.17501046278069493
    },
    {
      "step": 1780,
      "loss": 0.002335228957235813,
      "grad_norm": 0.10008710342840405
    },
    {
      "step": 1785,
      "loss": 0.05766913294792175,
      "grad_norm": 1.8089074435885273
    },
    {
      "step": 1790,
      "loss": 0.004683814477175474,
      "grad_norm": 0.3356412286241755
    },
    {
      "step": 1795,
      "loss": 0.014671334065496922,
      "grad_norm": 0.6300448951969831
    },
    {
      "step": 1800,
      "loss": 0.06737232208251953,
      "grad_norm": 1.734800919304507
    },
    {
      "step": 1805,
      "loss": 0.016595959663391113,
      "grad_norm": 0.5753626175844685
    },
    {
      "step": 1810,
      "loss": 0.016948817297816277,
      "grad_norm": 0.5903886707011256
    },
    {
      "step": 1815,
      "loss": 0.010639936663210392,
      "grad_norm": 0.4079917310793718
    },
    {
      "step": 1820,
      "loss": 0.04284755885601044,
      "grad_norm": 1.1998245804899454
    },
    {
      "step": 1825,
      "loss": 0.052635565400123596,
      "grad_norm": 1.33405630107629
    },
    {
      "step": 1830,
      "loss": 0.01502910628914833,
      "grad_norm": 0.800923902873184
    },
    {
      "step": 1835,
      "loss": 0.028261948376893997,
      "grad_norm": 0.9299309824913828
    },
    {
      "step": 1840,
      "loss": 0.0258147194981575,
      "grad_norm": 0.8492454280662703
    },
    {
      "step": 1845,
      "loss": 0.02249956503510475,
      "grad_norm": 0.7692229833132231
    },
    {
      "step": 1850,
      "loss": 0.1677580177783966,
      "grad_norm": 2.218150805735547
    },
    {
      "step": 1855,
      "loss": 0.043804772198200226,
      "grad_norm": 1.3832329344659788
    },
    {
      "step": 1860,
      "loss": 0.06988559663295746,
      "grad_norm": 1.9654098087252183
    },
    {
      "step": 1865,
      "loss": 0.0038882119115442038,
      "grad_norm": 0.1718443369564626
    },
    {
      "step": 1870,
      "loss": 0.002279700478538871,
      "grad_norm": 0.06671142115647118
    },
    {
      "step": 1875,
      "loss": 0.07019300758838654,
      "grad_norm": 1.81582711808124
    },
    {
      "step": 1880,
      "loss": 0.09419095516204834,
      "grad_norm": 1.420647479686948
    },
    {
      "step": 1885,
      "loss": 0.009854561649262905,
      "grad_norm": 0.345646259438619
    },
    {
      "step": 1890,
      "loss": 0.09333783388137817,
      "grad_norm": 1.597695172339479
    },
    {
      "step": 1895,
      "loss": 0.005665440112352371,
      "grad_norm": 0.22690664980245548
    },
    {
      "step": 1900,
      "loss": 0.057295192033052444,
      "grad_norm": 1.042207537101607
    },
    {
      "step": 1905,
      "loss": 0.045023880898952484,
      "grad_norm": 1.2316046883517136
    },
    {
      "step": 1910,
      "loss": 0.009745080955326557,
      "grad_norm": 0.515301049346421
    },
    {
      "step": 1915,
      "loss": 0.004785803612321615,
      "grad_norm": 0.1953739266329311
    },
    {
      "step": 1920,
      "loss": 0.017727943137288094,
      "grad_norm": 0.678869386764567
    },
    {
      "step": 1925,
      "loss": 0.03736624866724014,
      "grad_norm": 1.4122184461004827
    },
    {
      "step": 1930,
      "loss": 0.005912058521062136,
      "grad_norm": 0.18042086191823936
    },
    {
      "step": 1935,
      "loss": 0.038040220737457275,
      "grad_norm": 1.0317070113023168
    },
    {
      "step": 1940,
      "loss": 0.021859679371118546,
      "grad_norm": 0.7101541595781132
    },
    {
      "step": 1945,
      "loss": 0.024768931791186333,
      "grad_norm": 1.0434004999487163
    },
    {
      "step": 1950,
      "loss": 0.004911178257316351,
      "grad_norm": 0.24186387399755913
    },
    {
      "step": 1955,
      "loss": 0.008760904893279076,
      "grad_norm": 0.37899225967226513
    },
    {
      "step": 1960,
      "loss": 0.004903290420770645,
      "grad_norm": 0.22882070178359892
    },
    {
      "step": 1965,
      "loss": 0.010893075726926327,
      "grad_norm": 0.4403860889198804
    },
    {
      "step": 1970,
      "loss": 0.0009715580381453037,
      "grad_norm": 0.03733180624178177
    },
    {
      "step": 1975,
      "loss": 0.0010604870039969683,
      "grad_norm": 0.029944986280113998
    },
    {
      "step": 1980,
      "loss": 0.007832502014935017,
      "grad_norm": 0.3924534387415307
    },
    {
      "step": 1985,
      "loss": 0.02940937504172325,
      "grad_norm": 0.8541677388211385
    },
    {
      "step": 1990,
      "loss": 0.015435327775776386,
      "grad_norm": 0.46201587182525966
    },
    {
      "step": 1995,
      "loss": 0.05398877337574959,
      "grad_norm": 1.3163583847874671
    }
  ],
  "final_accuracy": 0.9904,
  "total_steps": 2000,
  "checkpoint_steps": [
    100,
    1000,
    2000
  ],
  "elapsed_time": 37.032334089279175
}