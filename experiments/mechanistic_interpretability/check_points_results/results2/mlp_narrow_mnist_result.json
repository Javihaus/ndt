{
  "experiment_name": "mlp_narrow_mnist",
  "measurements": [
    {
      "step": 0,
      "loss": 2.290389060974121,
      "grad_norm": 0.18035638080698652
    },
    {
      "step": 5,
      "loss": 2.282527446746826,
      "grad_norm": 0.21393772621239665
    },
    {
      "step": 10,
      "loss": 2.2423481941223145,
      "grad_norm": 0.28128916843909585
    },
    {
      "step": 15,
      "loss": 2.1963040828704834,
      "grad_norm": 0.4114774405200231
    },
    {
      "step": 20,
      "loss": 2.12166166305542,
      "grad_norm": 0.552152769373745
    },
    {
      "step": 25,
      "loss": 1.9971939325332642,
      "grad_norm": 0.8597712480352929
    },
    {
      "step": 30,
      "loss": 1.913945198059082,
      "grad_norm": 1.2729146217044438
    },
    {
      "step": 35,
      "loss": 1.6895673274993896,
      "grad_norm": 1.3994567408086298
    },
    {
      "step": 40,
      "loss": 1.5532054901123047,
      "grad_norm": 1.2241804979991882
    },
    {
      "step": 45,
      "loss": 1.3848475217819214,
      "grad_norm": 1.8158031742211704
    },
    {
      "step": 50,
      "loss": 1.4967257976531982,
      "grad_norm": 1.9511399177353197
    },
    {
      "step": 55,
      "loss": 1.206747055053711,
      "grad_norm": 2.3229277935732595
    },
    {
      "step": 60,
      "loss": 1.177074909210205,
      "grad_norm": 2.7046629510930553
    },
    {
      "step": 65,
      "loss": 0.9451709985733032,
      "grad_norm": 1.5831537521024732
    },
    {
      "step": 70,
      "loss": 1.005781888961792,
      "grad_norm": 3.8149180205100546
    },
    {
      "step": 75,
      "loss": 0.8216824531555176,
      "grad_norm": 2.3155638506507112
    },
    {
      "step": 80,
      "loss": 0.8727471232414246,
      "grad_norm": 2.6244097449529287
    },
    {
      "step": 85,
      "loss": 0.811814546585083,
      "grad_norm": 4.093607772476438
    },
    {
      "step": 90,
      "loss": 0.4693956673145294,
      "grad_norm": 2.043368633437123
    },
    {
      "step": 95,
      "loss": 0.7190843820571899,
      "grad_norm": 3.0414368990767278
    },
    {
      "step": 100,
      "loss": 0.7767764925956726,
      "grad_norm": 2.7469159322715626
    },
    {
      "step": 105,
      "loss": 0.6600235104560852,
      "grad_norm": 2.236747292467748
    },
    {
      "step": 110,
      "loss": 0.7320908308029175,
      "grad_norm": 2.0545085257519315
    },
    {
      "step": 115,
      "loss": 0.5818806290626526,
      "grad_norm": 4.210281948873101
    },
    {
      "step": 120,
      "loss": 0.7827619314193726,
      "grad_norm": 3.1267003352393132
    },
    {
      "step": 125,
      "loss": 0.5227538347244263,
      "grad_norm": 2.3288049953196155
    },
    {
      "step": 130,
      "loss": 0.6142101287841797,
      "grad_norm": 3.0672683815598987
    },
    {
      "step": 135,
      "loss": 0.5509839653968811,
      "grad_norm": 2.5628596556739596
    },
    {
      "step": 140,
      "loss": 0.44687992334365845,
      "grad_norm": 2.2955816314406294
    },
    {
      "step": 145,
      "loss": 0.44260406494140625,
      "grad_norm": 1.9938724775355936
    },
    {
      "step": 150,
      "loss": 0.6376875638961792,
      "grad_norm": 2.447752657454405
    },
    {
      "step": 155,
      "loss": 0.7456804513931274,
      "grad_norm": 2.8093576755915146
    },
    {
      "step": 160,
      "loss": 0.5017727613449097,
      "grad_norm": 3.1528969006229217
    },
    {
      "step": 165,
      "loss": 0.5465472936630249,
      "grad_norm": 3.3620559433838997
    },
    {
      "step": 170,
      "loss": 0.6122530102729797,
      "grad_norm": 3.647396061173295
    },
    {
      "step": 175,
      "loss": 0.372539758682251,
      "grad_norm": 2.121420755264689
    },
    {
      "step": 180,
      "loss": 0.5849167108535767,
      "grad_norm": 3.8270088893312098
    },
    {
      "step": 185,
      "loss": 0.5439666509628296,
      "grad_norm": 3.219149333341058
    },
    {
      "step": 190,
      "loss": 0.5184099078178406,
      "grad_norm": 2.6710390072858363
    },
    {
      "step": 195,
      "loss": 0.405484676361084,
      "grad_norm": 3.3396212623528867
    },
    {
      "step": 200,
      "loss": 0.5100359320640564,
      "grad_norm": 2.7550876401048923
    },
    {
      "step": 205,
      "loss": 0.4116431176662445,
      "grad_norm": 1.7766739172540513
    },
    {
      "step": 210,
      "loss": 0.5843092203140259,
      "grad_norm": 3.9463362325694513
    },
    {
      "step": 215,
      "loss": 0.3778625726699829,
      "grad_norm": 4.148331371680575
    },
    {
      "step": 220,
      "loss": 0.5458613038063049,
      "grad_norm": 2.962027626427445
    },
    {
      "step": 225,
      "loss": 0.32057520747184753,
      "grad_norm": 1.934723555924643
    },
    {
      "step": 230,
      "loss": 0.38472825288772583,
      "grad_norm": 3.536488667642789
    },
    {
      "step": 235,
      "loss": 0.47039276361465454,
      "grad_norm": 3.1846116866916416
    },
    {
      "step": 240,
      "loss": 0.6216497421264648,
      "grad_norm": 3.554612587908053
    },
    {
      "step": 245,
      "loss": 0.6217340230941772,
      "grad_norm": 3.5358448553807063
    },
    {
      "step": 250,
      "loss": 0.5196610689163208,
      "grad_norm": 3.5412684597079513
    },
    {
      "step": 255,
      "loss": 0.42123594880104065,
      "grad_norm": 3.847239281798424
    },
    {
      "step": 260,
      "loss": 0.3885483145713806,
      "grad_norm": 1.8926980154557367
    },
    {
      "step": 265,
      "loss": 0.37995970249176025,
      "grad_norm": 3.388864927134949
    },
    {
      "step": 270,
      "loss": 0.39294731616973877,
      "grad_norm": 2.130074303096894
    },
    {
      "step": 275,
      "loss": 0.33062106370925903,
      "grad_norm": 2.1007494290003126
    },
    {
      "step": 280,
      "loss": 0.5767114758491516,
      "grad_norm": 2.692709142562021
    },
    {
      "step": 285,
      "loss": 0.5854614973068237,
      "grad_norm": 2.7288050240675297
    },
    {
      "step": 290,
      "loss": 0.39246511459350586,
      "grad_norm": 1.865322215919884
    },
    {
      "step": 295,
      "loss": 0.3240790367126465,
      "grad_norm": 2.0779641537095115
    },
    {
      "step": 300,
      "loss": 0.2340930849313736,
      "grad_norm": 2.8864123744892938
    },
    {
      "step": 305,
      "loss": 0.4710500240325928,
      "grad_norm": 2.2300685777396074
    },
    {
      "step": 310,
      "loss": 0.31645339727401733,
      "grad_norm": 2.5497919645689824
    },
    {
      "step": 315,
      "loss": 0.49191170930862427,
      "grad_norm": 2.915257056129327
    },
    {
      "step": 320,
      "loss": 0.3993168771266937,
      "grad_norm": 2.6598352047765466
    },
    {
      "step": 325,
      "loss": 0.3857441544532776,
      "grad_norm": 3.0189687000060648
    },
    {
      "step": 330,
      "loss": 0.4614506959915161,
      "grad_norm": 2.696871656755908
    },
    {
      "step": 335,
      "loss": 0.23664668202400208,
      "grad_norm": 2.617494004395386
    },
    {
      "step": 340,
      "loss": 0.5309691429138184,
      "grad_norm": 3.252457602088025
    },
    {
      "step": 345,
      "loss": 0.2079583704471588,
      "grad_norm": 1.7578119894439477
    },
    {
      "step": 350,
      "loss": 0.19451063871383667,
      "grad_norm": 1.8139245659377716
    },
    {
      "step": 355,
      "loss": 0.4452071189880371,
      "grad_norm": 2.847344200745591
    },
    {
      "step": 360,
      "loss": 0.44296374917030334,
      "grad_norm": 2.205129546518481
    },
    {
      "step": 365,
      "loss": 0.4448711574077606,
      "grad_norm": 2.0347116900918523
    },
    {
      "step": 370,
      "loss": 0.31498944759368896,
      "grad_norm": 3.0292588745365694
    },
    {
      "step": 375,
      "loss": 0.3051098585128784,
      "grad_norm": 2.444038417146964
    },
    {
      "step": 380,
      "loss": 0.5693202018737793,
      "grad_norm": 3.9842848182354267
    },
    {
      "step": 385,
      "loss": 0.2271035611629486,
      "grad_norm": 1.5071140880514236
    },
    {
      "step": 390,
      "loss": 0.27544814348220825,
      "grad_norm": 2.621286447259967
    },
    {
      "step": 395,
      "loss": 0.2567537724971771,
      "grad_norm": 2.4155570005517446
    },
    {
      "step": 400,
      "loss": 0.3862682580947876,
      "grad_norm": 2.4592368959683824
    },
    {
      "step": 405,
      "loss": 0.31997567415237427,
      "grad_norm": 2.2496148242081295
    },
    {
      "step": 410,
      "loss": 0.35840165615081787,
      "grad_norm": 1.6429343388189663
    },
    {
      "step": 415,
      "loss": 0.2904893159866333,
      "grad_norm": 1.8906039387947609
    },
    {
      "step": 420,
      "loss": 0.44226258993148804,
      "grad_norm": 2.167032733950164
    },
    {
      "step": 425,
      "loss": 0.3154028058052063,
      "grad_norm": 1.7001327116799891
    },
    {
      "step": 430,
      "loss": 0.26208609342575073,
      "grad_norm": 3.051942838730751
    },
    {
      "step": 435,
      "loss": 0.2130163609981537,
      "grad_norm": 1.8326748236471921
    },
    {
      "step": 440,
      "loss": 0.3077356815338135,
      "grad_norm": 1.7176440781198745
    },
    {
      "step": 445,
      "loss": 0.5168492794036865,
      "grad_norm": 3.506411447886711
    },
    {
      "step": 450,
      "loss": 0.22674450278282166,
      "grad_norm": 2.1463984912912077
    },
    {
      "step": 455,
      "loss": 0.5190837383270264,
      "grad_norm": 2.8897861520164234
    },
    {
      "step": 460,
      "loss": 0.2802910804748535,
      "grad_norm": 1.9763191227623933
    },
    {
      "step": 465,
      "loss": 0.36030587553977966,
      "grad_norm": 3.712821907230109
    },
    {
      "step": 470,
      "loss": 0.5175761580467224,
      "grad_norm": 3.5114280581693644
    },
    {
      "step": 475,
      "loss": 0.2635886073112488,
      "grad_norm": 1.6051732718960594
    },
    {
      "step": 480,
      "loss": 0.2926977276802063,
      "grad_norm": 1.8302226091546174
    },
    {
      "step": 485,
      "loss": 0.4255959391593933,
      "grad_norm": 1.8834202448702437
    },
    {
      "step": 490,
      "loss": 0.24297848343849182,
      "grad_norm": 1.3918989527455463
    },
    {
      "step": 495,
      "loss": 0.4855186939239502,
      "grad_norm": 3.276476148246515
    },
    {
      "step": 500,
      "loss": 0.3228050470352173,
      "grad_norm": 3.264055043209753
    },
    {
      "step": 505,
      "loss": 0.45439326763153076,
      "grad_norm": 3.336659098912071
    },
    {
      "step": 510,
      "loss": 0.5745495557785034,
      "grad_norm": 3.7232787021475136
    },
    {
      "step": 515,
      "loss": 0.4702003598213196,
      "grad_norm": 3.9935116273774485
    },
    {
      "step": 520,
      "loss": 0.2670522928237915,
      "grad_norm": 2.4006681270085912
    },
    {
      "step": 525,
      "loss": 0.33755189180374146,
      "grad_norm": 3.3184782263607278
    },
    {
      "step": 530,
      "loss": 0.41769105195999146,
      "grad_norm": 2.2263046010660594
    },
    {
      "step": 535,
      "loss": 0.22994990646839142,
      "grad_norm": 1.5693833517691607
    },
    {
      "step": 540,
      "loss": 0.523549497127533,
      "grad_norm": 2.7450798585751923
    },
    {
      "step": 545,
      "loss": 0.411175400018692,
      "grad_norm": 2.8155117218736616
    },
    {
      "step": 550,
      "loss": 0.26745322346687317,
      "grad_norm": 1.91620382142743
    },
    {
      "step": 555,
      "loss": 0.4146289527416229,
      "grad_norm": 3.4369605026126693
    },
    {
      "step": 560,
      "loss": 0.328407883644104,
      "grad_norm": 2.0195161143341567
    },
    {
      "step": 565,
      "loss": 0.37439873814582825,
      "grad_norm": 2.678110352271656
    },
    {
      "step": 570,
      "loss": 0.33003121614456177,
      "grad_norm": 2.2140855822205836
    },
    {
      "step": 575,
      "loss": 0.18202611804008484,
      "grad_norm": 1.920915454986484
    },
    {
      "step": 580,
      "loss": 0.3109150528907776,
      "grad_norm": 1.577687046335014
    },
    {
      "step": 585,
      "loss": 0.18872538208961487,
      "grad_norm": 2.190877579762718
    },
    {
      "step": 590,
      "loss": 0.5509949326515198,
      "grad_norm": 2.024778273290206
    },
    {
      "step": 595,
      "loss": 0.19113995134830475,
      "grad_norm": 1.817909388058848
    },
    {
      "step": 600,
      "loss": 0.27020183205604553,
      "grad_norm": 1.495515152519836
    },
    {
      "step": 605,
      "loss": 0.29429173469543457,
      "grad_norm": 1.5140098133218702
    },
    {
      "step": 610,
      "loss": 0.3639007806777954,
      "grad_norm": 2.081322692127611
    },
    {
      "step": 615,
      "loss": 0.3092205226421356,
      "grad_norm": 2.249788279582665
    },
    {
      "step": 620,
      "loss": 0.5988622903823853,
      "grad_norm": 3.0013384935132845
    },
    {
      "step": 625,
      "loss": 0.21197731792926788,
      "grad_norm": 2.0240174530453645
    },
    {
      "step": 630,
      "loss": 0.42260438203811646,
      "grad_norm": 3.12623908489549
    },
    {
      "step": 635,
      "loss": 0.41331788897514343,
      "grad_norm": 2.304931270405805
    },
    {
      "step": 640,
      "loss": 0.2904845178127289,
      "grad_norm": 1.7368112536185984
    },
    {
      "step": 645,
      "loss": 0.42340654134750366,
      "grad_norm": 2.7850560235271287
    },
    {
      "step": 650,
      "loss": 0.29923027753829956,
      "grad_norm": 2.8114803485503153
    },
    {
      "step": 655,
      "loss": 0.4054184556007385,
      "grad_norm": 2.497978284635035
    },
    {
      "step": 660,
      "loss": 0.21345317363739014,
      "grad_norm": 1.9901131621286727
    },
    {
      "step": 665,
      "loss": 0.2260979861021042,
      "grad_norm": 1.5698377935932404
    },
    {
      "step": 670,
      "loss": 0.4069661498069763,
      "grad_norm": 2.9009486713927473
    },
    {
      "step": 675,
      "loss": 0.46488258242607117,
      "grad_norm": 3.909607664536035
    },
    {
      "step": 680,
      "loss": 0.41611218452453613,
      "grad_norm": 2.6132549783445085
    },
    {
      "step": 685,
      "loss": 0.5376018285751343,
      "grad_norm": 3.2189686665892396
    },
    {
      "step": 690,
      "loss": 0.2835156321525574,
      "grad_norm": 2.1814038251772536
    },
    {
      "step": 695,
      "loss": 0.2659383714199066,
      "grad_norm": 1.9286069946463142
    },
    {
      "step": 700,
      "loss": 0.38541877269744873,
      "grad_norm": 2.799904313485998
    },
    {
      "step": 705,
      "loss": 0.31203195452690125,
      "grad_norm": 2.2371720912897897
    },
    {
      "step": 710,
      "loss": 0.20455443859100342,
      "grad_norm": 1.8624407380589778
    },
    {
      "step": 715,
      "loss": 0.4343715012073517,
      "grad_norm": 1.866796243285436
    },
    {
      "step": 720,
      "loss": 0.271449476480484,
      "grad_norm": 1.5614246009396482
    },
    {
      "step": 725,
      "loss": 0.26568377017974854,
      "grad_norm": 2.055958682843969
    },
    {
      "step": 730,
      "loss": 0.2989843487739563,
      "grad_norm": 2.8789827493027866
    },
    {
      "step": 735,
      "loss": 0.3439224064350128,
      "grad_norm": 1.5061964795579732
    },
    {
      "step": 740,
      "loss": 0.11196921765804291,
      "grad_norm": 1.1235666356483145
    },
    {
      "step": 745,
      "loss": 0.17627540230751038,
      "grad_norm": 1.3224877236137909
    },
    {
      "step": 750,
      "loss": 0.25564929842948914,
      "grad_norm": 1.6813357091021819
    },
    {
      "step": 755,
      "loss": 0.18438579142093658,
      "grad_norm": 1.0582946504519772
    },
    {
      "step": 760,
      "loss": 0.3860798478126526,
      "grad_norm": 1.8287806344514164
    },
    {
      "step": 765,
      "loss": 0.4438071846961975,
      "grad_norm": 3.2820519022866543
    },
    {
      "step": 770,
      "loss": 0.3539341986179352,
      "grad_norm": 2.393254990286707
    },
    {
      "step": 775,
      "loss": 0.2282681167125702,
      "grad_norm": 1.740251997825789
    },
    {
      "step": 780,
      "loss": 0.29169517755508423,
      "grad_norm": 1.9867954108656427
    },
    {
      "step": 785,
      "loss": 0.20222249627113342,
      "grad_norm": 1.8065317455903933
    },
    {
      "step": 790,
      "loss": 0.1671079397201538,
      "grad_norm": 2.1974518218086696
    },
    {
      "step": 795,
      "loss": 0.28318333625793457,
      "grad_norm": 1.7425577353519326
    },
    {
      "step": 800,
      "loss": 0.30092722177505493,
      "grad_norm": 2.5864652458303654
    },
    {
      "step": 805,
      "loss": 0.38108545541763306,
      "grad_norm": 2.721050203769541
    },
    {
      "step": 810,
      "loss": 0.5328771471977234,
      "grad_norm": 3.409908041478384
    },
    {
      "step": 815,
      "loss": 0.2664126455783844,
      "grad_norm": 1.5123774730679398
    },
    {
      "step": 820,
      "loss": 0.2988739013671875,
      "grad_norm": 1.804618459421761
    },
    {
      "step": 825,
      "loss": 0.23611405491828918,
      "grad_norm": 1.950355568032097
    },
    {
      "step": 830,
      "loss": 0.36966630816459656,
      "grad_norm": 2.5962732433700513
    },
    {
      "step": 835,
      "loss": 0.4512367248535156,
      "grad_norm": 3.767842488185161
    },
    {
      "step": 840,
      "loss": 0.2741658091545105,
      "grad_norm": 1.5503468154071267
    },
    {
      "step": 845,
      "loss": 0.3918188810348511,
      "grad_norm": 2.584370953102739
    },
    {
      "step": 850,
      "loss": 0.2277924120426178,
      "grad_norm": 2.16060135666716
    },
    {
      "step": 855,
      "loss": 0.21519547700881958,
      "grad_norm": 1.9718142624392487
    },
    {
      "step": 860,
      "loss": 0.2407582402229309,
      "grad_norm": 2.44057831926434
    },
    {
      "step": 865,
      "loss": 0.2710299789905548,
      "grad_norm": 1.9338641887927328
    },
    {
      "step": 870,
      "loss": 0.14669877290725708,
      "grad_norm": 1.2778804354742825
    },
    {
      "step": 875,
      "loss": 0.2043127715587616,
      "grad_norm": 1.6770140043317554
    },
    {
      "step": 880,
      "loss": 0.24119338393211365,
      "grad_norm": 1.7622873592595667
    },
    {
      "step": 885,
      "loss": 0.30037248134613037,
      "grad_norm": 1.8520092898201224
    },
    {
      "step": 890,
      "loss": 0.2742684483528137,
      "grad_norm": 1.8743159689633089
    },
    {
      "step": 895,
      "loss": 0.3020891845226288,
      "grad_norm": 1.464176136681246
    },
    {
      "step": 900,
      "loss": 0.12772899866104126,
      "grad_norm": 1.0895015046494387
    },
    {
      "step": 905,
      "loss": 0.2991113066673279,
      "grad_norm": 1.9870033328709
    },
    {
      "step": 910,
      "loss": 0.25503405928611755,
      "grad_norm": 1.5355463461851198
    },
    {
      "step": 915,
      "loss": 0.43837371468544006,
      "grad_norm": 2.818711117936109
    },
    {
      "step": 920,
      "loss": 0.25683099031448364,
      "grad_norm": 2.536579896192708
    },
    {
      "step": 925,
      "loss": 0.13842083513736725,
      "grad_norm": 1.0239683134043676
    },
    {
      "step": 930,
      "loss": 0.5541282296180725,
      "grad_norm": 3.2382193416746694
    },
    {
      "step": 935,
      "loss": 0.2798938751220703,
      "grad_norm": 2.1973902168766015
    },
    {
      "step": 940,
      "loss": 0.21225057542324066,
      "grad_norm": 1.542211950727845
    },
    {
      "step": 945,
      "loss": 0.22233512997627258,
      "grad_norm": 1.9543010136937673
    },
    {
      "step": 950,
      "loss": 0.2583739459514618,
      "grad_norm": 2.2383470393244322
    },
    {
      "step": 955,
      "loss": 0.35876521468162537,
      "grad_norm": 1.599526800265104
    },
    {
      "step": 960,
      "loss": 0.4895893931388855,
      "grad_norm": 2.0793492535159075
    },
    {
      "step": 965,
      "loss": 0.5260034799575806,
      "grad_norm": 3.0353014942244774
    },
    {
      "step": 970,
      "loss": 0.3425781726837158,
      "grad_norm": 1.556338363181329
    },
    {
      "step": 975,
      "loss": 0.3264983296394348,
      "grad_norm": 2.748052209257378
    },
    {
      "step": 980,
      "loss": 0.4616994559764862,
      "grad_norm": 2.5603343305632995
    },
    {
      "step": 985,
      "loss": 0.3425107002258301,
      "grad_norm": 1.5160284815877751
    },
    {
      "step": 990,
      "loss": 0.33366841077804565,
      "grad_norm": 2.966326516161023
    },
    {
      "step": 995,
      "loss": 0.4493435025215149,
      "grad_norm": 2.744695021754508
    },
    {
      "step": 1000,
      "loss": 0.342097669839859,
      "grad_norm": 2.2382310572341573
    },
    {
      "step": 1005,
      "loss": 0.31839749217033386,
      "grad_norm": 1.9649264821603423
    },
    {
      "step": 1010,
      "loss": 0.25569236278533936,
      "grad_norm": 1.9762955189618243
    },
    {
      "step": 1015,
      "loss": 0.1820417046546936,
      "grad_norm": 2.14505157779205
    },
    {
      "step": 1020,
      "loss": 0.1959955096244812,
      "grad_norm": 2.420799816499515
    },
    {
      "step": 1025,
      "loss": 0.22547411918640137,
      "grad_norm": 1.2724180960180105
    },
    {
      "step": 1030,
      "loss": 0.3030948042869568,
      "grad_norm": 2.0556356020880435
    },
    {
      "step": 1035,
      "loss": 0.2494911551475525,
      "grad_norm": 1.4877445068652788
    },
    {
      "step": 1040,
      "loss": 0.40228432416915894,
      "grad_norm": 2.7316810966389244
    },
    {
      "step": 1045,
      "loss": 0.1738293319940567,
      "grad_norm": 1.3929849605145108
    },
    {
      "step": 1050,
      "loss": 0.21441873908042908,
      "grad_norm": 2.11498743558229
    },
    {
      "step": 1055,
      "loss": 0.37370556592941284,
      "grad_norm": 2.114747565183743
    },
    {
      "step": 1060,
      "loss": 0.21285486221313477,
      "grad_norm": 2.014464961684145
    },
    {
      "step": 1065,
      "loss": 0.3404759168624878,
      "grad_norm": 2.013567850501714
    },
    {
      "step": 1070,
      "loss": 0.17008012533187866,
      "grad_norm": 1.3792527623696371
    },
    {
      "step": 1075,
      "loss": 0.5844253301620483,
      "grad_norm": 2.6838971339857522
    },
    {
      "step": 1080,
      "loss": 0.2965054512023926,
      "grad_norm": 1.5911075949526818
    },
    {
      "step": 1085,
      "loss": 0.1920091211795807,
      "grad_norm": 1.9285206380260784
    },
    {
      "step": 1090,
      "loss": 0.17982155084609985,
      "grad_norm": 1.6332265509847472
    },
    {
      "step": 1095,
      "loss": 0.24514716863632202,
      "grad_norm": 1.6960072864610054
    },
    {
      "step": 1100,
      "loss": 0.11595350503921509,
      "grad_norm": 1.4376904723003563
    },
    {
      "step": 1105,
      "loss": 0.35210204124450684,
      "grad_norm": 2.5312069492578626
    },
    {
      "step": 1110,
      "loss": 0.20064878463745117,
      "grad_norm": 2.039548380125102
    },
    {
      "step": 1115,
      "loss": 0.18026423454284668,
      "grad_norm": 2.108072331158665
    },
    {
      "step": 1120,
      "loss": 0.20822551846504211,
      "grad_norm": 1.1678599392092954
    },
    {
      "step": 1125,
      "loss": 0.21097834408283234,
      "grad_norm": 2.1398546710347217
    },
    {
      "step": 1130,
      "loss": 0.05146905779838562,
      "grad_norm": 0.591955528209176
    },
    {
      "step": 1135,
      "loss": 0.15108846127986908,
      "grad_norm": 1.6838277230685808
    },
    {
      "step": 1140,
      "loss": 0.19297239184379578,
      "grad_norm": 1.9260123214650775
    },
    {
      "step": 1145,
      "loss": 0.3331852853298187,
      "grad_norm": 1.7673962234514782
    },
    {
      "step": 1150,
      "loss": 0.21601060032844543,
      "grad_norm": 2.363351463780217
    },
    {
      "step": 1155,
      "loss": 0.2822341322898865,
      "grad_norm": 2.25656414718465
    },
    {
      "step": 1160,
      "loss": 0.11373679339885712,
      "grad_norm": 1.307376525990881
    },
    {
      "step": 1165,
      "loss": 0.3517580032348633,
      "grad_norm": 2.328727810745254
    },
    {
      "step": 1170,
      "loss": 0.20251736044883728,
      "grad_norm": 2.148764345268051
    },
    {
      "step": 1175,
      "loss": 0.32289737462997437,
      "grad_norm": 2.4733172566182393
    },
    {
      "step": 1180,
      "loss": 0.20919151604175568,
      "grad_norm": 1.9997356798681436
    },
    {
      "step": 1185,
      "loss": 0.4549635648727417,
      "grad_norm": 1.900050274949811
    },
    {
      "step": 1190,
      "loss": 0.16992291808128357,
      "grad_norm": 1.9813822267983154
    },
    {
      "step": 1195,
      "loss": 0.41335105895996094,
      "grad_norm": 2.7694632389380027
    },
    {
      "step": 1200,
      "loss": 0.14578299224376678,
      "grad_norm": 1.9256710696081014
    },
    {
      "step": 1205,
      "loss": 0.1306113302707672,
      "grad_norm": 1.4628115754157986
    },
    {
      "step": 1210,
      "loss": 0.20221467316150665,
      "grad_norm": 2.5274548406627737
    },
    {
      "step": 1215,
      "loss": 0.28917133808135986,
      "grad_norm": 2.2573738371264063
    },
    {
      "step": 1220,
      "loss": 0.24404799938201904,
      "grad_norm": 2.6002200961718898
    },
    {
      "step": 1225,
      "loss": 0.13555684685707092,
      "grad_norm": 1.8267251381403316
    },
    {
      "step": 1230,
      "loss": 0.08279028534889221,
      "grad_norm": 1.1752204162230953
    },
    {
      "step": 1235,
      "loss": 0.08968658745288849,
      "grad_norm": 1.3236110135576216
    },
    {
      "step": 1240,
      "loss": 0.13441410660743713,
      "grad_norm": 0.8833643439144102
    },
    {
      "step": 1245,
      "loss": 0.14048685133457184,
      "grad_norm": 1.2236099622499375
    },
    {
      "step": 1250,
      "loss": 0.3625173568725586,
      "grad_norm": 2.7326935671088193
    },
    {
      "step": 1255,
      "loss": 0.12039592862129211,
      "grad_norm": 1.2687400372850322
    },
    {
      "step": 1260,
      "loss": 0.20617027580738068,
      "grad_norm": 1.7602449613684732
    },
    {
      "step": 1265,
      "loss": 0.2623395323753357,
      "grad_norm": 2.4118118094372725
    },
    {
      "step": 1270,
      "loss": 0.2321469932794571,
      "grad_norm": 1.4619825494753358
    },
    {
      "step": 1275,
      "loss": 0.1214025467634201,
      "grad_norm": 1.5436907115959464
    },
    {
      "step": 1280,
      "loss": 0.2563752233982086,
      "grad_norm": 1.542977133505944
    },
    {
      "step": 1285,
      "loss": 0.0949050784111023,
      "grad_norm": 1.0600212939053444
    },
    {
      "step": 1290,
      "loss": 0.36625000834465027,
      "grad_norm": 2.4302302821945476
    },
    {
      "step": 1295,
      "loss": 0.37006622552871704,
      "grad_norm": 1.8500236895502438
    },
    {
      "step": 1300,
      "loss": 0.28385448455810547,
      "grad_norm": 2.409586737096226
    },
    {
      "step": 1305,
      "loss": 0.2423911839723587,
      "grad_norm": 2.421921755325521
    },
    {
      "step": 1310,
      "loss": 0.10197461396455765,
      "grad_norm": 1.1774211277800828
    },
    {
      "step": 1315,
      "loss": 0.154829740524292,
      "grad_norm": 0.9587365185347305
    },
    {
      "step": 1320,
      "loss": 0.36683595180511475,
      "grad_norm": 2.4127258314368794
    },
    {
      "step": 1325,
      "loss": 0.11841221153736115,
      "grad_norm": 1.394490813151141
    },
    {
      "step": 1330,
      "loss": 0.25848379731178284,
      "grad_norm": 2.135362737241556
    },
    {
      "step": 1335,
      "loss": 0.31512147188186646,
      "grad_norm": 1.4372138104668088
    },
    {
      "step": 1340,
      "loss": 0.3824266791343689,
      "grad_norm": 2.293065212824121
    },
    {
      "step": 1345,
      "loss": 0.21008139848709106,
      "grad_norm": 1.6349417387158267
    },
    {
      "step": 1350,
      "loss": 0.3172457814216614,
      "grad_norm": 1.8558699909968606
    },
    {
      "step": 1355,
      "loss": 0.24559719860553741,
      "grad_norm": 1.8080094477716964
    },
    {
      "step": 1360,
      "loss": 0.32358402013778687,
      "grad_norm": 2.946407400650728
    },
    {
      "step": 1365,
      "loss": 0.1320098638534546,
      "grad_norm": 1.4681140605664105
    },
    {
      "step": 1370,
      "loss": 0.3152470588684082,
      "grad_norm": 1.715756911716858
    },
    {
      "step": 1375,
      "loss": 0.24489013850688934,
      "grad_norm": 1.56590418921033
    },
    {
      "step": 1380,
      "loss": 0.2161097675561905,
      "grad_norm": 2.004952212299877
    },
    {
      "step": 1385,
      "loss": 0.21251066029071808,
      "grad_norm": 2.0462682640051093
    },
    {
      "step": 1390,
      "loss": 0.1675850749015808,
      "grad_norm": 1.3284221664605276
    },
    {
      "step": 1395,
      "loss": 0.32741695642471313,
      "grad_norm": 1.667403739234444
    },
    {
      "step": 1400,
      "loss": 0.2810867428779602,
      "grad_norm": 2.169387781401904
    },
    {
      "step": 1405,
      "loss": 0.11793653666973114,
      "grad_norm": 0.9904979856369824
    },
    {
      "step": 1410,
      "loss": 0.2522937059402466,
      "grad_norm": 2.0327340275078143
    },
    {
      "step": 1415,
      "loss": 0.163772851228714,
      "grad_norm": 1.3908712089559156
    },
    {
      "step": 1420,
      "loss": 0.22908943891525269,
      "grad_norm": 1.6706386955284616
    },
    {
      "step": 1425,
      "loss": 0.21874311566352844,
      "grad_norm": 2.5805156614889686
    },
    {
      "step": 1430,
      "loss": 0.272221177816391,
      "grad_norm": 2.686274471199066
    },
    {
      "step": 1435,
      "loss": 0.42873886227607727,
      "grad_norm": 2.1549111048004956
    },
    {
      "step": 1440,
      "loss": 0.27119767665863037,
      "grad_norm": 2.316285457711168
    },
    {
      "step": 1445,
      "loss": 0.30132347345352173,
      "grad_norm": 1.9185072878591451
    },
    {
      "step": 1450,
      "loss": 0.15241852402687073,
      "grad_norm": 1.7342254452962638
    },
    {
      "step": 1455,
      "loss": 0.23650458455085754,
      "grad_norm": 1.5277945923480052
    },
    {
      "step": 1460,
      "loss": 0.2806224226951599,
      "grad_norm": 2.13270954543298
    },
    {
      "step": 1465,
      "loss": 0.18143361806869507,
      "grad_norm": 1.6230002682277531
    },
    {
      "step": 1470,
      "loss": 0.2694401144981384,
      "grad_norm": 2.2206559220825253
    },
    {
      "step": 1475,
      "loss": 0.4080314636230469,
      "grad_norm": 2.448967266631838
    },
    {
      "step": 1480,
      "loss": 0.17351530492305756,
      "grad_norm": 1.4410795706619048
    },
    {
      "step": 1485,
      "loss": 0.14289841055870056,
      "grad_norm": 2.110828756015493
    },
    {
      "step": 1490,
      "loss": 0.29080939292907715,
      "grad_norm": 2.729786592700036
    },
    {
      "step": 1495,
      "loss": 0.304161012172699,
      "grad_norm": 2.495531290769228
    },
    {
      "step": 1500,
      "loss": 0.18767008185386658,
      "grad_norm": 1.485821560956073
    },
    {
      "step": 1505,
      "loss": 0.17852288484573364,
      "grad_norm": 1.9667363778463842
    },
    {
      "step": 1510,
      "loss": 0.2982116937637329,
      "grad_norm": 1.9237996805337974
    },
    {
      "step": 1515,
      "loss": 0.18456998467445374,
      "grad_norm": 1.4344661090706738
    },
    {
      "step": 1520,
      "loss": 0.11860533058643341,
      "grad_norm": 1.3247939240160023
    },
    {
      "step": 1525,
      "loss": 0.12300250679254532,
      "grad_norm": 1.560716458491605
    },
    {
      "step": 1530,
      "loss": 0.17786714434623718,
      "grad_norm": 1.6090620417185748
    },
    {
      "step": 1535,
      "loss": 0.11554794758558273,
      "grad_norm": 1.228555712286393
    },
    {
      "step": 1540,
      "loss": 0.08000781387090683,
      "grad_norm": 1.1839980918847255
    },
    {
      "step": 1545,
      "loss": 0.14671650528907776,
      "grad_norm": 1.4501574858427417
    },
    {
      "step": 1550,
      "loss": 0.3144969046115875,
      "grad_norm": 2.6115527308382385
    },
    {
      "step": 1555,
      "loss": 0.30513548851013184,
      "grad_norm": 1.7850904300268797
    },
    {
      "step": 1560,
      "loss": 0.16010938584804535,
      "grad_norm": 2.0732307565441506
    },
    {
      "step": 1565,
      "loss": 0.19276678562164307,
      "grad_norm": 1.8101046164515249
    },
    {
      "step": 1570,
      "loss": 0.22682853043079376,
      "grad_norm": 2.144217694932228
    },
    {
      "step": 1575,
      "loss": 0.07965253293514252,
      "grad_norm": 0.8683309578351136
    },
    {
      "step": 1580,
      "loss": 0.25955426692962646,
      "grad_norm": 2.1683160595668216
    },
    {
      "step": 1585,
      "loss": 0.18087098002433777,
      "grad_norm": 1.7689758003226725
    },
    {
      "step": 1590,
      "loss": 0.29005342721939087,
      "grad_norm": 2.5665866764422938
    },
    {
      "step": 1595,
      "loss": 0.13832658529281616,
      "grad_norm": 1.6233332058430594
    },
    {
      "step": 1600,
      "loss": 0.38327500224113464,
      "grad_norm": 3.2330268864911766
    },
    {
      "step": 1605,
      "loss": 0.19859987497329712,
      "grad_norm": 2.2013744920467477
    },
    {
      "step": 1610,
      "loss": 0.3489300310611725,
      "grad_norm": 2.259466909516826
    },
    {
      "step": 1615,
      "loss": 0.3657590448856354,
      "grad_norm": 2.4148484615927677
    },
    {
      "step": 1620,
      "loss": 0.05133702605962753,
      "grad_norm": 0.6382896648260963
    },
    {
      "step": 1625,
      "loss": 0.33177536725997925,
      "grad_norm": 1.7328955091062486
    },
    {
      "step": 1630,
      "loss": 0.16944003105163574,
      "grad_norm": 1.9469090412889996
    },
    {
      "step": 1635,
      "loss": 0.17210623621940613,
      "grad_norm": 1.3195011286456444
    },
    {
      "step": 1640,
      "loss": 0.29511839151382446,
      "grad_norm": 2.3549150141182116
    },
    {
      "step": 1645,
      "loss": 0.1424711048603058,
      "grad_norm": 1.1356201822460419
    },
    {
      "step": 1650,
      "loss": 0.30701857805252075,
      "grad_norm": 2.1858808771309177
    },
    {
      "step": 1655,
      "loss": 0.30401086807250977,
      "grad_norm": 2.2414264783389837
    },
    {
      "step": 1660,
      "loss": 0.1837012767791748,
      "grad_norm": 1.9637644376427275
    },
    {
      "step": 1665,
      "loss": 0.2562338709831238,
      "grad_norm": 1.482438388106172
    },
    {
      "step": 1670,
      "loss": 0.05050646886229515,
      "grad_norm": 1.025422087903262
    },
    {
      "step": 1675,
      "loss": 0.24454651772975922,
      "grad_norm": 1.7316154425341266
    },
    {
      "step": 1680,
      "loss": 0.1874801367521286,
      "grad_norm": 1.3548627931471242
    },
    {
      "step": 1685,
      "loss": 0.25117501616477966,
      "grad_norm": 2.5866238243524333
    },
    {
      "step": 1690,
      "loss": 0.28426647186279297,
      "grad_norm": 2.898211940106121
    },
    {
      "step": 1695,
      "loss": 0.08239559829235077,
      "grad_norm": 1.0646114493568017
    },
    {
      "step": 1700,
      "loss": 0.24353840947151184,
      "grad_norm": 2.5925751835810456
    },
    {
      "step": 1705,
      "loss": 0.2630201578140259,
      "grad_norm": 1.5675671426760311
    },
    {
      "step": 1710,
      "loss": 0.25486207008361816,
      "grad_norm": 1.854393987917387
    },
    {
      "step": 1715,
      "loss": 0.2774352729320526,
      "grad_norm": 2.13530400594312
    },
    {
      "step": 1720,
      "loss": 0.40086203813552856,
      "grad_norm": 2.4509117408868604
    },
    {
      "step": 1725,
      "loss": 0.31439417600631714,
      "grad_norm": 2.672020744982942
    },
    {
      "step": 1730,
      "loss": 0.35479646921157837,
      "grad_norm": 3.024597869700315
    },
    {
      "step": 1735,
      "loss": 0.20763739943504333,
      "grad_norm": 1.536814261419475
    },
    {
      "step": 1740,
      "loss": 0.24021390080451965,
      "grad_norm": 3.0497556212512116
    },
    {
      "step": 1745,
      "loss": 0.27767854928970337,
      "grad_norm": 1.9360516493146775
    },
    {
      "step": 1750,
      "loss": 0.1555313766002655,
      "grad_norm": 1.0792899877306332
    },
    {
      "step": 1755,
      "loss": 0.2029845416545868,
      "grad_norm": 1.9928833090521894
    },
    {
      "step": 1760,
      "loss": 0.17743083834648132,
      "grad_norm": 1.9584330100495015
    },
    {
      "step": 1765,
      "loss": 0.27180802822113037,
      "grad_norm": 1.868655665513781
    },
    {
      "step": 1770,
      "loss": 0.2429015189409256,
      "grad_norm": 1.9810868468978373
    },
    {
      "step": 1775,
      "loss": 0.0786128044128418,
      "grad_norm": 1.2284587490796484
    },
    {
      "step": 1780,
      "loss": 0.27127307653427124,
      "grad_norm": 2.154986130815325
    },
    {
      "step": 1785,
      "loss": 0.07528312504291534,
      "grad_norm": 1.1706328998983477
    },
    {
      "step": 1790,
      "loss": 0.21201995015144348,
      "grad_norm": 1.5687813118806273
    },
    {
      "step": 1795,
      "loss": 0.1948804259300232,
      "grad_norm": 1.4756015149662813
    },
    {
      "step": 1800,
      "loss": 0.16444192826747894,
      "grad_norm": 2.076930167343459
    },
    {
      "step": 1805,
      "loss": 0.2505432367324829,
      "grad_norm": 1.855757448768757
    },
    {
      "step": 1810,
      "loss": 0.2573055624961853,
      "grad_norm": 2.289281717008934
    },
    {
      "step": 1815,
      "loss": 0.3898029029369354,
      "grad_norm": 2.284604656030286
    },
    {
      "step": 1820,
      "loss": 0.13026876747608185,
      "grad_norm": 1.8968738664807492
    },
    {
      "step": 1825,
      "loss": 0.10875969380140305,
      "grad_norm": 1.320651175480476
    },
    {
      "step": 1830,
      "loss": 0.1973291039466858,
      "grad_norm": 2.152183968920648
    },
    {
      "step": 1835,
      "loss": 0.1429883986711502,
      "grad_norm": 1.453468897287407
    },
    {
      "step": 1840,
      "loss": 0.21231023967266083,
      "grad_norm": 1.8699477112726295
    },
    {
      "step": 1845,
      "loss": 0.1544308066368103,
      "grad_norm": 1.453502402378288
    },
    {
      "step": 1850,
      "loss": 0.13146263360977173,
      "grad_norm": 1.5170137770627545
    },
    {
      "step": 1855,
      "loss": 0.1671745330095291,
      "grad_norm": 1.3160467107264346
    },
    {
      "step": 1860,
      "loss": 0.1630348116159439,
      "grad_norm": 1.325651015704705
    },
    {
      "step": 1865,
      "loss": 0.27040284872055054,
      "grad_norm": 2.0828422625310665
    },
    {
      "step": 1870,
      "loss": 0.05810222029685974,
      "grad_norm": 0.7747030919659579
    },
    {
      "step": 1875,
      "loss": 0.08071884512901306,
      "grad_norm": 2.035719806574598
    },
    {
      "step": 1880,
      "loss": 0.2652667760848999,
      "grad_norm": 1.4976375571539855
    },
    {
      "step": 1885,
      "loss": 0.1995408535003662,
      "grad_norm": 1.7939084807708476
    },
    {
      "step": 1890,
      "loss": 0.08887811750173569,
      "grad_norm": 1.2709928440213383
    },
    {
      "step": 1895,
      "loss": 0.20573017001152039,
      "grad_norm": 1.7795291589016116
    },
    {
      "step": 1900,
      "loss": 0.2076561152935028,
      "grad_norm": 1.7132021377177746
    },
    {
      "step": 1905,
      "loss": 0.16364935040473938,
      "grad_norm": 1.229292471241425
    },
    {
      "step": 1910,
      "loss": 0.1349954903125763,
      "grad_norm": 1.1831397356501843
    },
    {
      "step": 1915,
      "loss": 0.31824034452438354,
      "grad_norm": 1.6180154583175617
    },
    {
      "step": 1920,
      "loss": 0.1352476179599762,
      "grad_norm": 2.050429587241258
    },
    {
      "step": 1925,
      "loss": 0.058328453451395035,
      "grad_norm": 0.7185395908873183
    },
    {
      "step": 1930,
      "loss": 0.23575955629348755,
      "grad_norm": 1.7478841198338846
    },
    {
      "step": 1935,
      "loss": 0.3311229944229126,
      "grad_norm": 2.046174762347835
    },
    {
      "step": 1940,
      "loss": 0.25858041644096375,
      "grad_norm": 1.8843783299073433
    },
    {
      "step": 1945,
      "loss": 0.2952263653278351,
      "grad_norm": 2.425886161756939
    },
    {
      "step": 1950,
      "loss": 0.044152483344078064,
      "grad_norm": 0.5959000467137999
    },
    {
      "step": 1955,
      "loss": 0.14520364999771118,
      "grad_norm": 1.5057535070238894
    },
    {
      "step": 1960,
      "loss": 0.2468685507774353,
      "grad_norm": 2.069055554765002
    },
    {
      "step": 1965,
      "loss": 0.15361303091049194,
      "grad_norm": 1.8658124524890023
    },
    {
      "step": 1970,
      "loss": 0.19842112064361572,
      "grad_norm": 1.562699271327725
    },
    {
      "step": 1975,
      "loss": 0.23357939720153809,
      "grad_norm": 1.7500664730230329
    },
    {
      "step": 1980,
      "loss": 0.1553601622581482,
      "grad_norm": 1.1029042799409432
    },
    {
      "step": 1985,
      "loss": 0.07915709167718887,
      "grad_norm": 1.2666574345337174
    },
    {
      "step": 1990,
      "loss": 0.15413112938404083,
      "grad_norm": 1.3363037641395343
    },
    {
      "step": 1995,
      "loss": 0.22384265065193176,
      "grad_norm": 1.7255576676811852
    }
  ],
  "final_accuracy": 0.9437,
  "total_steps": 2000,
  "checkpoint_steps": [
    100,
    1000,
    2000
  ],
  "elapsed_time": 35.50213098526001
}