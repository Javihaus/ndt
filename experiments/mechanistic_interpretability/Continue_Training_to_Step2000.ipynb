{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue Training to Step 2000\n",
    "\n",
    "**Purpose**: Load step 1000 checkpoints and continue training to step 2000\n",
    "\n",
    "**Runtime**: ~15 minutes on GPU A100\n",
    "\n",
    "**Prerequisites**: \n",
    "- Upload the existing `checkpoint_experiments_results.zip` to Colab\n",
    "- Extract it to get step 1000 checkpoints\n",
    "\n",
    "**Output**: 3 additional checkpoints (step 2000 for each experiment)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Runtime**: Set to GPU (Runtime → Change runtime type → GPU → A100)\n",
    "2. **Upload**: Upload `checkpoint_experiments_results.zip` to /content/\n",
    "3. **Execute cells in order**\n",
    "4. **Download**: New checkpoints will be zipped at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision tqdm scikit-learn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Existing Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the uploaded zip file\n",
    "!unzip -q /content/checkpoint_experiments_results.zip -d /content/\n",
    "\n",
    "print(\"✓ Checkpoints extracted\")\n",
    "print(\"\\nExisting checkpoints:\")\n",
    "!ls -lh /content/checkpoints/transformer_deep_mnist/\n",
    "!ls -lh /content/checkpoints/cnn_deep_mnist/\n",
    "!ls -lh /content/checkpoints/mlp_narrow_mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Architecture Definitions\n",
    "\n",
    "(Same architectures as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Architecture\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int], num_classes: int):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x.view(x.size(0), -1))\n",
    "\n",
    "# CNN Architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int, conv_channels: List[int]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_channels = in_channels\n",
    "        for channels in conv_channels:\n",
    "            layers.extend([\n",
    "                nn.Conv2d(prev_channels, channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2)\n",
    "            ])\n",
    "            prev_channels = channels\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        self.flat_size = conv_channels[-1] * 3 * 3\n",
    "        self.fc = nn.Linear(self.flat_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Transformer Architecture\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim: int, d_model: int, nhead: int, \n",
    "                 num_layers: int, num_classes: int, seq_len: int = 16):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.input_proj = nn.Linear(input_dim // seq_len, d_model)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(seq_len, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, self.seq_len, -1)\n",
    "        x = self.input_proj(x)\n",
    "        x = x + self.pos_encoder.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def create_model(arch_name: str, input_dim: int, num_classes: int):\n",
    "    \"\"\"Create model based on architecture name.\"\"\"\n",
    "    if arch_name == 'mlp_narrow':\n",
    "        return SimpleMLP(input_dim, [32, 32, 32, 32], num_classes)\n",
    "    elif arch_name == 'cnn_deep':\n",
    "        return SimpleCNN(1, num_classes, [32, 64, 128])\n",
    "    elif arch_name == 'transformer_deep':\n",
    "        return SimpleTransformer(input_dim, d_model=128, nhead=4, num_layers=4, num_classes=num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown architecture: {arch_name}\")\n",
    "\n",
    "print(\"✓ Architectures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_loaders(batch_size=64):\n",
    "    \"\"\"Load MNIST dataset.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='/content/data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='/content/data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, 28*28, 10\n",
    "\n",
    "print(\"Loading MNIST dataset...\")\n",
    "train_loader, test_loader, input_dim, num_classes = get_mnist_loaders(batch_size=64)\n",
    "print(f\"✓ Data loaded: {len(train_loader.dataset)} train, {len(test_loader.dataset)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Continue Training Function\n",
    "\n",
    "**Key Fix**: Changed loop condition to `step <= num_steps` to reach step 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_training_to_2000(checkpoint_path, model, train_loader, test_loader, \n",
    "                               device, experiment_name):\n",
    "    \"\"\"Load step 1000 checkpoint and continue training to step 2000.\"\"\"\n",
    "    \n",
    "    # Load checkpoint\n",
    "    print(f\"\\nLoading checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    start_step = checkpoint['step']\n",
    "    print(f\"✓ Loaded checkpoint from step {start_step}\")\n",
    "    print(f\"✓ Starting loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    # Continue training from 1000 to 2000\n",
    "    target_step = 2000\n",
    "    step = start_step\n",
    "    measurements = []\n",
    "    \n",
    "    print(f\"\\nContinuing training from step {start_step} to {target_step}...\")\n",
    "    pbar = tqdm(total=target_step - start_step, desc=experiment_name)\n",
    "    \n",
    "    # FIXED: Changed condition to <= to reach step 2000\n",
    "    while step <= target_step:\n",
    "        for inputs, labels in train_loader:\n",
    "            if step > target_step:  # FIXED: Changed from >= to >\n",
    "                break\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Training step\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient norm\n",
    "            grad_norm = sum(p.grad.norm().item() ** 2 for p in model.parameters() \n",
    "                          if p.grad is not None) ** 0.5\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Record measurement every 5 steps\n",
    "            if step % 5 == 0:\n",
    "                measurements.append({\n",
    "                    'step': step,\n",
    "                    'loss': loss.item(),\n",
    "                    'grad_norm': grad_norm\n",
    "                })\n",
    "            \n",
    "            step += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Save step 2000 checkpoint\n",
    "    checkpoint_dir = Path(checkpoint_path).parent\n",
    "    step_2000_path = checkpoint_dir / 'checkpoint_step_02000.pt'\n",
    "    \n",
    "    torch.save({\n",
    "        'step': 2000,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.item(),\n",
    "        'experiment_name': experiment_name\n",
    "    }, step_2000_path)\n",
    "    \n",
    "    print(f\"\\n✓ Saved checkpoint: {step_2000_path.name}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    final_accuracy = correct / total\n",
    "    print(f\"✓ Final accuracy at step 2000: {final_accuracy:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'measurements': measurements,\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'checkpoint_path': str(step_2000_path)\n",
    "    }\n",
    "\n",
    "print(\"✓ Continue training function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run All Continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configurations\n",
    "EXPERIMENTS = {\n",
    "    'transformer_deep_mnist': 'transformer_deep',\n",
    "    'cnn_deep_mnist': 'cnn_deep',\n",
    "    'mlp_narrow_mnist': 'mlp_narrow'\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "start_time = time.time()\n",
    "\n",
    "for exp_name, arch_name in EXPERIMENTS.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"CONTINUING: {exp_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(arch_name, input_dim, num_classes)\n",
    "    print(f\"Model: {arch_name}\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Path to step 1000 checkpoint\n",
    "    checkpoint_path = Path(f'/content/checkpoints/{exp_name}/checkpoint_step_01000.pt')\n",
    "    \n",
    "    # Continue training\n",
    "    exp_start = time.time()\n",
    "    result = continue_training_to_2000(\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        experiment_name=exp_name\n",
    "    )\n",
    "    exp_elapsed = time.time() - exp_start\n",
    "    \n",
    "    result['elapsed_time'] = exp_elapsed\n",
    "    all_results.append(result)\n",
    "    \n",
    "    print(f\"\\n✓ Completed in {exp_elapsed/60:.1f} minutes\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL CONTINUATIONS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"New checkpoints created: {len(all_results)}\")\n",
    "print(\"\\nFinal accuracies at step 2000:\")\n",
    "for result in all_results:\n",
    "    print(f\"  {result['experiment_name']}: {result['final_accuracy']:.4f}\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'total_time': total_time,\n",
    "    'num_experiments': len(all_results),\n",
    "    'experiments': all_results\n",
    "}\n",
    "with open('/content/results/continuation_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Summary saved: /content/results/continuation_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify New Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\nVERIFYING NEW CHECKPOINTS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for exp_name in EXPERIMENTS.keys():\n",
    "    checkpoint_dir = Path(f'/content/checkpoints/{exp_name}')\n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    for checkpoint_file in sorted(checkpoint_dir.glob('*.pt')):\n",
    "        size_mb = checkpoint_file.stat().st_size / 1e6\n",
    "        print(f\"  ✓ {checkpoint_file.name}: {size_mb:.1f}MB\")\n",
    "\n",
    "# Total checkpoint count\n",
    "all_checkpoints = list(Path('/content/checkpoints').rglob('*.pt'))\n",
    "print(f\"\\nTotal checkpoints: {len(all_checkpoints)} (expected: 9)\")\n",
    "\n",
    "if len(all_checkpoints) == 9:\n",
    "    print(\"\\n✅ SUCCESS: All 9 checkpoints are now present!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ WARNING: Expected 9 checkpoints, found {len(all_checkpoints)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Package Complete Results for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new zip with ALL checkpoints (including the 3 new step 2000 ones)\n",
    "!zip -r /content/checkpoint_experiments_COMPLETE.zip /content/checkpoints /content/results\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY FOR DOWNLOAD\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFile: checkpoint_experiments_COMPLETE.zip\")\n",
    "print(\"\\nContents:\")\n",
    "print(\"  - 9 checkpoint files (.pt) - NOW COMPLETE!\")\n",
    "print(\"    • 3 experiments × 3 checkpoints each\")\n",
    "print(\"    • Steps: 100, 1000, 2000\")\n",
    "print(\"  - 4 result files (.json)\")\n",
    "print(\"\\nDownload instructions:\")\n",
    "print(\"  1. Click the folder icon on the left\")\n",
    "print(\"  2. Right-click 'checkpoint_experiments_COMPLETE.zip'\")\n",
    "print(\"  3. Select 'Download'\")\n",
    "print(\"\\nOr run the next cell to download directly:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('/content/checkpoint_experiments_COMPLETE.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After downloading the complete results:\n",
    "\n",
    "1. **Upload the complete zip file** to the Claude Code environment\n",
    "2. **Extract to the experiments directory**\n",
    "3. **Run Modified Phase 2 Analysis**:\n",
    "   ```bash\n",
    "   cd experiments/mechanistic_interpretability\n",
    "   python3 modified_phase2_analysis.py\n",
    "   ```\n",
    "\n",
    "The analysis will now have all 9 checkpoints and can:\n",
    "- Compare early (step 100) vs mid (step 1000) vs late (step 2000) features\n",
    "- Test hypothesis: Are early features qualitatively different from late?\n",
    "- Generate complete visualizations and final report\n",
    "\n",
    "**Total runtime for this continuation**: ~15 minutes (vs 90 minutes for full re-run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
