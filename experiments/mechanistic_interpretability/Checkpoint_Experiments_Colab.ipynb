{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Modified Phase 2: Checkpoint Experiments\n",
    "\n",
    "**Goal**: Run 3 experiments with checkpoint saving at steps [100, 1000, 2000]\n",
    "\n",
    "**Experiments**:\n",
    "1. transformer_deep_mnist\n",
    "2. cnn_deep_mnist\n",
    "3. mlp_narrow_mnist\n",
    "\n",
    "**Runtime**: ~90 minutes on GPU A100\n",
    "\n",
    "**Output**: 9 checkpoints (~75MB total) + experiment results\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Runtime**: Set to GPU (Runtime → Change runtime type → GPU → A100)\n",
    "2. **Execute cells in order**\n",
    "3. **Results saved to**: `/content/checkpoints/` and `/content/results/`\n",
    "4. **Download**: At end, zip and download results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision tqdm scikit-learn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "checkpoint_plan"
   },
   "source": [
    "## 2. Checkpoint Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plan"
   },
   "outputs": [],
   "source": [
    "# Checkpoint configuration\n",
    "CHECKPOINT_PLAN = {\n",
    "    'transformer_deep_mnist': {\n",
    "        'arch': 'transformer_deep',\n",
    "        'dataset': 'mnist',\n",
    "        'checkpoint_steps': [100, 1000, 2000],\n",
    "        'total_steps': 2000\n",
    "    },\n",
    "    'cnn_deep_mnist': {\n",
    "        'arch': 'cnn_deep',\n",
    "        'dataset': 'mnist',\n",
    "        'checkpoint_steps': [100, 1000, 2000],\n",
    "        'total_steps': 2000\n",
    "    },\n",
    "    'mlp_narrow_mnist': {\n",
    "        'arch': 'mlp_narrow',\n",
    "        'dataset': 'mnist',\n",
    "        'checkpoint_steps': [100, 1000, 2000],\n",
    "        'total_steps': 2000\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "Path('/content/checkpoints').mkdir(exist_ok=True)\n",
    "Path('/content/results').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Checkpoint plan loaded:\")\n",
    "print(f\"Total experiments: {len(CHECKPOINT_PLAN)}\")\n",
    "print(f\"Checkpoints per experiment: 3\")\n",
    "print(f\"Total checkpoints: 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "architectures"
   },
   "source": [
    "## 3. Architecture Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arch_code"
   },
   "outputs": [],
   "source": [
    "# MLP Architecture\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int], num_classes: int):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x.view(x.size(0), -1))\n",
    "\n",
    "# CNN Architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int, conv_channels: List[int]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_channels = in_channels\n",
    "        for channels in conv_channels:\n",
    "            layers.extend([\n",
    "                nn.Conv2d(prev_channels, channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2)\n",
    "            ])\n",
    "            prev_channels = channels\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        # After 3 pooling layers: 28x28 -> 14x14 -> 7x7 -> 3x3\n",
    "        self.flat_size = conv_channels[-1] * 3 * 3\n",
    "        self.fc = nn.Linear(self.flat_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Transformer Architecture\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim: int, d_model: int, nhead: int, \n",
    "                 num_layers: int, num_classes: int, seq_len: int = 16):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.input_proj = nn.Linear(input_dim // seq_len, d_model)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(seq_len, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, self.seq_len, -1)\n",
    "        x = self.input_proj(x)\n",
    "        x = x + self.pos_encoder.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def create_model(arch_name: str, input_dim: int, num_classes: int):\n",
    "    \"\"\"Create model based on architecture name.\"\"\"\n",
    "    if arch_name == 'mlp_narrow':\n",
    "        return SimpleMLP(input_dim, [32, 32, 32, 32], num_classes)\n",
    "    elif arch_name == 'cnn_deep':\n",
    "        return SimpleCNN(1, num_classes, [32, 64, 128])  # MNIST has 1 channel\n",
    "    elif arch_name == 'transformer_deep':\n",
    "        return SimpleTransformer(input_dim, d_model=128, nhead=4, num_layers=4, num_classes=num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown architecture: {arch_name}\")\n",
    "\n",
    "print(\"✓ Architectures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## 4. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_code"
   },
   "outputs": [],
   "source": [
    "def get_mnist_loaders(batch_size=64):\n",
    "    \"\"\"Load MNIST dataset.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='/content/data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='/content/data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, 28*28, 10  # input_dim, num_classes\n",
    "\n",
    "print(\"✓ Data loading ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 5. Training with Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_code"
   },
   "outputs": [],
   "source": [
    "def train_with_checkpoints(model, train_loader, test_loader, device, \n",
    "                           num_steps, checkpoint_steps, checkpoint_dir, \n",
    "                           experiment_name):\n",
    "    \"\"\"Train model and save checkpoints at specified steps.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    checkpoint_steps_set = set(checkpoint_steps)\n",
    "    measurements = []\n",
    "    step = 0\n",
    "    \n",
    "    print(f\"\\nTraining {experiment_name}...\")\n",
    "    print(f\"Target steps: {num_steps}\")\n",
    "    print(f\"Checkpoint steps: {checkpoint_steps}\")\n",
    "    \n",
    "    pbar = tqdm(total=num_steps, desc=experiment_name)\n",
    "    \n",
    "    while step < num_steps:\n",
    "        for inputs, labels in train_loader:\n",
    "            if step >= num_steps:\n",
    "                break\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Training step\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient norm\n",
    "            grad_norm = sum(p.grad.norm().item() ** 2 for p in model.parameters() \n",
    "                          if p.grad is not None) ** 0.5\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if step in checkpoint_steps_set:\n",
    "                checkpoint_path = checkpoint_dir / f'checkpoint_step_{step:05d}.pt'\n",
    "                torch.save({\n",
    "                    'step': step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                    'experiment_name': experiment_name\n",
    "                }, checkpoint_path)\n",
    "                print(f\"\\n✓ Saved checkpoint: {checkpoint_path.name}\")\n",
    "            \n",
    "            # Record measurement every 5 steps\n",
    "            if step % 5 == 0:\n",
    "                measurements.append({\n",
    "                    'step': step,\n",
    "                    'loss': loss.item(),\n",
    "                    'grad_norm': grad_norm\n",
    "                })\n",
    "            \n",
    "            step += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    final_accuracy = correct / total\n",
    "    print(f\"\\n✓ Final accuracy: {final_accuracy:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'measurements': measurements,\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'total_steps': num_steps,\n",
    "        'checkpoint_steps': checkpoint_steps\n",
    "    }\n",
    "\n",
    "print(\"✓ Training function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_experiments"
   },
   "source": [
    "## 6. Run All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_all"
   },
   "outputs": [],
   "source": [
    "# Load data once\n",
    "print(\"Loading MNIST dataset...\")\n",
    "train_loader, test_loader, input_dim, num_classes = get_mnist_loaders(batch_size=64)\n",
    "print(f\"✓ Data loaded: {len(train_loader.dataset)} train, {len(test_loader.dataset)} test\\n\")\n",
    "\n",
    "# Run all experiments\n",
    "all_results = []\n",
    "start_time = time.time()\n",
    "\n",
    "for exp_name, config in CHECKPOINT_PLAN.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"EXPERIMENT: {exp_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = Path(f'/content/checkpoints/{exp_name}')\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(config['arch'], input_dim, num_classes)\n",
    "    print(f\"Model: {config['arch']}\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Train\n",
    "    exp_start = time.time()\n",
    "    result = train_with_checkpoints(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        num_steps=config['total_steps'],\n",
    "        checkpoint_steps=config['checkpoint_steps'],\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        experiment_name=exp_name\n",
    "    )\n",
    "    exp_elapsed = time.time() - exp_start\n",
    "    \n",
    "    result['elapsed_time'] = exp_elapsed\n",
    "    all_results.append(result)\n",
    "    \n",
    "    # Save result\n",
    "    result_file = Path(f'/content/results/{exp_name}_result.json')\n",
    "    with open(result_file, 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Completed in {exp_elapsed/60:.1f} minutes\")\n",
    "    print(f\"✓ Result saved: {result_file}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total time: {total_time/60:.1f} minutes ({total_time/3600:.1f} hours)\")\n",
    "print(f\"Experiments: {len(all_results)}\")\n",
    "print(f\"Checkpoints saved: {len(all_results) * 3}\")\n",
    "print(\"\\nAccuracies:\")\n",
    "for result in all_results:\n",
    "    print(f\"  {result['experiment_name']}: {result['final_accuracy']:.4f}\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'total_time': total_time,\n",
    "    'num_experiments': len(all_results),\n",
    "    'experiments': all_results\n",
    "}\n",
    "with open('/content/results/summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Summary saved: /content/results/summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify"
   },
   "source": [
    "## 7. Verify Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\nVERIFYING CHECKPOINTS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for exp_name, config in CHECKPOINT_PLAN.items():\n",
    "    checkpoint_dir = Path(f'/content/checkpoints/{exp_name}')\n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    total_size = 0\n",
    "    for step in config['checkpoint_steps']:\n",
    "        checkpoint_path = checkpoint_dir / f'checkpoint_step_{step:05d}.pt'\n",
    "        if checkpoint_path.exists():\n",
    "            size_mb = checkpoint_path.stat().st_size / 1e6\n",
    "            total_size += size_mb\n",
    "            print(f\"  ✓ Step {step}: {size_mb:.1f}MB\")\n",
    "        else:\n",
    "            print(f\"  ✗ Step {step}: MISSING\")\n",
    "    print(f\"  Total: {total_size:.1f}MB\")\n",
    "\n",
    "# Total checkpoint size\n",
    "all_checkpoints_size = sum(\n",
    "    f.stat().st_size for f in Path('/content/checkpoints').rglob('*.pt')\n",
    ") / 1e6\n",
    "print(f\"\\nTotal checkpoints size: {all_checkpoints_size:.1f}MB\")\n",
    "print(f\"Checkpoint count: {len(list(Path('/content/checkpoints').rglob('*.pt')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 8. Package Results for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "package"
   },
   "outputs": [],
   "source": [
    "# Create zip file\n",
    "!zip -r /content/checkpoint_experiments_results.zip /content/checkpoints /content/results\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY FOR DOWNLOAD\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFile: checkpoint_experiments_results.zip\")\n",
    "print(\"\\nContents:\")\n",
    "print(\"  - 9 checkpoint files (.pt)\")\n",
    "print(\"  - 3 experiment result files (.json)\")\n",
    "print(\"  - 1 summary file (summary.json)\")\n",
    "print(\"\\nDownload instructions:\")\n",
    "print(\"  1. Click the folder icon on the left\")\n",
    "print(\"  2. Right-click 'checkpoint_experiments_results.zip'\")\n",
    "print(\"  3. Select 'Download'\")\n",
    "print(\"\\nOr run the next cell to download directly:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_code"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('/content/checkpoint_experiments_results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "After downloading the results:\n",
    "\n",
    "1. **Extract the zip file** to your local machine\n",
    "2. **Upload to Claude Code environment**:\n",
    "   - Place checkpoints in: `experiments/checkpoints/`\n",
    "   - Place results in: `experiments/new/results/phase1_checkpoints/`\n",
    "3. **Run Modified Phase 2 Analysis**:\n",
    "   ```bash\n",
    "   cd experiments/mechanistic_interpretability\n",
    "   python3 modified_phase2_analysis.py\n",
    "   ```\n",
    "\n",
    "The analysis will:\n",
    "- Load the 9 checkpoints\n",
    "- Extract features (attention/filters/activations)\n",
    "- Compute similarity matrices\n",
    "- Test hypothesis: Are early features qualitatively different from late?\n",
    "- Generate visualizations and final report"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
